{"/deploy/":{"data":{"":"A collection of pages to administrate Rhize: install, upgrade, back up, and more.\nInstallA guide to install Rhize services on your Kubernetes cluster. Back upGuides to back up your data on Rhize RestoreGuides to restore your data on Rhize MaintainGuides to maintain your data on Rhize UpgradeHow to upgrade Rhize About OpenID connectThe Rhize GraphQL implementation uses OpenIDConnect for Authentication and role-based access control. This section describes how to set up Keycloak "},"title":"Deploy"},"/deploy/about-openidconnect/":{"data":{"":"Rhize uses OpenIDConnect to connect to a Keycloak server to authenticate users and manage Role-based access controls.\nOpen ID Connect is a security architecture that uses JSON Web Tokens (JWTs) to access secured resources. JWTs are issued by Keycloak. Users can also be managed in Keycloak. Or you can manage users in other services such as LDAP, Google, Azure AD, Facebook, etc.\nThe general authentication flow is as follows:\nWhen a user accesses the user interface, the UI redirects to Keycloak. Depending on how it is configured, Keycloak redirects to the authentication provider so that the user can log in. If the user is successfully authenticated, Keycloak redirects back to the user interface with an authentication code in the URL parameters. The UI calls a secure API to exchange the authentication code for a JWT. The UI then uses that JWT to access secure APIs such as the Rhize GraphQL API. The Rhize DB, libreBaas, has the public key from Keycloak, which can be used to verify the JWT.\nsequenceDiagram actor User participant UI as Web UI participant Rhize as Rhize DB participant KC as Keycloak participant AP as AuthProvider Rhize-\u003e\u003eKC: Get Public Key User-\u003e\u003eUI: Log In UI--\u003e\u003eKC: Redirect KC--\u003e\u003eAP: Redirect AP-\u003e\u003eUser: Credentials AP--\u003e\u003eKC: Auth Result KC--\u003e\u003eUI: Redirect with Code UI-\u003e\u003eKC: Exchange Code for Token KC-\u003e\u003eUI: Reply with id_token and access_token UI-\u003e\u003eRhize: Access API with Bearer Token Rhize-\u003e\u003eRhize: Verify Token with Public Key from Keycloak "},"title":"About OpenID connect"},"/deploy/backup/":{"data":{"":"Backup is critical to ensure reliability and recovery.\nThese guides show you how to back up different services and data on Rhize. They also serve as blueprints for automation.\nYour organization must determine how frequently you backup services, and how long you store them for. The correct practice here is highly contextual, depending on the size of the data, the importance of the data, and the general regulatory and governance demands of your industry.\nBack up the Graph DB to S3How to back up the Rhize graph database to Amazon S3 storage. Back up the Graph DBHow to back up the Rhize graph database Back up Audit PostgreSQLHow to backup Audit PostgreSQL on your Rhize deployment Back up KeycloakHow to backup Keycloak on your Rhize deployment Back up GrafanaHow to backup Grafana on your Rhize deployment "},"title":"Back up"},"/deploy/backup/audit/":{"data":{"":"This guide shows you the procedure to backup your Audit PostgreSQL database on your Rhize Kubernetes deployment.","next-steps#Next Steps":" To back up other Rhize services, read how to backup: Keycloak. Grafana. The Graph Database. ","prerequisites#Prerequisites":"Before you start, ensure you have the following:\nA designated backup location, for example ~/rhize-backups/libre-audit. Access to the Rhize Kubernetes Environment Optional: kubectx utilities kubectx to manage multiple clusters kubens to switch between and configure namespaces easily Optional: the k8 Lens IDE, if you prefer to use Kubernetes graphically Also, before you start, confirm you are in the right context and namespace.\n## context kubectl config current-context ## namespace kubectl get namespace To change the namespace for all subsequent kubectl commands to libre, run this command:\nkubectl config set-context --current --namespace=libre For a reference of useful kubectl commands, refer to the official kubectl Cheat Sheet.","steps#Steps":"To back up Audit PostgreSQL, follow these steps:\nCheck the logs for the Audit pods, either in Lens or with kubectl logs. Ensure there are no errors.\nRetrieve the Audit user password using the following command:\nkubectl get secret \u003cSECRET-NAME\u003e -o jsonpath=\"{.data.\u003cSECRET-KEY\u003e}\" | base64 --decode Execute a command on the Audit Postgres pod to perform a full backup:\nkubectl exec -i audit-postgres-0 -- pg_dumpall -U \u003cDB_USER\u003e | gzip \u003e audit-postgres-backup-$(date +\"%Y%m%dT%I%M%p\").sql.gz On success, the backup creates a GZIP file, audit-postgres-backup-YYYYMMDDTHHMMSS.sql.gz. To check that the backup succeeded, unzip the files and inspect the data."},"title":"Back up Audit PostgreSQL"},"/deploy/backup/binary/":{"data":{"":"This guide shows you how to back up the Rhize Graph database to Amazon S3 and S3-compatible storage.","next-steps#Next Steps":" Test the Restore Graph Database From S3 procedure to ensure you can recover data from Amazon S3 in case of an emergency. To back up other Rhize services, read how to backup Grafana. ","prerequisites#Prerequisites":"Before you start, ensure you have the following:\nA designated S3 backup location, for example s3://s3.\u003cAWS-REGION\u003e.amazonaws.com/\u003cAWS-BUCKET-NAME\u003e. Access to your Rhize Kubernetes Environment Optional: kubectx utilities kubectx to manage multiple clusters kubens to switch between and configure namespaces easily Optional: the k8 Lens IDE, if you prefer to use Kubernetes graphically. Before you start, confirm you are in the right context and namespace:\n## context kubectl config current-context ## namespace kubectl get namespace To change the namespace for all subsequent kubectl commands to libre, run this command:\nkubectl config set-context --current --namespace=libre For a reference of useful kubectl commands, refer to the official kubectl Cheat Sheet.","steps#Steps":"To back up the database, follow these steps:\nCheck the logs for the alpha and zero pods, either in Lens or with kubectl logs. Ensure there are no errors.\nkubectl logs libre-baas-baas-alpha-0 --tail=80 Set the following environmental variables:\nAWS_ACCESS_KEY_ID. Your AWS access key with permissions to write to the destination bucket AWS_SECRET_ACCESS_KEY. Your AWS access key with permissions to write to the destination bucket AWS_SESSION_TOKEN. Your AWS session token (if required) Make a POST request to your Keycloak /token endpoint to get an access_token value. For example, with curl and jq:\n## replace USERNAME and PASSWORD with your credentials USERNAME=backups@libremfg.com \\ \u0026\u0026 PASSWORD=password \\ \u0026\u0026 curl --location \\ --request POST \"${BAAS_OIDC_URL}/realms/libre/protocol/openid-connect/token\" \\ --header 'Content-Type\\ application/x-www-form-urlencoded' \\ --data-urlencode 'grant_type=password' \\ --data-urlencode \"username=\u003cUSERNAME\u003e\" \\ --data-urlencode \"password=\u003cPASSWORD\u003e\" \\ --data-urlencode \"client_id=\u003cBASS_CLIENT_ID\u003e\" \\ --data-urlencode \"client_secret=\u003cBASS_CLIENT_SECRET\u003e\" | jq .access_token Using the token from the previous step, send a POST to \u003calpha service\u003e:8080/admin to create a backup of the node to your S3 bucket. For example, with curl:\ncurl --location 'http://alpha:8080/admin' \\ --header 'Content-Type: application/json' \\ --header 'Authorization: Bearer \u003cTOKEN\u003e' \\ --data '{\"query\":\"mutation {\\n backup(input: {destination: \\\"s3://s3.\u003cAWS-REGION\u003e.amazonaws.com/\u003cAWS-BUCKET-NAME\u003e\\\"}) {\\n response {\\n message\\n code\\n }\\n taskId\\n }\\n}\",\"variables\":{}}' List available backups to confirm your backup succeeded:\ncurl --location 'http://alpha:8080/admin' \\ --header 'Content-Type: application/json' \\ --header 'Authorization: Bearer \u003cTOKEN\u003e' \\ --data '{\"query\":\"query backup {\\n\\tlistBackups(input: {location: \\\"s3://s3.\u003cAWS-REGION\u003e\u003e.amazonaws.com/\u003cAWS-BUCKET-NAME\u003e\\\"}) {\\n\\t\\tbackupId\\n\\t\\tbackupNum\\n\\t\\tpath\\n\\t\\tsince\\n\\t\\ttype\\n\\t}\\n}\",\"variables\":{}}' "},"title":"Back up the Graph DB to S3"},"/deploy/backup/grafana/":{"data":{"":"This guide shows you the procedure to back up Grafana on your Rhize Kubernetes deployment. For general instructions, refer to the official Back up Grafana documentation.","confirm-success#Confirm success":"To confirm the backup, check their sha256 sums and their content.\nTo check the sums:\nChange to the directory where you sent the backups:\ncd \u003cBACKUP\u003e/\u003cON_YOUR_DEVICE\u003e/ Confirm the checksums match:\nsha256sum -c backup.sums \\ \u003cLATEST_DATA_FILE\u003e.tar.gz \u003cLATEST_CONF_FILE\u003e.tar.gz To check that the content is correct, unzip the files and inspect the data.","next-steps#Next steps":" Test the Restore Grafana procedure to ensure you can recover data in case of an emergency. To back up other Rhize services, read how to backup the Graph Database. ","prerequisites#Prerequisites":"Before you start, ensure you have the following:\nA designated backup location, for example ~/rhize-backups/grafana. Access to the Rhize Kubernetes Environment Optional: kubectx utilities kubectx to manage multiple clusters kubens to switch between and configure namespaces easily Optional: the k8 Lens IDE, if you prefer to use Kubernetes graphically Also, before you start, confirm you are in the right context and namespace.\n## context kubectl config current-context ## namespace kubectl get namespace To change the namespace for all subsequent kubectl commands to libre, run this command:\nkubectl config set-context --current --namespace=libre For a reference of useful kubectl commands, refer to the official kubectl Cheat Sheet.","steps#Steps":"To back up the Grafana, follow these steps:\nCheck the logs for the Grafana pods, either in Lens or with kubectl logs. Ensure there are no errors.\nOpen a pod shell for one of the Grafana pods:\nkubectl exec --stdin --tty \u003cGRAFANA_POD_NAME\u003e -- /bin/bash For details, read the Kubernetes topic Get Shell to a Running Container.\nUse tar to backup the Grafana data and conf directories:\n## Data Directory Backup Command tar -v -c -f /home/grafana/grafana-data-$(date +\"%Y-%m-%dT%H.%M.%S\").tar.gz /var/lib/grafana ## Conf Directory Backup Command tar -v -c -f /home/grafana/grafana-conf-$(date +\"%Y-%m-%dT%H.%M.%S\").tar.gz /usr/share/grafana/conf Change to the backup directory. For example:\ncd /home/grafana/ Check for the latest .gz files (for example, with ls -lt). There should be new backup data and conf files whose names include timestamps from when you ran the preceding tar commands.\nCreate a checksum file for the latest backups:\nsha256sum \u003cLATEST_DATA_FILE\u003e.tar.gz \u003cLATEST_CONF_FILE\u003e.tar.gz \u003e backup.sums Exit the container shell, and then copy files out of the container to your backup location:\n## exit shell exit ## copy container files to backup kubectl cp \u003cGRAFANA_POD\u003e:/home/grafana/\u003cNEW_DATA_BACKUP_FILENAME\u003e \\ ./\u003cNEW_DATA_BACKUP_FILENAME\u003e -c grafana kubectl cp \u003cGRAFANA_POD\u003e:/home/grafana/\u003cNEW_CONF_BACKUP_FILENAME\u003e \\ ./\u003cNEW_CONF_BACKUP_FILENAME\u003e -c grafana kubectl cp \u003cGRAFANA_POD\u003e:/home/grafana/backup.sums \\ ./backup.sums -c grafana "},"title":"Back up Grafana"},"/deploy/backup/graphdb/":{"data":{"":"This guide shows you how to back up the Rhize Graph database.\nYou can also use it to model an automation workflow.","confirm-success#Confirm success":"On success, the backup creates three zipped files:\nThe GraphQL schema The DB schema A JSON file with the real database data. To check that the backup succeeded, unzip the files and inspect the data.","next-steps#Next Steps":" Test the Restore Graph Database procedure to ensure you can recover data in case of an emergency. To back up other Rhize services, read how to backup Grafana. ","prerequisites#Prerequisites":"Before you start, ensure you have the following:\nA designated backup location, for example ~/rhize-backups/database. Access to your Rhize Kubernetes Environment Optional: kubectx utilities kubectx to manage multiple clusters kubens to switch between and configure namespaces easily Optional: the k8 Lens IDE, if you prefer to use Kubernetes graphically. Before you start, confirm you are in the right context and namespace:\n## context kubectl config current-context ## namespace kubectl get namespace To change the namespace for all subsequent kubectl commands to libre, run this command:\nkubectl config set-context --current --namespace=libre For a reference of useful kubectl commands, refer to the official kubectl Cheat Sheet.","steps#Steps":"To back up the database, follow these steps:\nCheck the logs for the alpha and zero pods, either in Lens or with kubectl logs. Ensure there are no errors.\nkubectl logs libre-baas-baas-alpha-0 --tail=80 Open a pod shell for one of the alpha pods. If you are using the terminal, run this command:\nkubectl exec --stdin --tty libre-baas-baas-alpha-0 \\ -n libre -- /bin/bash For details, read the Kubernetes topic Get Shell to a Running Container.\nMake a POST request to your Keycloak /token endpoint to get an access_token value. For example, with curl and jq:\n## replace USERNAME and PASSWORD with your credentials USERNAME=backups@libremfg.com \\ \u0026\u0026 PASSWORD=password \\ \u0026\u0026 curl --location \\ --request POST \"${BAAS_OIDC_URL}/realms/libre/protocol/openid-connect/token\" \\ --header 'Content-Type\\ application/x-www-form-urlencoded' \\ --data-urlencode 'grant_type=password' \\ --data-urlencode \"username=${USERNAME}\" \\ --data-urlencode \"password=${PASSWORD}\" \\ --data-urlencode \"client_id=${BAAS_OIDC_CLIENT_ID}\" \\ --data-urlencode \"client_secret=${OIDC_SECRET}\" | jq .access_token Using the token from the previous step, send a POST to localhost:8080/admin to create a backup of the node. For example, with curl:\ncurl --location --request POST 'http://localhost:8080/admin' \\ --header 'Authorization: Bearer \u003cTOKEN\u003e' \\ --header 'Content-Type: application/json' \\ --data-raw '{\"query\":\"mutation {\\r\\n export(input: {format: \\\"json\\\", destination: \\\"/dgraph/backups/'\"$(date +\"%Y-%m-%dT%H.%M.%SZ\")\"'\\\"}) {\\r\\n response {\\r\\n message\\r\\n code\\r\\n }\\r\\n}\\r\\n}\",\"variables\":{}}' Change to the backup directory (the destination parameter in the preceding curl command). For example:\ncd /dgraph/backups Check for the latest directory. Its name should be the timestamp of when you sent the preceding curl request. For example:\nls -lt With these flags, the first listed directory should be the latest backup, named something like 2023-10-31T16.55.56Z\nCreate a file that holds the sha256 checksums of the latest backup files. You’ll use this file to confirm the copy is identical.\nsha256sum \u003cLATEST_BACKUP_DIR\u003e/dgraph.\u003cPODNAME\u003e/*.gz \u003e \u003cLATEST_BACKUP_DIR\u003e/backup.sums Exit the container shell, then copy files out of the container to your backup location:\n## exit shell exit ## copy container files to backup kubectl cp --retries=10 \u003cNAMESPACE\u003e/\u003cPODNAME\u003e:backups/\u003cCONTAINER_BACKUP\u003e \\ ./\u003cBACKUP\u003e/\u003cON_YOUR_DEVICE\u003e Use the checksum to confirm that the pod files and the local files are the same. If you are using Windows, you can run an equivalent check with the CertUtil utility:\nbashcmd ## Change to the directory cd ./\u003cBACKUP\u003e/\u003cON_YOUR_DEVICE\u003e/ ## Check sums sha256sum -c backup.sums *.gz CertUtil -hashfile C:\\\u003cBACKUP\u003e\\\u003cON_YOUR_DEVICE\u003e\\backup.sums sha256 "},"title":"Back up the Graph DB"},"/deploy/backup/keycloak/":{"data":{"":"This guide shows you how to back up Keycloak on your Rhize Kubernetes deployment.","confirm-success#Confirm success":"On success, the backup creates a gzip file, keycloak-postgres-backup-YYYYMMDDTHHMMSS.sql.gz.\nTo check that the backup succeeded, unzip the files and inspect the data.","next-steps#Next Steps":" Test the Restore Keycloak procedure to ensure you can recover data in case of an emergency. To back up other Rhize services, read how to backup: Grafana. The Graph Database. ","prerequisites#Prerequisites":"Before you start, ensure you have the following:\nA designated backup location, for example ~/rhize-backups/keycloak. Access to the Rhize Kubernetes Environment Optional: kubectx utilities kubectx to manage multiple clusters kubens to switch between and configure namespaces easily Optional: the k8 Lens IDE, if you prefer to use Kubernetes graphically Also, before you start, confirm you are in the right context and namespace.\n## context kubectl config current-context ## namespace kubectl get namespace To change the namespace for all subsequent kubectl commands to libre, run this command:\nkubectl config set-context --current --namespace=libre For a reference of useful kubectl commands, refer to the official kubectl Cheat Sheet.","steps#Steps":"To back up Keycloak, follow these steps:\nCheck the logs for the Keycloak pods, either in Lens or with kubectl logs. Ensure there are no errors.\nRetrieve the Keycloak user password using the following command, replacing with your namespace:\nkubectl get secret keycloak-\u003cnamespace\u003e-postgresql -o jsonpath=\"{.data.postgres-password}\" | base64 --decode Execute a command on the Keycloak Postgres pod to perform a full backup, replacing with your namespace:\nkubectl exec -i keycloak-\u003cNAMESPACE\u003e-postgresql-0 -- pg_dumpall -U postgres | gzip \u003e keycloak-postgres-backup-$(date +\"%Y%m%dT%I%M%p\").sql.gz When prompted, use the password from the previous step. Expect the prompt multiple times for each database.\nCheck the logs for the Keycloak Postgres pods, either in Lens or with kubectl logs. Ensure there are no errors relating to the backup."},"title":"Back up Keycloak"},"/deploy/install/":{"data":{"":"This guide shows you how to install Rhize services on your Kubernetes cluster.\nℹ️ This procedure aims to be as generic and vendor-neutral as possible. Some configuration depends on where and how you run your IT infrastructure—what cloud provider you use, preferred auxiliary tools, and so on—so your team must adapt the process for its particular use cases. OverviewA high-level overview of the Rhize install process. Set up KubernetesHow to install Rhize services on your Kubernetes cluster. Configure KeycloakThe Rhize GraphQL implementation uses OpenIDConnect for Authentication and role-based access control. This section describes how to set up Keycloak Install Rhize servicesInstructions to install services in the Rhize Kubernetes cluster. "},"title":"Install"},"/deploy/install/keycloak/":{"data":{"":"Rhize uses Keycloak as an OpenID provider. In your cluster, the Keycloak server to authenticate users, services, and manage Role-based access controls.\nThis topic describes how to set up Keycloak in your Rhize cluster. For a conceptual overview of the authentication flow, read About OpenID Connect","next-steps#Next steps":"Install services.","prerequisites#Prerequisites":"First, ensure that you have followed the instructions from Set up Kubernetes. All prerequisites for that step apply here.","steps#Steps":"Follow these steps to configure a Keycloak realm and associate Rhize services to Keycloak clients, groups, roles, and policies.\nLog in Go to localhost on the port where you forwarded the URL. If you used the example values from the last step, that’s localhost:5101.\nUse the container credentials to log in.\nTo find this, look in the keycloak.yaml file.\nCreate a realm A Keycloak realm is like a tenant that contains all configuration.\nTo create your Rhize realm, follow these steps.\nIn the side menu, select Master then Create Realm. For the Realm Name, enter libre. Create. In the side menu, select Realm Settings. Enter the following values: Field value Frontend URL Keycloak frontend URL Require SSL External requests After you’ve created the realm, you can create clients.\nℹ️ If created with the Libre Theme init container, configure the Login Theme in Realm settings for libre. Create clients In Keycloak, clients are entities that request Keycloak to authenticate a user. You need to create a client for each service.\nThe DB client requires additional configuration of flows and grants. Other clients, such as the UI and Dashboard, use the standard flow to coordinate authorization between the browser and Keycloak to simplify security and improve user convenience.\nℹ️ Each standard-flow client has its own subdomain. Refer to Default URLs and Ports for our recommended conventions. Create DB client Create a client for the DB as follows:\nIn the side menu, select Clients \u003e create client.\nConfigure the General Settings:\nClient Type: OpenID Connect Client ID: libreBaas Name: Libre Backend as a Service Description: Libre Backend as a Service When finished, select Next.\nConfigure the Capability config:\nClient Authentication: On Authorization: On For Authentication flow, enable: 🗸 Standard flow 🗸 Direct access grants 🗸 Implicit flow Select Next, then Save.\nOn success, this opens the Client details page for the newly created client.\nSelect the Service accounts roles tab and assign the following roles to the libreBaas service account. To locate roles, change the filter to Filter by clients:\nmanage-clients manage-account manage-users Create UI client Create a client for the UI as follows:\nIn the side menu, select Clients \u003e create client.\nConfigure the General Settings:\nClient Type: OpenID Connect Client ID: libreUI Name: Libre User Interface Description: Libre User Interface When finished, select Next.\nConfigure the Capability config:\nClient Authentication: On Authorization: On For Authentication flow, enable: 🗸 Standard flow 🗸 Direct access grants 🗸 Implicit flow Configure the Access Settings:\nRoot URL: \u003cUI_SUBDOMAIN\u003e.\u003cYOUR_DOMAIN\u003e without trailing slashes Home URL: \u003cUI_SUBDOMAIN\u003e.\u003cYOUR_DOMAIN\u003e without trailing slashes Web Origins: \u003cUI_SUBDOMAIN\u003e.\u003cYOUR_DOMAIN\u003e without trailing slashes Select Next, then Save.\nCreate dashboard client In the side menu, select Clients \u003e create client.\nConfigure the General Settings:\nClient Type: OpenID Connect Client ID: dashboard Name: Libre Dashboard Description: Libre Dashboard Configure the Capability config:\nClient Authentication: On Authorization: On For Authentication flow, enable: 🗸 Standard flow 🗸 Direct access grants 🗸 Implicit flow Configure the Access Settings:\nRoot URL: \u003cDASHBOARD_SUBDOMAIN\u003e.\u003cYOUR_DOMAIN\u003e without trailing slashes Home URL: \u003cDASHBOARD_SUBDOMAIN\u003e.\u003cYOUR_DOMAIN\u003e without trailing slashes Valid redirect URIs: \u003cDASHBOARD_URL\u003e/login/generic_oauth without trailing slashes Valid post logout redirect URIs: + without trailing slashes Home URL: \u003cDASHBOARD_SUBDOMAIN\u003e.\u003cYOUR_DOMAIN\u003e without trailing slashes Select Next, then Save.\nCreate other service clients The other services do not need authorization but do need client authentication. By default you need to add only the client ID.\nFor example, to create the BPMN engine client:\nIn the side menu, select Clients \u003e create client. For Client ID, enter libreBpmn Configure the Capability config: Client Authentication: On Select Next, then Save. Repeat this process for each of the following services:\nClient ID Description libreAudit The audit log service libreCore The edge agent libreRouter API router Based on your architecture, repeat for any Libre Edge Agents, libreAgent.\nScope services In Keycloak, a scope bounds the access a service has. Rhize creates a default client scope, then binds services to that scope.\nCreate a client scope To create a scope for your Rhize services, follow these steps:\nSelect Client Scopes \u003e Create client scope. Fill in the following values: Name: libreClientScope Description: Libre Client Scope Type: None Display on consent screen: On Include in token scope: On Create. Select the Mappers tab, then Configure new mapper. Add an audience mapper for the DB client: Mapper Type: Audience Name: libreBaasAudienceMapper Include Client Audience: libreBaas Add to ID Token: On Add to access token: On Repeat the preceding step for a mapper for the UI client: Mapper Type: Audience Name: libreUIAudienceMapper Include Client Audience: libreUI Add to ID Token: On Add to access token: Off Repeat the preceding step for a mapper for the BPMN client: Mapper Type: Audience Name: libreBPMNAudienceMapper Include Client Audience: libreBpmn Add to ID Token: On Add to access token: On If using the Rhize Audit microservice, repeat the preceding step for an Audit scope and audience mapper: Mapper Type: Audience Name: libreAuditAudienceMapper Include Client Audience: Included Custom Audience: audit Add to ID Token: On Add to access token: On Add services to the scope Go to Clients. Select libreBaas. Select the Client Scopes tab. Select Add Client scope Select libreClientScope from the list. Add \u003e Default. Repeat this process for the dashboard, libreUI, libreBpmn, libreCore, libreRouter, libreAudit (if applicable). Based on your architecture repeat for any Libre Edge Agent clients.\nCreate roles and groups In Keycloak, roles identify a category or type of user. Groups are a common set of attributes for a set of users.\nRhize creates an ADMIN role and group.\nAdd the admin realm role Select Realm Roles. Then Create role. Enter the following values: Role name: ADMIN Description: ADMIN Save. Add the Admin Group In the left hand menu, select Groups \u003e Create group. Give the group a name like libreAdminGroup. Create. Now map a role.\nFrom the group list, select the group you just created. Select the Role mapping tab. Select Assign Role Select ADMIN. Assign. Add the dashboard realm roles Select Realm Roles, and then Create role. Name the role dashboard-admin. Save. Repeat the process to create a role dashboard-dev. Add the dashboard groups In the left hand menu, select Groups, and then Create Group. Name the group dashboard-admin Create. Repeat the process to create dashboard-dev and dashboard-user groups. Now map the group to a role:\nSelect dashboard-admin from the list Select the Role mapping tab. Select Assign Role. Select dashboard-admin Assign. Repeat the process for dashboard-dev Add the group client scope In the left hand menu, select Client scopes and Create client scope. Name it groups and provide a description. Save. Now map the scope:\nSelect the Mappers tab. Add predefined mappers. Select groups. Add. Add new client scopes to dashboard client In the left hand menu, select Clients, and then dashboard. Select the Client scopes tab. Add client scope. Select groups and libreClientScope. Add Default. Add Client Policy In Keycloak, policies define authorization. Rhize requires authorization for the database service.\nIn the left hand menu, select Clients, and then libreBaas. Select the Authorization tab. Select Policies \u003e Create Policy Select Group \u003e Create Policy. Name the policy libreAdminGroupPolicy. Select Add Groups. Select libreAdminGroup. Add. For Logic, choose Positive. Save. Add users In the left hand menu, select Users, and Add User. Fill in the following values: Username: system@libremfg.ai. Email: system@libremfg.ai. Email Verified: On First name: system Last name: Libre Join Groups: libreAdminGroup Create. Now create a user password:\nSelect the Credentials tab. Set Password. Enter a strong password. For Temporary, choose Off. Save. Repeat this process for the following accounts:\nAudit: Username: libreAudit@libremfg.ai Email: libreAudit@libremfg.ai Email Verified: On First name: Audit Last name: Libre Join Groups: libreAdminGroup Core: Username: libreCore@libremfg.ai Email: libreCore@libremfg.ai Email Verified: On First name: Core Last name: Libre Join Groups: libreAdminGroup BPMN Username: libreBpmn@libremfg.ai Email: libreBpmn@libremfg.ai Email Verified: On First name: Bpmn Last name: Libre Join Groups: libreAdminGroup Router Username: libreRouter@libremfg.ai Email: libreRouter@libremfg.ai Email Verified: On First name: Router Last name: Libre Join Groups: libreAdminGroup Agent Username: libreAgent@libremfg.ai Email: libreAgent@libremfg.ai Email Verified: On First name: Agent Last name: Libre Join Groups: libreAdminGroup Enable Keycloak Audit Trail With the libre realm selected:\nSelect Realm Settings, and then Events. Select the tab User event settings. Enable Save Events and set an expiration. Save. Repeat the process for the Admin event settings tab. Configure password policy With the libre realm selected:\nSelect Authentication and then the Policies tab. Select the Password policy tab. Add your organisation’s password policy. Configure brute-force protections With the libre realm selected:\nSelect Realm settings and then the Security defenses tab. In Brute force detection, enable the feature and configure it to your requirements. "},"title":"Configure Keycloak"},"/deploy/install/overview/":{"data":{"":"This guide walks you through how to Install Rhize and its services in a Kubernetes environment. You can also use these docs to model automation workflows in your CI.\nThis procedure aims to be as generic and vendor-neutral as possible. Some configuration depends on where and how you run your IT infrastructure—what cloud provider you use, preferred auxiliary tools, and so on—so your team must adapt the process for its particular use cases.","condensed-instructions#Condensed instructions":"This guide has three steps, each of which has its own page. The essential procedure is as follows:\nSet up the Kubernetes environment.\nWithin a Kubernetes cluster, create a new namespace. In this namespace, add the Rhize container images and Helm repositories. In the cluster, create passwords. Use Helm to install Keycloak. Configure Keycloak.\nIn Keycloak, create a realm and clients for each service. In the cluster, create secrets for the KeyCloak clients. Install services.\nUse Helm to install libreBaas . In Keycloak, give libreBaas admin permissions. Use these admin permissions to POST the database schema. Return to Keycloak and add the newly created permissions to the libreBaas group. Use Helm to install all other services in this sequence: Install the service dependencies. Edit its YAML files to override defaults as needed. Install through Helm. "},"title":"Overview"},"/deploy/install/services/":{"data":{"":"","add-the-rhize-helm-chart-repository#Add the Rhize Helm Chart Repository":"","db#Install and add roles for the DB":"The final installation step is to install the Rhize services in your Kubernetes cluster.\nPrerequisites This topic assumes you have done the following:\nSet up Kubernetes and Configured Keycloak. All the prerequisites for those topics apply here.\nConfigured load balancing for the following DNS records:\nService Domain Admin UI rhize-ui.\u003cYOUR_DOMAIN\u003e Keycloak rhize-auth.\u003cYOUR_DOMAIN\u003e GraphQL rhize-api.\u003cYOUR_DOMAIN\u003e NATS rhize-mqtt.\u003cYOUR_DOMAIN\u003e Grafana rhize-grafana.\u003cYOUR_DOMAIN\u003e Note that rhize- is only the recommended prefix of the subdomain. Your organization may use something else.\nOverrides Each service is installed through a Helm YAML file. For some of these services, you might need to edit this file to add credential information and modify defaults.\nCommon values that are changed include:\nURLs and URL links The number of replicas running for each pod Ingress values for services exposed on the internet Get client secrets. Go to Keycloak and get the secrets for each client you’ve created.\nCreate Kubernetes secrets for each service. You can either create a secret file, or pass raw data from the command line.\nHow you create Kubernetes secrets depends on your implementation details and security procedures. For guidance, refer to the official Kubernetes topic, Managing Secrets using kubectl. With raw data, the command might look something like this.\nkubectl create secret generic libre-client-secrets \\ -n libre --from-literal=dashboard=\u003cUSER \\ --from-literal=libreAgent=123 \\ --from-literal=libreAudit=123 \\ --from-literal=libreBaas=KYbMHlRLhXwiDNFuDCl3qtPj1cNdeMSl \\ --from-literal=libreBPMN=123 \\ --from-literal=libreCore=123 \\ --from-literal=libreUI=123 \\ --from-literal=router=123 As you install services through Helm, their respective YAML files reference these secrets.\nAdd the Rhize Helm Chart Repository You must add the helm chart repository for Rhize.\nAdd the Helm Chart Repository\nhelm repo add libre https://gitlab.com/api/v4/projects/42214456/packages/helm/stable Install and add roles for the DB You must install the libreBaas database service first. You also need to configure the libreBaas service to have roles in Keycloak.\nIf enabling the Audit Trail, also the include the configuration in Enable change data capture.\nUse Helm to install the database:\nhelm install -f baas.yaml libre-baas libre/baas -n libre To confirm it works, run the following command:\nkubectl get pods All statuses should be RUNNING.\nReturn to the Keycloak UI and add all libre roles to the admin group.\nProxy the http:8080 port on libre-baas-dgraph-alpha.\nkubectl port-forward -n libre pod/baas-baas-alpha-0 8080:8080 Get a token using the credentials. With curl, it looks like this:\ncurl --location --request POST 'https://\u003ccustomer\u003e- auth.libre/realms/libre/protocol/openid-connect/token' \\ --header 'Content-Type: application/x-www-form-urlencoded' \\ --data-urlencode 'grant_type=password' \\ --data-urlencode 'username=system@libre.com' \\ --data-urlencode 'password=\u003cPASSWORD\u003e' \\ --data-urlencode 'client_id=libreBaas' \\ --data-urlencode 'client_secret=\u003cCLIENT_SECRET\u003e' Post the schema:\ncurl --location --request POST 'http://localhost:\u003cFORWARDED_PORT\u003e/admin/schema' \\ --header 'Authorization: Bearer \u003cTOKEN\u003e' \\ --header 'Content-Type: application/octet-stream' \\ --data-binary '@\u003cSCHEMA_FILE\u003e' This creates more roles.\nGo to Keycloak UI and add all new libreBaas roles to the ADMIN group.\nIf the install is successful, the Keycloak UI is available on its default port.","get-client-secrets#Get client secrets.":"","install-services#Install services":"Each of the following procedures installs a service through Helm.\nThe syntax to install a Rhize service must have arguments for the following:\nThe chart YAML file The packaged chart The path to the unpackaged chart or directory Additionally, use the -n flag to ensure that the install is scoped to the correct namespace:\nhelm install \u003cservice_name\u003e \\ -f \u003cservice-overide-file\u003e.yaml \\ \u003cpath/to/directory\u003e \\ -n \u003cns\u003e For the full configuration options, read the official Helm install reference.\nNATS NATS is the message broker that powers Rhize’s event-driven architecture.\nInstall NATS with these steps:\nModify the NATS Helm file with your code editor. Edit any necessary overrides.\nInstall with Helm:\nhelm install nats -f nats.yaml libre/nats -n libre Tempo Rhize uses Tempo to trace BPMN processes.\nInstall Tempo with these steps:\nIf it doesn’t exist, add the Tempo repository:\nhelm repo add grafana https://grafana.github.io/helm-charts Modify the Helm file as needed.\nInstall with Helm:\nhelm install tempo -f tempo.yaml grafana/tempo -n libre Core The Libre Core service is the custom edge agent that monitors data sources, like OPC-UA servers, and publishes and subscribes topics to NATS.\nRequirements: Core requires the libreBaas and NATS services.\nInstall the Core agent with these steps:\nIn the core.yaml Helm file, edit the clientSecret and password with settings from the Keycloak client.\nOverride any other values, as needed.\nInstall with Helm:\nhelm install core -f core.yaml libre/core -n libre BPMN The BPMN service is the custom engine Rhize uses to process low-code workflows modeled in the BPMN UI.\nRequirements: The BPMN service requires the libreBaas, NATS, and Tempo services.\nInstall the BPMN engine with these steps:\nOpen bpmn.yaml Update the clientSecret and password for your BPMN Keycloak credentials.\nModify any other values, as needed.\nInstall with Helm:\nhelm install bpmn -f bpmn.yaml libre/bpmn -n libre Router Rhize uses the Apollo router to unite queries for different services in a single endpoint.\nRequirements: Router requires the GraphDB, BPMN, and Core services.\nInstall the router with these steps:\nModify the router Helm YAML file as needed.\nInstall with Helm:\nhelm install router -f router.yaml libre/router -n libre If the install is successful, the Router explorer is available on its default port.\nGrafana Rhize uses Grafana for its dashboard to monitor real time data.\nInstall Grafana with these steps:\nModify the Grafana Helm YAML file as needed.\nAdd the Helm repository\nhelm repo add grafana https://grafana.github.io/helm-charts Install with Helm:\nhelm install grafana -f grafana.yaml grafana/grafana -n libre If the install is successful, the Grafana service is available on its default port.\nAgent The Rhize agent bridges your plant processes with the Rhize data hub. It collects data emitted from the plant and publishes it to the NATS message broker.\nRequirements: Agent requires the Graph DB, Nats, and Tempo services.\nInstall the agent with these steps:\nModify the Agent Helm file as needed.\nInstall with Helm:\nhelm install agent -f agent.yaml libre/agent -n libre ","install-ui#Install UI":"The UI is the graphical frontend to handle events and define work masters.\nRequirements: The UI requires the GraphDB, BPMN, Core, and Router services.\nAfter installing all other services, install the UI with these steps:\nForward the port from the Router API.\nOpen the UI Helm file. Update the envVars object with settings from the UI Keycloak client.\nModify any other values, as needed.\nInstall with Helm:\nhelm install ui -f ui-overrides.yaml libre/admin-ui -n libre If the install is successful, the UI is available on its default port.","optional-audit-trail-service#Optional: Audit Trail service":"The Rhize Audit service provides an audit trail for database changes to install. The Audit service uses PostgreSQL for storage.\nInstall Audit Service with these steps:\nModify the Audit trail Helm YAML file. It is recommended to change the PostgreSQL username and password values.\nInstall with Helm:\nhelm install audit -f audit.yaml libre/audit -n libre Create partition tables in the PostgreSQL database:\ncreate table public.audit_log_partition( like public.audit_log ); select partman.create_parent( p_parent_table := 'public.audit_log', p_control := 'time', p_interval := '1 Month', p_template_table := 'public.audit_log_partition'); For details about maintaining the Audit trail, read Archive the PostgresQL Audit trail.\nEnable change data capture The Audit trail requires change data capture (CDC) to function. To enable CDC in libre BAAS, include the following values for the Helm chart overrides:\nalpha: # Change Data Capture (CDC) cdc: # Enable enabled: true # If configured for security, configure in NATS url. For example `nats://username:password@nats:4222` nats: nats://nats:4222 # Adjust based on high-availability requirements and cluster size. replicas: 1 Enable Audit subgraph To use the Audit trail in the UI, you must add the Audit trail subgraph into the router. To enable router to use and compose the subgraph:\nUpdate the Router Helm chart overrides, router.yaml, to include: # Add Audit to the router subgraph url override router: configuration: override_subgraph_url: AUDIT: http://audit:8084/query # If supergraph compose is enabled supergraphCompose: supergraphConfig: subgraphs: AUDIT: routing_url: http://audit:8084/query schema: subgraph_url: http://audit:8084/query Update the Router deployment $ helm upgrade --install router -f router.yaml libre/router -n libre ","optional-calendar-service#Optional: calendar service":"The Libre calendar service monitors work calendar definitions and creates work calendar entries in real time, both in the Graph and time-series databases.\nRequirements: The calendar service requires the GraphDB, Keycloak, and NATS services.\nℹ️ The work calendar requires a time-series DB installed such as InfluxDB, QuestDB or TimescaleDB. The following instructions are specific to QuestDB. Install the calendar service with these steps:\nCreate tables in the time series. For example:\nCREATE TABLE IF NOT EXISTS PSDT_POT( EquipmentId SYMBOL, EquipmentVersion STRING, WorkCalendarId STRING, WorkCalendarIid STRING, WorkCalendarDefinitionId STRING, WorkCalendarDefinitionEntryId STRING, WorkCalendarDefinitionEntryIid STRING, WorkCalendarEntryId STRING, WorkCalendarEntryIid SYMBOL, HierarchyScopeId STRING, EntryType STRING, ISO22400CalendarState STRING, isDeleted boolean, updatedAt TIMESTAMP, time TIMESTAMP, lockerCount INT, lockers STRING ) TIMESTAMP(time) PARTITION BY month DEDUP UPSERT KEYS(time, EquipmentId, WorkCalendarEntryIid); CREATE TABLE IF NOT EXISTS PDOT_PBT( EquipmentId SYMBOL, EquipmentVersion STRING, WorkCalendarId STRING, WorkCalendarIid STRING, WorkCalendarDefinitionId STRING, WorkCalendarDefinitionEntryId STRING, WorkCalendarDefinitionEntryIid STRING, WorkCalendarEntryId STRING, WorkCalendarEntryIid SYMBOL, HierarchyScopeId STRING, EntryType STRING, ISO22400CalendarState STRING, isDeleted boolean, updatedAt TIMESTAMP, time TIMESTAMP, lockerCount INT, lockers STRING ) TIMESTAMP(time) PARTITION BY month DEDUP UPSERT KEYS(time, EquipmentId, WorkCalendarEntryIid); CREATE TABLE IF NOT EXISTS Calendar_AdHoc( EquipmentId SYMBOL, EquipmentVersion STRING, WorkCalendarId STRING, WorkCalendarIid STRING, WorkCalendarDefinitionId STRING, WorkCalendarDefinitionEntryId STRING, WorkCalendarDefinitionEntryIid STRING, WorkCalendarEntryId STRING, WorkCalendarEntryIid SYMBOL, HierarchyScopeId STRING, EntryType STRING, ISO22400CalendarState STRING, isDeleted boolean, updatedAt TIMESTAMP, time TIMESTAMP, lockerCount INT, lockers STRING ) TIMESTAMP(time) PARTITION BY month DEDUP UPSERT KEYS(time, EquipmentId, WorkCalendarEntryIid); Modify the calendar YAML file as needed.\nDeploy with helm\nhelm install calendar-service -f calendar-service.yaml libre/calendar-service -n libre ","optional-change-service-configuration#Optional: change service configuration":"The services installed in the previous step have many parameters that you can configure for your performance and deployment requirements. Review the full list in the Service configuration reference.","prerequisites#Prerequisites":"","troubleshoot#Troubleshoot":"For general Kubernetes issues, the Kubernetes dashboard is great for troubleshooting, and you can configure it to be accessible through the browser.\nFor particular problems, try these commands:\nIs my service running?\nTo check deployment status, use this command:\nkubectl get deployments Look for the pod name and its status.\nAccess service through browser\nSome services are accessible through the browser. To access them, visit local host on the service’s default port.\nI installed a service too early. If you installed a service too early, use Helm to uninstall:\nhelm uninstall libreBaas Then perform the steps you need and reinstall when ready."},"title":"Install Rhize services"},"/deploy/install/setup-kubernetes/":{"data":{"":"","next-steps#Next steps":" Configure Keycloak Install services. ","prereqs#Prerequisites":"This guide shows you how to install Rhize services on your Kubernetes cluster. You can also use this procedure as the model for an automation workflow in your CI.\nPrerequisites Before starting, ensure that you have the following technical requirements.\nSoftware requirements:\nkubectl Helm Curl, or some similar program to make HTTP requests from the command line Access requirements:\nAdministrative privileges for a running Kubernetes cluster in your environment. Your organization must set this up. Access to Rhize Helm charts and its build repository. Rhize provides these to all customers. Optional utilities. For manual installs, the following auxiliary tools might make the experience a little more human friendly:\nOptional: kubectx utilities\nkubectx to manage multiple clusters kubens to switch between and configure namespaces easily Optional: the k8 Lens IDE, if you prefer to use Kubernetes graphically\nAgain, these are helpers, not requirements. You can install everything with only the kubectl and helm commands.","steps-to-set-up-kubernetes#Steps to set up Kubernetes":"First, record your site and environment. Then, follow these steps.\nCreate a namespace called libre.\nkubectl create ns libre Confirm it works with kubectl get ns.\nOn success, the output shows an active libre namespace.\nSet this namespace as a default with\nkubectl config set-context --current --namespace=libre Alternatively, you can modify the kube config file or use the kubens tool.\nAdd the Helm repository:\nhelm repo add \\ --username \u003cEMAIL_ADDRESS\u003e \\ --password \u003cACCESS_TOKEN\u003e \\ libre \\ \u003cREPO\u003e Create the container image pull secret:\nkubectl create secret docker-registry libre-registry-credential \\ --docker-server=\u003cDOCKER_SERVER\u003e \\ ## the repository --docker-password= \u003cACCESS_TOKEN\u003e \\ --docker-email= \u003cEMAIL_ADDRESS\u003e Confirm the secrets with this command:\nkubectl get secrets Add the Bitnami Helm repository:\nhelm repo add bitnami https://charts.bitnami.com/bitnami Pull the build template repository (we will supply this).\nUpdate overrides to keycloak.yaml. Then install with this command:\nhelm install keycloak -f ./keycloak.yaml bitnami/keycloak -n libre Set up port forwarding from Keycloak. For example, this forwards traffic to port 5101 on localhost\nkubectl port-forward svc/keycloak 5101:80 "},"title":"Set up Kubernetes"},"/deploy/maintain/":{"data":{"":"Maintenance is critical to ensure reliability over time.\nThese guides show you how to maintain different services and data on Rhize. They also serve as blueprints for automation.\nYour organization must determine how you maintain your services, and how often you archive or remove data. The correct practice here is highly contextual, depending on the size of the data, the importance of the data, and the general regulatory and governance demands of your industry.\nArchive the PostgreSQL Audit trailHow to archive a partition of the Audit trail on your Rhize deployment BPMN execution recoveryIf a BPMN node suddenly fails, Rhize has a number of recovery methods to ensure that the workflow finishes executing. Export Keycloak eventsGuide to export events from Keycloak "},"title":"Maintain"},"/deploy/maintain/audit/":{"data":{"":"The audit trial can generate a high volume of data, so it is a good practice to periodically archive portions of it. An archive separates a portion of the data from the database and keeps it for long-term storage. This process involves the use of PostgreSQL Table Partitions.\nArchiving a partition improves query speed for current data, while providing a cost-effective way to store older.","next-steps#Next Steps":" For full backups or Rhize services, read how to back up: Keycloak The Audit trail Grafana The Graph Database ","prerequisites#Prerequisites":"Before you start, ensure you have the following:\nA designated backup location, for example ~/rhize-archives/libre-audit. Access to the Rhize Kubernetes Environment - Optional: kubectx utilities kubectx to manage multiple clusters kubens to switch between and configure namespaces easily Optional: the k8 Lens IDE, if you prefer to use Kubernetes graphically Also, before you start, confirm you are in the right context and namespace.\n## context kubectl config current-context ## namespace kubectl get namespace To change the namespace for all subsequent kubectl commands to libre, run this command:\nkubectl config set-context --current --namespace=libre For a reference of useful kubectl commands, refer to the official kubectl Cheat Sheet.","steps#Steps":"To archive the PostgreSQL Audit trail, follow these steps:\nRecord the \u003cPARTITION_NAME\u003e of the partition you wish to detach and archive. This is based on the retention-period query for the names of the existing partitions:\nkubectl exec -i audit-postgres-0 -- psql -h localhost \\ -d audit -U \u003cDB_USER\u003e \\ -c \"select * from partman.show_partitions('public.audit_log')\" Detach the target partitions from the main table:\nkubectl exec -i audit-postgres-0 -- psql -h localhost \\ -d audit -U \u003cDB_USER\u003e \\ -c 'alter table audit_log detach partition \u003cPARTITION_NAME\u003e;' Backup the partition table:\npg_dump -U \u003cDB_USER\u003e -h audit-postgres-0 -p5433 \\ --file ./audit-p20240101.sql --table public.audit_log_p20240101 audit On success, the backup creates a GZIP file, \u003cPARITION_NAME\u003e.sql. To check that the backup succeeded, unzip the files and inspect the data.\nDrop the partition table to remove it from the database:\nkubectl exec -i audit-postgres-0 -- psql -h localhost -d audit \\ -U \u003cDB_USER\u003e -c 'drop table \u003cPARTITION_NAME\u003e;' "},"title":"Archive the PostgreSQL Audit trail"},"/deploy/maintain/bpmn-nodes/":{"data":{"":"","bpmn-age-out#All BPMN elements age out after ten minutes":"BPMN processes often have longer execution durations and many steps. If a BPMN node suddenly fails (for example through a panic or loss of power), Rhize needs to ensure that the workflow completes.\nTo achieve high availability and resiliency, Rhize services run in Kubernetes nodes, and the NATS message broker typically has data replication. As long as the remaining BPMN nodes are not already at full processing capacity, if a BPMN node fails while executing a process, the Rhize system recovers and finishes the workflow.\nThis recovery is automatic, though users may experience an execution gap of up to 30 seconds.\nBPMN failure and recovery modes How Rhize recovers from a halted process depends on where the system failed.\nBPMN node failure If a BPMN container suddenly fails, the process that was currently executing times out after 30 seconds. As long as the node had not been running for longer than 10 minutes, NATS re-sends the message to another BPMN node and the process finishes.\nNATS node unavailable If the NATS node fails, recovery depends on your replication and backup strategy.\nIf the stream has R3 replication or greater, a new NATS node picks up the process. No noticeable performance issues should occur.\nIf the stream has no replication, everything in the node is lost. However, if you took a snapshot of a stream with nats stream backup before the node became unavailable, and the WorkflowSpecifications KV is the same at backup and restore sites, then you can use the nats stream restore command to replay the stream from when the backup was made.\nTo learn more, read the NATS topic on Disaster recovery.\nAll BPMN elements age out after ten minutes If an element in a BPMN workflow takes longer than 10 minutes, NATS ages the workflow out of the queue. The process continues, but if the pod executing the element dies or is interrupted, that workflow is permanently dropped.\nThis ten-minute execution limit should be sufficient for any element in a BPMN process. Processes that take longer, such as cooling or fermentation periods, should be implemented as BPMN event triggers or as polls that periodically check data sources between intervals of sleep.","bpmn-failure-and-recovery-modes#BPMN failure and recovery modes":""},"title":"BPMN execution recovery"},"/deploy/maintain/keycloak-events/":{"data":{"":"Keycloak stores User and Admin event data in its database. This information can be valuable for your audits.\nThis guide shows you how to export your Keycloak events to a file. To read Keycloak event data, use its Admin CLI. You can access the CLI from within the Keycloak’s container.","prerequisites#Prerequisites":"Ensure you have the following:\nThe ability to run commands in a Keycloak container or pod. A Keycloak admin username and password. ","procedure#Procedure":"To export Keycloak events, first open a shell in your Keycloak container or pod. For example, in Kubernetes and Docker:\nKubernetesDocker kubectl exec -it keycloak_pod_name -n namespace_name -- /bin/sh docker exec -it keycloak_container_name /bin/sh Then follow these steps:\nChange to the directory where the script for the Admin CLI is. This directory is by default /opt/bitnami/keycloak/bin. Run ./kcadm.sh get realms/libre/events --server http://localhost:8080 --realm master --user \u003cADMIN\u003e. Replace \u003cADMIN\u003e with the Keycloak admin username. If the Keycloak port differs from the default, replace :8080 with the configured port number. When prompted, enter the Keycloak admin password. On success, event data prints to the console.","write-event-data-to-file#Write event data to file":"The event output can be long. You can use the following commands write the data to a file (replacing \u003cADMIN_PW\u003e with the Keycloak admin password).\nKubernetesDocker kubectl exec -it keycloak_pod_name -n namespace_name -- \\ /bin/sh -c \"cd /opt/bitnami/keycloak/bin \u0026\u0026 (echo \"\u003cADMIN_PW\u003e\" \\ | ./kcadm.sh get realms/libre/events --server http://localhost:8080 \\ --realm master --user admin)\" \\ | sed '1,2d' \u003e output.json docker exec -it keycloak_container_name \\ /bin/sh -c \"cd /opt/bitnami/keycloak/bin \u0026\u0026 (echo \"\u003cADMIN_PW\u003e\" \\ | ./kcadm.sh get realms/libre/events --server http://localhost:8080 \\ --realm master --user admin)\" \\ | sed '1,2d' \u003e output.json "},"title":"Export Keycloak events"},"/deploy/restore/":{"data":{"":"These guides show you how to restore data from backup. They also serve as blueprints for automation.\nEven if you don’t need to restore data, it’s a good practice to test restoration periodically.\nRestore the GraphDBHow to restore a backup of the Rhize Graph DB. Restore the GraphDB from S3How to restore a backup of the Rhize Graph DB from Amazon S3. Restore Audit backupHow to restore the backup of the Audit PostgreSQL on your Rhize deployment Restore KeycloakHow to restore a Keycloak backup on Rhize Restore GrafanaHow to restore a Grafana backup on Rhize "},"title":"Restore"},"/deploy/restore/audit/":{"data":{"":"This guide shows you the procedure to restore your Audit PostgreSQL database in your Rhize Kubernetes deployment.","next-steps#Next Steps":" Test the Backup Audit procedure Plan and execute a Maintenance Strategy to handle your audit data. ","prerequisites#Prerequisites":"Before you start, ensure you have the following:\nkubectl An Audit PostgreSQL backup Also, before you start, confirm you are in the right context and namespace.\n## context kubectl config current-context ## namespace kubectl get namespace To change the namespace for all subsequent kubectl commands to libre, run this command:\nkubectl config set-context --current --namespace=libre For a reference of useful kubectl commands, refer to the official kubectl Cheat Sheet.","steps#Steps":"To restore Audit PostgreSQL, follow these steps:","steps-1#Steps":" Confirm the cluster and namespace are correct:\n## context kubectl config current-context ## namespace kubectl get namespace To change the namespace for all subsequent kubectl commands to libre, run this command:\nkubectl config set-context --current --namespace=libre For a reference of useful kubectl commands, refer to the official kubectl Cheat Sheet.\nRetrieve the Audit user password using the following command:\nkubectl get secret \u003cSECRET-NAME\u003e -o jsonpath=\"{.data.\u003cSECRET-KEY\u003e}\" | base64 --decode Extract your backup file:\ngzip -d audit-postgres-backup-YYYYMMDDTHHMMAA.sql Restore the backup:\ncat audit-postgres-backup-YYYYMMDDTHHMMAA.sql | kubectl exec -i audit-postgres-0 -- psql postgresql://postgres:\u003cDB_PASSWORD\u003e@localhost:5432 -U \u003cDB_USER_NAME\u003e "},"title":"Restore Audit backup"},"/deploy/restore/binary/":{"data":{"":"This guide shows you how to restore the Graph database from Amazon S3 to your Rhize environment.","prerequisites#Prerequisites":"Before you start, ensure you have the following:\nThe GraphDB Helm chart kubectl A Database backup ","steps#Steps":" Set the follow environmental variables:\nAWS_ACCESS_KEY_ID your AWS access key with permissions to write to the destination bucket AWS_SECRET_ACCESS_KEY your AWS access key with permissions to write to the destination bucket AWS_SESSION_TOKEN your AWS session token (if required) Confirm the cluster and namespace are correct.\n## context kubectl config current-context ## namespace kubectl get namespace To change the namespace for all subsequent kubectl commands to libre, run this command:\nkubectl config set-context --current --namespace=libre For a reference of useful kubectl commands, refer to the official kubectl Cheat Sheet.\nUpgrade or install the Helm chart.\nhelm upgrade --install -f baas.yaml libre-baas libre/baas -n libre Wait for libre-baas-alpha-0 to start serving the GraphQL API.\nMake a POST request to your Keycloak /token endpoint to get an access_token value. For example, with curl and jq:\n## replace USERNAME and PASSWORD with your credentials USERNAME=backups@libremfg.com \\ \u0026\u0026 PASSWORD=password \\ \u0026\u0026 curl --location \\ --request POST \"${BAAS_OIDC_URL}/realms/libre/protocol/openid-connect/token\" \\ --header 'Content-Type\\ application/x-www-form-urlencoded' \\ --data-urlencode 'grant_type=password' \\ --data-urlencode \"username=\u003cUSERNAME\u003e\" \\ --data-urlencode \"password=\u003cPASSWORD\u003e\" \\ --data-urlencode \"client_id=\u003cBASS_CLIENT_ID\u003e\" \\ --data-urlencode \"client_secret=\u003cBASS_CLIENT_SECRET\u003e\" | jq .access_token Using the token from the previous step, send a POST to \u003calpha pod\u003e:8080/admin to retrieve a list of available backups from the s3 bucket.\ncurl --location 'http://alpha-0:8080/admin' \\ --header 'Content-Type: application/json' \\ --header 'Authorization: Bearer \u003cTOKEN\u003e' \\ --data '{\"query\":\"query {\\n\\tlistBackups(input: {location: \\\"s3://s3.\u003cAWS-REGION\u003e.amazonaws.com/\u003cAWS-BUCKET-NAME\u003e\\\"}) {\\n\\t\\tbackupId\\n\\t\\tbackupNum\\n\\t\\tencrypted\\n\\t\\tpath\\n\\t\\tsince\\n\\t\\ttype\\n readTs\\n\\t}\\n}\",\"variables\":{}}' Using the backup id and token from the previous step, send a POST to \u003calpha pod\u003e:8080/admin to start the restore from the s3 bucket to the alpha node. For example, with curl:\ncurl --location 'http://alpha-0:8080/admin' \\ --header 'Content-Type: application/json' \\ --header 'Authorization: Bearer \u003cTOKEN\u003e' \\ --data '{\"query\":\"mutation{\\n restore(input:{\\n location: \\\"s3://s3.\u003cAWS-REGION\u003e.amazonaws.com/\u003cAWS-BUCKET-NAME\u003e\\\",\\n backupId: \\\"\u003cBACKUP_ID\u003e\\\"\\n }){\\n message\\n code\\n }\\n}\",\"variables\":{}}' "},"title":"Restore the GraphDB from S3"},"/deploy/restore/grafana/":{"data":{"":"This guide shows you how to restore Grafana in your Rhize environment.","prerequisites#Prerequisites":"Before you start, ensure you have the following:\nkubectl A Grafana backup ","steps#Steps":" Confirm the cluster and namespace are correct:\n## context kubectl config current-context ## namespace kubectl get namespace To change the namespace for all subsequent kubectl commands to libre, run this command:\nkubectl config set-context --current --namespace=libre For a reference of useful kubectl commands, refer to the official kubectl Cheat Sheet.\nIf a checksum file does not exist for the latest backups, create one:\nsha256sum \u003cLATEST_DATA_FILE\u003e.tar.gz \u003cLATEST_CONF_FILE\u003e.tar.gz \u003e backup.sums Copy the checksum file into the new Grafana Pod within the /home/grafana directory:\nkubectl cp ./backup.sums \\ \u003cGRAFANA_POD_NAME\u003e:/home/grafana Copy the Grafana data tar file into the new Grafana Pod within the /home/grafana directory:\nkubectl cp ./\u003cLATEST_DATA_FILE\u003e.tar.gz \\ \u003cGRAFANA_POD_NAME\u003e:/home/grafana Copy the Grafana configuration tar file into the new Grafana Pod within the /home/grafana directory:\nkubectl cp ./\u003cLATEST_CONF_FILE\u003e.tar.gz \\ \u003cGRAFANA_POD_NAME\u003e:/home/grafana Confirm that the checksums match:\nkubectl exec -it \u003cGRAFANA_POD_NAME\u003e -- /bin/bash \u003cGRAFANA_POD_NAME\u003e:~$ cd /home/grafana \u003cGRAFANA_POD_NAME\u003e:~$ sha256sum -c backup.sums ./\u003cLATEST_DATA_FILE\u003e.tar.gz: OK ./\u003cLATEST_CONF_FILE\u003e.tar.gz: OK Untar the data file:\ntar -xvf \u003cLATEST_DATA_FILE\u003e.tar.gz --directory / Untar the configuration file:\ntar -xvf \u003cLATEST_CONF_FILE\u003e.tar.gz --directory /home/grafana/ Move over the top of current configuration.\nℹ️ Typically some files are configured as a Kubernetes ConfigMap and may need to be configured as part of installation. The following command prompts when it is going to overwrite a file, and if it has the permissions to do so. mv /home/grafana/usr/share/grafana/conf/* /usr/share/grafana/conf/ Remove restore files and directory\nrm /home/grafana/\u003cLATEST_DATA_FILE\u003e.tar.gz rm /home/grafana/\u003cLATEST_CONF_FILE\u003e.tar.gz rm /home/grafana/backup.sums rm -r /home/grafana/usr Restart the Grafana Deployment.\nkubectl rollout restart deployment grafana -n libre "},"title":"Restore Grafana"},"/deploy/restore/graphdb/":{"data":{"":"This guide shows you how to restore the Graph database in your Rhize environment.","prerequisites#Prerequisites":"Before you start, ensure you have the following:\nThe GraphDB Helm chart kubectl A Database backup ","steps#Steps":" Confirm the cluster and namespace are correct.\n## context kubectl config current-context ## namespace kubectl get namespace To change the namespace for all subsequent kubectl commands to libre, run this command:\nkubectl config set-context --current --namespace=libre For a reference of useful kubectl commands, refer to the official kubectl Cheat Sheet.\nChange to the libre-baas helm chart overrides, baas.yaml. Set alpha.initContainers.init.enable to true.\nUpgrade or install the Helm chart.\nhelm upgrade --install -f baas.yaml libre-baas libre/baas -n libre In the Alpha 0 initialization container, create the backup directory.\nkubectl exec -t libre-baas-alpha-0 -c libre-baas-alpha-init -- \\ mkdir -p /dgraph/backups If the backup directory does not have a checksums file, create one.\nsha256sum ./\u003cPATH_TO_BACKUP\u003e/*.gz \u003e ./\u003cPATH_TO_BACKUP\u003e/backup.sums Copy the backup into the initialization container.\nkubectl cp --retries=10 ./\u003cPATH_TO_BACKUP\u003e \\ libre-baas-alpha-0:/dgraph/backups/\u003cPATH_TO_BACKUP\u003e \\ -c libre-baas-alpha-init After the process finishes, confirm that the checksums match:\nkubectl exec -it libre-baas-alpha-0 -c libre-baas-alpha-init -- \\ 'sha256sum -c /dgraph/backups/\u003cPATH_TO_BACKUP\u003e/backup.sums /dgraph/backups/\u003cPATH_TO_BACKUP\u003e/*.gz' Restore the backup to the restore directory. Replace the \u003cPATH_TO_BACKUP\u003e and \u003cNAMESPACE\u003e in the arguments for the following command:\nkubectl exec -t libre-baas-alpha-0 -c libre-baas-alpha-init -- \\ dgraph bulk -f /dgraph/backups/\u003cPATH_TO_BACKUP\u003e/g01.json.gz \\ -g /dgraph/backups/\u003cPATH_TO_BACKUP\u003e/g01.gql_schema.gz \\ -s /dgraph/backups/\u003cPATH_TO_BACKUP\u003e/g01.schema.gz \\ --zero=libre-baas-zero-0.libre-baas-zero-headless.\u003cNAMESPACE\u003e.svc.cluster.local:5080 \\ --out /dgraph/restore --replace_out Copy the backup to the correct directory:\nkubectl exec -t libre-baas-alpha-0 -c libre-baas-alpha-init -- \\ mv /dgraph/restore/0/p /dgraph/p Complete the initialization container for alpha 0.\nkubectl exec -t libre-baas-alpha-0 -c libre-baas-alpha-init -- touch /dgraph/doneinit Wait for libre-baas-alpha-0 to start serving the GraphQL API.\nMake a database mutation to force a snapshot to be taken. For example, create a UnitOfMeasure then delete it:\nkubectl exec -t libre-baas-alpha-0 -c libre-baas-alpha -- \\ curl --location --request POST 'http://localhost:8080/graphql' \\ --header 'Content-Type: application/json' \\ --data-raw '{\"query\":\"mutation RestoringDatabase($input:[AddUnitOfMeasureInput!]!){\\r\\n addUnitOfMeasure(input:$input){\\r\\n unitOfMeasure{\\r\\n id\\r\\n dataType\\r\\n code\\r\\n }\\r\\n}\\r\\n}\",\"variables\":{\"input\":[{\"code\":\"Restoring\",\"isActive\":true,\"dataType\":\"BOOL\"}]}}' Wait until you see libre-baas creating a snapshot in the logs. For example:\n$ kubectl logs libre-baas-alpha-0 ++ hostname -f ++ awk '{gsub(/\\.$/,\"\"); print $0}' ... I0314 20:32:21.282271 19 draft.go:805] Creating snapshot at Index: 16, ReadTs: 9 Revert any database mutations:\nkubectl exec -t libre-baas-alpha-0 -c libre-baas-alpha -- \\ curl --location --request POST 'http://localhost:8080/graphql' \\ --header 'Content-Type: application/json' \\ --data-raw '{\"query\":\"mutation {\\r\\n deleteUnitOfMeasure(filter:{code:{eq:\\\"Restoring\\\"}}){\\r\\n unitOfMeasure{\\r\\n id\\r\\n }\\r\\n }\\r\\n}\",\"variables\":{\"input\":[{\"code\":\"Restoring\",\"isActive\":true,\"dataType\":\"BOOL\"}]}}' Complete the initialization container for alpha 1:\nkubectl exec -t libre-baas-alpha-1 -c libre-baas-alpha-init -- \\ touch /dgraph/doneinit And alpha 2:\nkubectl exec -t libre-baas-alpha-2 -c libre-baas-alpha-init -- \\ touch /dgraph/doneinit "},"title":"Restore the GraphDB"},"/deploy/restore/keycloak/":{"data":{"":"This guide shows you how to restore Keycloak in your Rhize environment.\nRestoring Keycloak to a running instance involves downtime.\nTypically, this downtime lasts less than a minute. The exact duration needed depends on network constraints, backup size, and the performance of the Kubernetes cluster.","prerequisites#Prerequisites":"Before you start, ensure you have the following:\nkubectl A Keycloak backup ","steps#Steps":" Confirm the cluster and namespace are correct:\n## context kubectl config current-context ## namespace kubectl get namespace To change the namespace for all subsequent kubectl commands to libre, run this command:\nkubectl config set-context --current --namespace=libre For a reference of useful kubectl commands, refer to the official kubectl Cheat Sheet.\nRetrieve the Keycloak user password using the following command, replacing \u003cNAMESPACE\u003e with your namespace:\nkubectl get secret keycloak-\u003cNAMESPACE\u003e-postgresql -o jsonpath=\"{.data.postgres-password}\" | base64 --decode Extract your backup file:\ngzip -d keycloak-postgres-backup-YYYYMMDDTHHMMAA.sql To prevent new records from being created while the backup is restored, scale down the Keycloak replicas to 0. Keycloak will be unavailable after this command.\nkubectl scale statefulsets keycloak --replicas=0 Scale down the replicas of PostgreSQL to 0, so that existing persistent volume claims and persistent volumes can be removed:\nkubectl scale statefulsets keycloak-postgresql --replicas=0 Remove the Postgres persistent volume claim:\nkubectl delete pvc data-keycloak-postgresql-0 Identify the Keycloak Postgres volumes:\nkubectl get pv | grep keycloak This displays a list of persistent volume claims related to Keycloak. For example:\npvc-95176bc4-88f4-4178-83ab-ee7b256991bc 10Gi RWO Delete Terminating libre/data-keycloak-postgresql-0 hostpath 48d Note the names of the ´pvc-*` items. You’ll need them for the next step.\nRemove the persistent volumes with this command, replacing \u003cPVC_FROM_PREVIOUS_STEP\u003e with the pvc-* name from the previous step:\n$ kubectl delete pv \u003cPVC_FROM_PREVIOUS_STEP\u003e Scale up the replicas of PostgreSQL to 1:\nkubectl scale statefulsets keycloak-postgresql --replicas=1 Restore the backup:\ncat keycloak-postgres-backup-YYYYMMDDTHHMMAA.sql | kubectl exec -i keycloak-postgresql-0 -- psql postgresql://postgres:\u003cyour-postgres-password\u003e@localhost:5432 -U postgres Scale up the replicas of Keycloak to 1:\nkubectl scale statefulsets keycloak --replicas=1 Proxy the web portal of Keycloak:\nkubectl port-forward svc/keycloak 5101:80 Confirm access by checking http://localhost:80."},"title":"Restore Keycloak"},"/deploy/upgrade/":{"data":{"":"This guide shows you how to upgrade Rhize.\nBack up first Before upgrading, consider backing up your Rhize services. To review the procedure, read our Back up guides. ","prerequisites#Prerequisites":"Before you start, ensure you have the following:\nAccess to the Rhize Kubernetes Environment helm Optional: kubectx utilities kubectx to manage multiple clusters kubens to switch between and configure namespaces easily Optional: the k8 Lens IDE, if you prefer to use Kubernetes graphically Be sure that you notify relevant parties of the coming upgrade.","procedure#Procedure":"First, record the old and new versions, their context, and namespaces.\nCheck the logs for the libre pods, either in Lens or with kubectl logs. Ensure there are no errors.\nUse Git to pull your Rhize customer build directory.\nChange to the kubernetes/charts/libre directory.\nCheck your Kubernetes context and namespace.\n## context kubectl config current-context ## namespace kubectl get namespace To change the namespace for all subsequent kubectl commands to libre, run this command:\nkubectl config set-context --current --namespace=libre For a reference of useful kubectl commands, refer to the official kubectl Cheat Sheet.\nUse the helm list command to check for libre services.\nUpgrade with the following command:\nhelm upgrade libre -f \u003cNAMESPACE\u003e.yaml -n namespace Get a token using your credentials. With curl, it looks like this:\ncurl --location --request POST 'https://\u003cAUTH_SUBDOMAIN\u003e-\u003cYOUR_DOMAIN\u003e auth.libre/realms/libre/protocol/openid-connect/token' \\ --header 'Content-Type: application/x-www-form-urlencoded' \\ --data-urlencode 'grant_type=password' \\ --data-urlencode 'username=system@libre.com' \\ --data-urlencode 'password=\u003cPASSWORD\u003e' \\ --data-urlencode 'client_id=libreBaas' \\ --data-urlencode 'client_secret=\u003cCLIENT_SECRET\u003e' Redeploy the schema. To do so, you need to interact with the alpha service on port 8080. You can do this in multiple ways. Either enter the alpha shell with a command such as kubectl exec --stdin baas-alpha-0 -- sh, or forward the port to your local instance using a command such as kubectl port-forward baas-alpha-0 8080:8080.\nFor example, using port forwarding, a curl command to deploy the schema looks like this:\ncurl --location -X POST 'http://localhost:\u003cFORWARDED_PORT\u003e/admin/schema' \\ -H \"Authorization: Bearer $\u003cTOKEN\u003e\" \\ -H \"content-Type: application/octet-stream\" \\ --data-binary \u003cPATH/TO/SCHEMA\u003e.sdl The schema file is likely called something like schema.sdl.\nRestart the deployment.","verify-success#Verify success":"Verify success in Kubernetes by checking that the version upgraded properly and that the logs are correct.\nInform your team that the upgrade was successful."},"title":"Upgrade"},"/explanations/":{"data":{"":"The design of the Rhize Manufacturing Data Hub comes from decades of work in industrial automation. Rhize is designed to scale to the largest organizations, integrate with legacy systems, and address the complex reality of manufacturing head on. These to do this, these are our explicit design goals:\nProvide a highly reliable means to collect manufacturing data from varied sources. Standardize this data in a way that accurately places it within an event in the context of an entire manufacturing operation. Offer a programmable engine to write custom logic to process the data and send it between systems. Support being built on top of it, where Rhize serves as the backend for MES/MOM frontends. Expose this system through an interface that is accessible to the widest number of stakeholders in the manufacturing operation. These following topics explain the concepts, architecture, and philosophy that underpin the Rhize application:\nManufacturing Data HubWhat is a Manufacturing Data Hub? Why is it necessary? How the design of Rhize meets the needs of modern manufacturing. ISA-95 diagramsHelpful diagrams to present a high-level overview of the ISA-95 models for entities and information flows. How to speak ISA-95More than a standard, ISA-95 is a specialized vocabulary that describes all elements of manufacturing operation. "},"title":"Explanations"},"/explanations/how-to-speak-isa-95/":{"data":{"":"","foundational-concepts#Foundational concepts":"","frequently-used-models#Frequently used models":"","now-youre-talking#Now you\u0026rsquo;re talking":"ISA-95 provides a common language to discuss manufacturing. When you speak with other manufacturing stakeholders, you can use the standard’s precise vocabulary to ensure that everyone is speaking about the same thing.\nThe standard also describes how different manufacturing entities relate to each other. With the right application architecture, these relationships can form a complete and coherent data model of a full manufacturing operation. Thus learning how to speak the language of ISA-95 can help standardize communication between both humans and machines.\nHowever, while ISA-95 is not as complex as a natural human language, it is lengthy. This document provides a brief introduction to some essential terminology.\nFoundational concepts The following concepts frame all conversation that involves ISA-95. If you are a manufacturing veteran, they might be familiar to you. But a high-level review never hurt anyone.\nThe levels of a manufacturing operation Discussions that involve ISA-95 frequently reference the levels of a system. You may hear phrases like “this workflow integrates level-4 data with level-3 activities,” or “the batch is a level-3 construct”. In this context, level corresponds the degree of granularity necessary to discuss and exchange data for different purposes in the manufacturing operation.\nLevel Operational Perspective Example system 4 Business planning ERP 3 Manufacturing operations management MES, CMMS, Quality control 2 Monitoring and acquisition SCADA 1 Sensors PLCs While ISA-95 focuses on level 3 and the interaction between levels 3 and 4, your models can incorporate data from level 2.\nRole based equipment hierarchy: the view from up top The equipment hierarchy represents how equipment can contain other equipment, as a production line might contain a conveyor belt and a pneumatic actuator.\nISA-95 defines equipment across multiple scales. The scale may be as broad as the building where a plant makes items or as a granular as an individual unit that performs one small action within a complex process.\nThese equipment hierarchies often provide a naming convention to prefix addresses for plant data. For example, an MQTT topic might be named site1/bakery2/kitchenA/ovens/a_temp_sensor. In Rhize, the Equipment UI provides an interface to model your plant according to this compositional hierarchy.\nRelationships Most diagrams about models in ISA-95 show a collection of objects that are connected by lines and arrows. These lines and arrows represent relationships.\nUnderstanding how entities relate is fundamental to understanding how to the manufacturing process works as a complete system. Nothing in a manufacturing operation happens in a vacuum, and ISA-95 describes these connections with a precise vocabulary of relation.\nSome important relations include:\nDefined by. As a member may be defined by a class. References As an operations definition references a bill of material Assembled from. As a final material lot may be assembled from various intermediate lots. Made up of. As work schedules are made up of work requests, and work centers are made up of work units. For the full list of relationships, refer to ISA-95 Part 2. To explore the relationships in an interactive way, you can use the Rhize GraphQL API explorer.\nThe activities of an MES Much of the ISA-95 standard discusses operations at the view of level 3, that is the MES or Manufacturing Operations Management (MOM) system. But what activities are part of a MOM system? This is the subject of ISA-95 Part 3.\nDifferent activities of a level-3 system and their interactions with other levels. Broadly, activities can be categorized as reference, pre-execution, execution, and post-execution. The 8 major activities of a MOM are as follows:\nProduct definition: What goes into a product, and what resources does it require? Resource management: What resources are available to produce goods? Detailed production scheduling: When and what does the business want to produce? After a business determines its demand, it can build a schedule using its available resources. Production dispatching: How will the plant assign the available resources to produce the schedule? Once the schedule is received, the level-3 system can assign resources to orders by referencing the definitions and capabilities. Production execution management: How does the plant execute the order? Production data collection: What data is emitted and stored during execution? Production tracking: What components and actions went into production? For example, EBR, Genealogy, and Track and Trace are all use-cases of production tracking. Production performance analysis: How well did the actual production run go, as compared to its ideal? For example, measures of OEE, deviation analysis, and golden batches are all use-cases of performance analysis. To make sense of these activities, you also need to have a concept of the relationship between planned and performed work.\nDefinition, demand, result Almost all manufacturing processes share a common flow, from definition to production to analysis:\nA business defines how a good is to be produced. The business then creates a schedule that demands that a number of these goods are produced according to its definition. The plant uses its available resources and references the existing definitions to execute the orders from the schedule. The relationships between requests, responses, segments, and resources across level 4 and level 3 systems. As long as a business continues to make things, its processes always include a definition of work, a demand for work, and the result of production. These categories inform not only the activities of manufacturing but also the models of production that make each of these activities sensible.\nFrequently used models ISA-95 describes how production entities relate to one another within and across the manufacturing activities. Entities are physical or abstract objects that make up the composition of a manufacturing process in its past, present, future, and ideal state. Here are some of the most common entities.\nResources All aspects of manufacturing involve resources. Without resources nothing can be done or made. ISA-95 Parts 1 and 2 provide a rich and extensive vocabulary for discussing resources. Generally the resource models have the following patterns:\nClasses provide groupings and associations The members of a class are objects that exist in the real world. These instances are represented with versions. As the work executes, the actuals define what resource was really used for a specific job. These resources are part of specifications and requirements for definitions of work All of these resource models can be extended with properties. Equipment 🎥Watch:Creating an equipment model in Rhize Equipment is a tool used in the production process. Important equipment models include the following:\nEquipment classes. Equipment that shares some purpose, such as rotating widget makers. Equipment (Instance). An instance of an equipment class, such as compressor-5, version 2. Actual. The equipment that really performed a job. For example, the actual could be the ID of the compressor involved in some specific production. Equipment Properties. Attributes of an equipment or equipment class. For example, a property of an compressor might be rotation_speed. Relationships between equipment are organized according to the role-based equipment hierarchy and, optionally, the hierarchy scope.\nMaterial 🎥Watch:Five ways to view material through ISA-95 Material is all the input matter required to produce a finished good. Import material models include the following:\nMaterial classes. Material classes represent a broad group of associated materials. An example might be raw_sugar.\nMaterial definitions. A standardized definition of some material, ensuring consistency in the operation.\nMaterial lots. Material lots and sublots are the identifiable units that go into a larger assembly. For example, a material lot might be a pallet of sugar from a supplier, and the sublot might be the individual sugar bags.\nLots can have parent/child relationships to express material compositions. The composition could be reversible, as in a machine assembled from interchangeable parts, or permanent, as in the case of a processes that involve one-way chemical transformations.\nMaterial Actual. A material actual is the quantity of material in a job that is used, consumed, marked as scrap, and so on.\nMaterial properties. Properties of material that are relevant to the production process, for example, meltingPoint or containsLactose.\nPersonnel Personnel are the people who execute a job. Important personnel models include:\nPersonnel class. A group of people with an associated function, for example coil_operators. Person. The “instance” of a personnel class, where the “version” may track properties like certifications and years of experience. Personnel actual. The people who really perform a certain job. Personnel properties. Attributes such as trained to operate heavy machinery. Properties could also communicate a person’s location or current assignment. Hierarchy scope: multiple views of equipment hierarchies The hierarchy scope is a special grouping of equipment that does not necessarily follow the conventional role-based hierarchy. For example, Rhize uses hierarchy scope to define calendar rules and calculate metrics for a set of machines whose shift rules don’t necessarily correspond to the hierarchy. You might also set a hierarchy scope to calculate metrics or track production across an arbitrary grouping of equipment.\nThe calendar service uses the relationships between equipment, hierarchy scope, and work calendars. Segments: process steps to execute The process segment defines the unit of work as it is visible from the business. For example, a baking operation may have the segments mixing, baking, cooling, testing, and storing.\nA segment indicates that a unit of work is meaningful for the business to follow. While mixing might be example of a valid segment, an individual turn of the mixing motor would be a very unlikely segment, as the action is too granular to provide any useful context to the business.\nA segment may specify its necessary work definitions and resources. Process segments also serve as information containers to analyze and track the production of a good at some stage in its lifecycle.\nWork done and requested An operations schedule is associated with a work requests, which has associated job orders Besides resources, manufacturers also need to track and describe how work is demanded and performed.\nISA-95 offers vocabulary to describe the views of this work from both the level-4 (business) perspective and the level 3 (execution) perspective. If you’re wondering whether a model refers to level 3 or 4, keep this trick in mind:\nModels that start with “operation” refer to level 4; models that start with “work” refer to level 3.\nThe operational view of work In all conventional manufacturing, demand originates from the “top,” that is, from the business or level-4 system. Production results are compared against this original demand. Thus, all conventional models of manufacturing include a model of demand, definitions, and results from the level-4 perspective.\nThis operational view of work is defined in ISA-95 Part 2. Here is a quick primer on the major models:\nOperations definitions define the resources required to perform a schedule. Operations schedules include the requests to produce goods. These requests typically demand that production occur at certain times or by certain deadlines. Operations performance is the collection of responses to a request. Performance models provide information about the state of a request, such as WAITING, READY, RUNNING, and COMPLETED. Operations capability provides information about the resources for past and future operations. These capability models provide a way to determine a plant’s theoretical maximum capacity and a way to analyze how well previous runs performed against this capacity. The level-3 view of work The flow of requests and performance from level 4 to level 3 ISA-95 Part 4 defines the level-3 models of work. These models are more granular and detailed than their corresponding operational models.\nTypically, one operations request corresponds to one work request, and they differ in the degree of detail reported in the work request. For example, the operations request may ask for 1000 intermediate widgets, and the work request produces these intermediate widgets.\nHowever, a work request may also fulfill multiple or even fractional operations requests―for example, a work request may produce 1500 intermediate widgets, allocating 1000 to fulfill the operational request and sending the spare 500 to storage.\nDefined work The Work Master provides a set of resource specifications to do some work. It may be associated with segment. When it is planned in a real job order, the work master is “cloned” as a Work Directive.\nPlanned work Planned work broadly follows the following hierarchy:\nWork Schedule. A schedule to perform some amount of work. The schedule contains one or more work requests. Work Request. A collection of job orders to make something Job Order. An order to execute a specific part in a work request Performed work 🎥Watch:Query a job response and its linked data The performance of a production run is queried through the job response. This response exists in the following hierarchy:\nWork Performance. A collection of work responses that detail the performance of the work done for some work schedule Work response. A collection of job responses that map to a work request Job response. The data about the real performance of a job order, including its start and end times and resource actuals. Now you’re talking In this document, you’ve learned the basic vocabulary to discuss manufacturing according to a standardized model. However, this is still an extremely brief entry into ISA-95, whose full standard has 9 parts and thousands of words.\nNevertheless, the best way to acquire a language is to practice it. Can you think of how all the preceding terms apply to your manufacturing operation? Try to apply the terms with some colleagues!","the-activities-of-an-mes#The activities of an MES":"","work-done-and-requested#Work done and requested":""},"title":"How to speak ISA-95"},"/explanations/isa-95-diagrams/":{"data":{"":"These diagrams provide some highly simplified visual explanations gleaned from parts of the thorough ISA-95 standard. To read about how the standard fits within the data architecture of Rhize, read our blog post Rethinking perspectives on ISA-95.","definition-demand-result#Definition, demand, result":"At a high-level, manufacturing consists of:\nGoods being demanded Goods being produced Between these two points, the manufacturing operations performs this work according to its definitions of work and the resources that it has available. This diagram shows how ISA-95 defines these fundamental relationships from the perspective of the business (ERP) and manufacturing operation (MOM).\nClick to expand ","information-exchange-between-models-and-levels#Information exchange between models and levels":"This diagram shows how information exchanges between resource models and systems. It is a highly simplified view of some flows described in parts 2 and 4 of the standard. Each entity in the diagram carries information in multiple dimensions, including:\nThe entity’s position in the resource model (as shown by vertical orientation and color) Its relationship to other entities Its role in the integration between level-3 and level-4 systems. 🎥Watch:Watch two manufacturing experts explain this diagram Click to expand ","overview-of-relations-levels-and-stages#Overview of relations, levels, and stages":"This diagram shows the top-level relations of manufacturing entities in level-3 and level-4 systems across different stages of production. These entities and their relationships also represent some foundational objects in the Rhize data model (exposed through the Rhize GraphQL Interface).\nClick to expand ","the-activity-model#The activity model":"This diagram shows that activities that a manufacturing-operations-management system might perform. Each activity provides information to or receives information from another activity, and all of these activities have their own necessary entities related to resources, planning, and performance.\nThough the focus of this model is on level-3 activities, the data that is exchanged may necessarily involve data from levels 2 and 3. These activities are explained in thorough detail in ISA-95 Part 3.\nClick to expand 🎥Watch:Episode 2 of the Rhize podcast "},"title":"ISA-95 diagrams"},"/explanations/manufacturing-data-hub/":{"data":{"":"This article explains what the components of a Manufacturing Data Hub (MDH) are and why the system must have these particular components to meet the needs of large, modern manufacturing environments.\nIn another phrasing, this article explains why Rhize made the choices it did to become the world’s first manufacturing data hub. For introductory explanations about Rhize in particular, read What is Rhize? and How Rhize works.","read-more#Read more":"Our blog has some articles that define what a data hub is:\nWhat is a Manufacturing Data Hub? Data hub vs. Data lake: the difference for manufacturers ","what-is-an-mdh#What is an MDH?":"A manufacturing data hub is a system that collects all manufacturing events, stores them in a standard model, and has a programmable engine that can run user logic to receive, transform, and send messages across different devices in a manufacturing operation. As it comes with all the necessary backend components—message handling, logic, and storage—an MDH also serves as a backend for manufacturers to build custom MES and MOM applications.\nComponents An MDH is not only a storage or message system. It is a coherent system of interrelated parts and interfaces whose components include the following:\nA high-performance graph database with a standardized schema A data model based on manufacturing standards An API to interact with the data An agent that listens for tag changes from data sources A rules engine that monitors tag changes and triggers user workflows when conditions are met A message broker that communicates events to and from various systems A workflow engine that processes, transforms, and contextualizes tag and event data The IT infrastructure that this all runs on, which at a certain scale must be clustered. Technical requirements Besides its components and features, Rhize is also explicitly designed to meet the needs of a manufacturing operation of any scale. This goal requires a high standard of operational performance, robustness, and extensibility:\nZero Downtime Architecture. A data hub such as Rhize is used in mission-critical environments that run every hour of every day. Outages are unacceptable. Operators must be able to update every component of the system without taking it offline. Secure. Users must be able to securely access and integrate with the hub across applications in the enterprise. So the database requires native OAuth2 security integration for seamless single-sign-on. ACID-compliant. The critical features of an MES require the guarantee of an ACID database. Consistency and availability must be maintained even as the system scales horizontally. Headless operation. Users must be able to use the data hub as a backend to run MES functions, using frameworks or low-code tools to build any frontend on top of these functions. This flexibility is how the MDH can adapt to any manufacturing process: Rhize provides the means to store, standardize, and handle information flows; users build on top of this backend to create the applications, workflows, and interfaces that make sense for their use cases. Type-safe. Uncontrolled schemas in messages become brittle at scale. Unlike a pure MQTT architecture, which does not check the schema of message payloads, the MDH must enforce that data has a standard structure at the moment that the data is written to the database. Extensible but Standardized. While the data hub is built on the ISA-95 standard, it must be able to extend to include customer-specific schemas. Process orchestration. The hub must be able to coordinate tasks handled by multiple systems concurrently and provide a way for users to automate and combine workflows. ","why-an-mdh-needs-this-design#Why an MDH needs this design":"Rhize chose its components deliberately, after careful consideration and years of real-world experience. The following sections describe how these components work together and how this design arose from the landscape of manufacturing automation.\nPoint-to-point reaches scaling issues In early efforts to digitize manufacturing, information often flows from point to point. Each node in the system communicates directly with the other nodes that it sends data to or receives data from. While this form of communication is initially simple to implement, it also tightly couples services. As the system scales, the complexity of point-to-point communication increases non-linearly. With each node, the system becomes increasingly fragile and unobservable.\nFor example, notice how many channels of communication are maintained in this stylized diagram of information exchange between level-three and level-four systems in a point-to-point topology:\nA simplified view of how information might exchange between level 3 and 4 systems in a point-to-point topology. Pub/sub messaging decouples devices When point-to-point communication becomes too difficult to maintain, the hub-and-spoke approach presents the logical next step. In this topology, a central hub coordinates communication between nodes.\nSome systems achieve a hub and spoke through polling, where each device sends a request to the hub at some interval to check whether resources changed. However, considering the number of devices and volume of exchange in manufacturing, polling can create a massive amount of unnecessary traffic.\nInstead of polling, the publish-subscribe pattern can be a more efficient way to decouple communication. Event producers publish topics to a message broker, and the message broker sends the event to consumers that subscribe to the particular topic. With the proliferation of IoT devices and the popularity of the MQTT protocol, publish-subscribe messaging has gained widespread adoption in manufacturing.\nDiagram showing event producers and subscribers in decoupled pub-sub communication While publish-subscribe patterns resolve issues of coupled communication and data accessibility, they don’t address how to make this data useful:\nHow can users collect and organize the data for analysis? How can this data drive further automation? An operation with many data sources likely also has many different structures of its in-flight messages. The MQTT format, for example, has no prescribed payload format: structured JSON and binary blobs are equally valid. While such flexibility provides excellent convenience for producers, the lack of uniformity can make wider integration efforts convoluted and unmaintainable.\nThe philosophy of a data hub is to address these problems without comprising flexibility. So its central components, the database, schema, and message and event handlers, can integrate disparate systems while providing a common, standardized storage for the data exchanged between these systems.\nA standard graph model provides context Every event, person, and object in a manufacturing system is inter-connected. To adequately process incoming event data, manufacturers need to contextualize it, where each event carries additional information about its context within the larger system. This context is framed by the information model that represents the system.\nFor a coherent model of the system, manufacturers need to be able to store assets and event data according to a standardized schema. To scale, this model needs to be suitably thorough and generic. For any organization, it would be an enormous undertaking to write a bespoke model. Fortunately, decades of collaboration have already generated a suitable standard: ISA-95.\nThe ISA-95 standard provides a comprehensive data model for manufacturing. As it happens, its object-oriented system of attributes and relations has an inherent graph structure. This harmony of ISA-95 and graph structures makes a graph database a perfect foundation for the manufacturing data hub:\nThe schema provides the complete model, suitable to any scale The graph database provides the structure that coheres with the inter-related reality of manufacturing However, while publish-subscribe messaging decouples communication, and the ISA-95 database provides a sensible way to contextualize the messages of this data, the system still needs a bridge between the message stream and the long-term, standardized storage. This gap reveals missing components:\nThe system needs to structure raw message data in its ISA-95 representation. Users need a way to access the database and message flow to program their own applications. Without a bridge between messages and storage, an MDH is incomplete The rules engine creates events After the hub receives a message, it must evaluate whether the data is significant enough to constitute an event. This is the function of the rules engine: it assesses message values for changes and then evaluates whether these values should be classified as significant events.\nThe rules converts significant values into manufacturing events. Once the system receives an event, users then need a way to be able to process it.\nA workflow engine makes the system responsive For true “ubiquitous automation” of manufacturing processes, an MDH must provide a way for users to write their own logic to handle events as they happen. Thus, the hub needs a workflow engine that can send messages, process data, and interact with the database in real-time.\nAn example BPMN workflow that receives a job order, evaluates whether maintenance is needed, then stores the data in a standardized database The data hub also must be agnostic about the information it receives, so the event handler must have a way to transform incoming events and represent them in the database schema. Without such transformation, the burden of ISA-95 standardization falls on the event producer. This is both inconvenient for the user and more architecturally fragile: a second place where transformation can happen is also a second point of failure. As long as the producer and consumer can accept and receive the same encoding (for example, JSON), the data hub should be responsible for enforcing standardization.\nWith components for data ingestion, processing, and standardized storage, the MDH has all the necessary functionality to serve as a knowledge graph, MES backend, and integrator of legacy systems. However, this capability cannot be realized unless the system is usable for the widest range of people.\nSensible interfaces make it accessible For many who work in industrial automation, IT is part of a job, not a job in itself. So, a well-designed manufacturing data hub should provide high-quality abstractions and interfaces that minimize the need for thinking about IT systems (instead of manufacturing ones).\nGraphQL, with its single endpoint and precise controls, makes an ideal API interface for the MDH graph database. Queries resemble the structure of the data itself, requiring no recursive joins or intermediary object-relation models. GraphQL also pairs perfectly with a data model based on ISA-95, whose object model has a graph structure, itself a model of the interconnected reality of actual manufacturing.\nWhile GraphQL provides the API to query and transform the data, it cannot execute logic. So, the rules engine should also provide a graphical interface to make event handling as accessible as possible.\nHowever, one final piece is missing: the system must be robust enough for the data-intensive world of manufacturing IT.\nDistributed execution provides robustness The final factor is that the components of an MDH must run on distributed systems. Large manufacturing operations can generate enormous volumes of data. At some point, scaling vertically (with better hardware on single devices) is impossible. Besides size constraints, single devices also create single points of failure.\nSo, an MDH must scale horizontally, where different nodes share computation responsibilities, and extra volumes add data replication."},"title":"Manufacturing Data Hub"},"/get-started/":{"data":{"":" What is Rhize?A Hub to join all manufacturing data in place. Build manufacturing execution systems do deep analysis. How Rhize worksA high-level overview of how Rhize collects, exchanges, and stores data, starting with data collection and ending with user interaction. "},"title":"Get started"},"/get-started/how-rhize-works/":{"data":{"":"","data-inputs#Data inputs":"","database#Data storage":"","deployment#Deployment":"This article provides a high-level overview of how Rhize works, starting with data collection and ending with user interaction. To make these concepts more concrete, the next section provides examples of each process.\nThe heart of the Rhize platform is its manufacturing knowledge graph, which stores data from all levels of the operation and exposes this data through a single endpoint. Around the database are services that exchange messages and process events in real-time. The system runs on distributed, containerized systems, ensuring horizontal scalability and high availability.\nFinally, outside of the Rhize deployment are the two most important components: the manufacturing operation, which sends event data to Rhize, and the Rhize users, who interact with Rhize data through a number of special-purpose interfaces.\nA simplified view of Rhize’s architecture Examples in practice To make the next sections less abstract, consider these examples of how Rhize creates a common data hub for diverse human and system interaction.\nData inputs An instrument fitter configures an MQTT-compatible device to send sensor data to Rhize. A business analyst sends an ERP order through an integration with the GraphQL API. Message exchange A piece of equipment publishes information about its status over MQTT. A BPMN process subscribes to the equipment’s TestResult subtopic. When the TestResult status changes to fail, the BPMN process publishes a maintenance order to the broker. The ERP system, which subscribes to the maintenance topic, prepares a document for maintenance personnel. Data storage A data scientist writes a Python script that discovers production outliers for a specific segment class across all production sites. A procurer uses an Excel-Rhize integration to call the API and receive a production order that the BPMN process wrote to the database one month earlier. User-data interaction An operator queries sensor values from a custom-built mobile interface. A quality-engineer observes real-time data in a custom dashboard built for statistical process control. Deployment A DevOps engineer pushes an upgrade that handles message-streams more efficiently. This upgrade rolls out node-by-node across each instance. A plant process causes a heavy inflow of data. The system autoscales to meet the temporary computation demand. Data inputs A manufacturing data hub is useless without manufacturing data.\nThe Rhize agent collects data from MQTT brokers and devices, OPC-UA servers. You can also send data and documents over HTTP through a GraphQL call. The Rhize UI also has a graphical interface to model production objects.\nAll this data is mapped to Rhize’s ISA-95 schema, which creates a coherent model for all objects in the data hub.\nMessage exchange More granular view of Rhize-Customer messaging Rhize’s architecture is event-driven, low-latency, and scalable. To communicate events in real-time and across services, Rhize uses a publish-subscribe model through the NATS message broker. The message infrastructure enables complex interaction between services without creating dependencies between them.\nServices in the Rhize application subscribe to their relevant topics and handle events as they come in. Services also publish events to the event broker. Thus, Rhize services can communicate with each other and with customer systems in a completely decoupled manner.\nData storage With a schema defined by the ISA-95 standard, the graph database creates contextual relationships that link all data stored in the system. This graph data, accessible through a single endpoint, provides a single source to perform vast combinations of analysis.\nAlong with event data from disparate places and decoupled services, the database also stores declarative configuration data to instruct different services on how to respond to events.\nFinally, the time-series component of the database accepts real-time data streams.\nThe interfaces The Rhize application comes with a graphical interface. Some uses include:\nConfigure BPMN rules. A low-code tool for analysts and operators to create programmable events. Upload master data. Based on the ISA-95 object models. Administrate. Authenticate and scope access to systems and personnel. These interfaces sit on top of the GraphQL API gateway, which serves as a programming interface for data analysis. Rhize customers also use the GraphQL interface to build their own applications, either with dedicated frontend developers or through low-code tools like Appsmith.\nLast but not least, the time-series data is observable through monitoring tools like Grafana.\nDeployment Rhize runs on Kubernetes and is configured through CI/CD servers.\nUsing Kubernetes, Rhize can deploy to multiple instances using a common configuration. Such distribution removes single points of failure, and system upgrades can happen on a rolling basis, with zero downtime. All deployment is version controlled, which makes regressions easier to recover from.\nDeployment is vendor neutral, giving organizations complete control to run the system on their local networks or preferred cloud host. The modern tools of DevOps also makes the system easier to maintain, as they come with vast tooling ecosystems and training material.","examples-in-practice#Examples in practice":"","interfaces#The interfaces":"","message-exchange#Message exchange":""},"title":"How Rhize works"},"/get-started/introduction/":{"data":{"":"Rhize is a real-time, event-driven manufacturing data hub. It unites data analysis, event monitoring, and process execution in one platform. Its interface and architecture is designed to conform to your processes. We assume nothing about what your manufacturing workflows look like.\nRhize has only one strong opinion: all manufacturing objects and data must be modeled on the ISA-95 standard. This standards-based schema is how Rhize connects every data event across an entire operation. If you aren’t an ISA-95 fan, we’re happy to convert you, but adoption is a requirement to use the platform.\nAnd if you do adopt ISA-95, you open your organization to Rhize’s far-reaching transformations.","a-data-hub-for-manufacturing#A data hub for manufacturing":"Rhize is a data hub that collects, stores, integrates, and processes data from your manufacturing system. Rhize accepts the event as the driver of change in manufacturing state. Its architecture is designed to receive and process message events emitted from the operation in real time.\nTo make each event coherent in the context of all others, it must conform to a standard. Rhize uses the ISA-95 standard as its data model, and the database schema is the most complete digital representation of ISA-95 in the world. The flexibility of ISA-95 scales to an entire enterprise operation, and future changes in processes require no ad-hoc changes of the schema.\nThe database represents data as a single graph, a structure ideally suited for the association of nodes and edges inherent in ISA-95. The database is exposed through a single GraphQL endpoint. Besides keeping the interface small, the GraphQL query language coheres exactly with the underlying graph model.\nThe API is totally open, meaning that your operators can use it as a backend to build any MES, MOM, and data-science applications that they want. Rhize also has a built-in low-code BPMN workflow creator, so operators can write logic to handle event data with only API calls and JSON transformation.\nFinally, Rhize’s architecture supports distributed deployment, and its components are loosely coupled microservices. This clustered approach is necessary for organizations to scale horizontally and maintain high reliability.","a-tool-that-fits-to-your-processes#A tool that fits to your processes":"The development of Rhize is the culmination of decades of experience from real practitioners. We know that manufacturing is messy, and each process has thousands of particulars. Even within the same company and segment, processes frequently differ from site to site.\nOur design philosophy empowers manufacturing operators to shape their tool for their work demands. Some examples of the flexibility include:\nA headless MES. While Rhize has a graphical interface, all data is reachable through a single API endpoint. This means your teams can rapidly build custom frontends―and do it with the most comfortable API for frontend development, GraphQL. Low-code interface. Model your schema and execute processes using BPMN, a visual programming language. The visual interface makes Rhize and your manufacturing automation accessible to the widest possible audience. Generic data collection. Rhize receives data from all levels of the manufacturing process. The NATS broker publishes and subscribes to low-level data from MQTT and OPC-UA, but the database can also receive ERP inventories and documents sent over HTTP. Read about use cases.","modern-it-for-manufacturing#Modern IT for manufacturing":"Rhize’s system design is flexible, practical, and built on modern tools and practices:\nAll data is accessible through a single endpoint. While the data collected and processed may span multiple segments, events, units, areas, and even plants, all data is stored in a graph database and exposed through a GraphQL interface.\nAutomated and containerized deployment. Rhize brings innovations from DevOps to manufacturing. In practice, this means that Rhize is interoperable with whatever system the manufacturer uses, deployment is version-controlled, and your system can use rolling upgrades with zero downtime.\nBuilt on open standards. Rhize is based on open standards, like ISA-95, and open protocols, like MQTT. Open industry standards and protocols ensure that the application and your manufacturing processes speak a common language. Rhize heavily uses open-source software, which brings interoperability, reduced vendor lock, and robust tooling ecosystems."},"title":"What is Rhize?"},"/how-to/":{"data":{"":"Topics about how to use Rhize to query data, build and run workflows, and build frontends.\nUse GraphQLGuides to use the GraphQL interface to query information, add records, and build custom UIs. Write BPMN workflowsCreate BPMN workflows to handle inputs, listen for events, and throw triggers. Define production modelsCreate models for equipment, data sources, operations definitions, work definitions, and so on. Connect event dataSet up event-driven messaging for Rhize Use work calendarsHow to configure work calendars to account for planned and unplanned downtime in your operation. Use the KPI serviceHow to configure KPI Service to record key ISO22400 OEE Metrics. AuditHow to use the Audit log to inspect all events in the Rhize system "},"title":"User guides"},"/how-to/audit/":{"data":{"":"The Audit Log provides a tamper-proof and immutable audit trail of all events that occur in the Rhize system. Users with appropriate permissions can access the audit log either through the UI menu or the GraphQL API.","audit-through-graphql#Audit through GraphQL":"The audit log is also exposed through the GraphQL API. To access it, use the queryAuditLog operation, and add filters for the time range and users.\nHere’s an example query:\nquery { queryAuditLog(filter: {start: \"2023-01-01T00:00:00Z\", end:\"2023-12-31T00:00:00Z\", tagFilter:[{id:\"user\", in: [\"admin@libremfg.com\"]},{id:\"operation\", in: [\"set\"]}]}, order: {}) { operationType meta { time user } event { operation uid attribute value } } } ","audit-through-the-ui#Audit through the UI":"To inspect the audit log through the Rhize UI, follow these steps:\nFrom the UI menu, select Audit. Select the users that you want to include in the audit. Use the time filters to select the pre-defined or custom range that you want to return. On success, a log of events appears for the users and time ranges specified. For a description of the fields returned, refer to the Audit fields section.\nAudit fields In the audit UI, each record in the audit has the following fields:\nField Description Timestamp The time when the event occurred User The user who performed the operation Operation The GraphQL operation involved Entity Internal ID The ID of the resource that was changed Attribute What changed in the resource. This corresponds to the object properties as defined by the API and its underlying schema Value The new value of the updated attribute ","prerequisites#Prerequisites":"To use the audit log, ensure you have the following:\nIf accessing to your Rhize UI environment, a user account with appropriate permissions\nIf accessing through GraphQL, you also need:\nThe ability to Use the Rhize GraphQL API A token configured so that audience includes audit, and the scopes contain audit:query. This scope should be created by BaaS, not manually. For details, refer to Set up Keycloak."},"title":"Audit"},"/how-to/bpmn/":{"data":{"":"In the following topics, learn how to use Rhize’s BPMN engine to orchestrate processes. Coordinate tasks between different systems, transform and calculate data, set triggers to run workflows automatically.\nOverview: orchestrate processesAn overview of how to use Rhize’s custom BPMN engine and UI to orchestrate workflows. Trigger workflowsHow to trigger a workflow in Rhize. Use the API, publish a message to the broker, listen to a data source, or set timers. Use JSONataThe Rhize guide to JSONata, with example transformations and calculations that are relevant to manufacturing. Handle errors and debugStrategies to handle errors in your BPMN workflows, and ways to debug workflows when things don’t work as expected. Tune BPMN performanceTips to debug and improve the performance of your BPMN process Special variablesSpecial variables used by Rhize BPMN workflows Naming conventionsRecommended naming conventions for BPMN processes and their nodes BPMN learning resourcesLinks to supplemental tools and material to learn BPMN BPMN elementsA reference of all BPMN elements used in the Rhize BPMN engine. "},"title":"Write BPMN workflows"},"/how-to/bpmn/bpmn-elements/":{"data":{"":"This document describes the parameters available to each BPMN element in the Rhize UI.\nThese parameters control how users set conditions, transform data, access variables, call services, and so on.","call-activities#Call activities":"\nA call activity invokes another workflow. In this flow, the process that contains the call is the parent, and the process that is called is the child.\nCall activities have the following parameters:\nParameters Description Called element The ID of the called process The inputs have the following parameters:\nParameters Description Local variable name The name of the variable as it will be accessed in the child process (that is, the key name) assignment value The value to pass from the parent variable context The outputs have the following parameters:\nParameters Description Local variable name What to name the incoming data, as it will be accessed in the parent process (that is, the key name) assignment value The value to pass from the child variable context For a guide to reusing functions, read the Reuse workflows section in the “Create workflow” guide.","common-parameters#Common parameters":"Every BPMN workflow and every element that the workflow contains have the following parameters:\nParameter Description ID Mandatory unique ID. For guidance, follow the BPMN naming conventions. Name Optional human readable name. If empty, takes ID value. Documentation Optional freeform text for additional information Extension properties Optional metadata to add to workflow or node ","events#Events":"Events are something that happen in the course of a process. In BPMN, events are drawn with circles. Events have a type and a dimension.\nEventsMessage typeTimer type Message events subscribe or publish to the Rhize broker. Timer events start according to some interval or date, or wait for some duration. In event-driven models, events can happen in one of three dimensions:\nStart All processes begin with some trigger that starts an event. Start events are drawn with a single thin circle. Intermediate. Possible events between the start and end. Intermediate events might start from some trigger, or create some result. They are drawn with a double thin line. End All processes end with some result. End events are drawn with a single thick line. Besides these dimensions, BPMN also classifies events by whether they catch a trigger or throw a result. All start events are catch events; that is, they react to some trigger. All end events are throw events; that is, they terminate with some output—even an error. Intermediate events may throw or catch.\nRhize supports various event types to categorize an event, as described in the following sections. As with Gateways and Activities, event types are marked by their icons. Throwing events are represented with icons that are filled in.\nStart events Start events are triggered by the CreateAndRunBPMN and CreateAndRunBPMNSync mutation operations. The parameters for a start event are as follows:\nParameter Description Outputs Optional variables to add to the process variable context. JSON or JSONata. Message start events Message events are triggered from a message published to the Rhize broker. The parameters for a message event are as follows:\nParameter Description Message The topic the message subscribes to on the Rhize Broker. The topic structure follows MQTT syntax. Outputs Optional variables to add to the process variable context. JSON or JSONata. Timer start events Timer start events are triggered either at a specific date or recurring intervals. The parameters for a timer start event are as follows:\nParameter Description Timer One of Cycle, to begin at recurring intervals. For example,R5/2024-05-09T08:12:55/PT10S starts on 2024-05-09 and executes every 10 seconds for 5 repetitions. If \u003cSTART_DATE\u003e is not set, Rhize uses 2023-01-01T00:00:00Z. Date, to happen at a certain time, for example, 2024-05-09T08:12:55 Enter values in ISO 8601 format. Outputs Optional variables to add to the process variable context. JSON or JSONata. Intermediate message events Intermediate message events throw a message to the Rhize NATS broker. This may provide info for a subscribing third-party client, or initiate another BPMN workflow.\nThe parameters for an intermediate message event are as follows:\nParameter Description Message The topic the message publishes to on the Rhize Broker. The topic structure follows MQTT syntax Inputs Variables to send in the body. For messages to the Rhize broker, use the special variable BODY. Value can be JSON or JSONata. Headers Additional headers to send in the request Outputs JSON or JSONata. Optional variables to add to the process variable context. Intermediate timer events An intermediate message pauses for some duration. The parameters for an intermediate timer event are as follows:\nParameter Description Timer A duration to pause. Enter values in ISO 8601 format. Outputs Optional variables to add to the process variable context. The assignment value can be JSON or JSONata. ","gateways#Gateways":"Gateways control how sequence flows interact as they converge and diverge within a process. They represent mechanisms that either allow or disallow a passage.\nBPMN notation represents gateways as diamonds with single thin lines, as is common in many diagrams with decision flows. Besides decisions, however, Rhize’s BPMN notation also includes parallel gateways.\nAs with Events and Activities, gateway types are marked by their icons.\nDrawn as diamonds, gateways represent branches in a sequence flow.\nExclusive gateway Marked by an “X” icon, an exclusive gateway represents a point in a process where only one path is followed. In some conversations, an exclusive gateway is also called an XOR.\nIf a gateway has multiple sequence flows, all flows except one must have a conditional JSONata expression that the engine can evaluate. To designate a default, leave one flow without an expression.\nAn exclusive gateway with a condition and default. Configure conditions as JSONata expressions\nExclusive gateways can only branch. That is, they cannot join multiple flows.\nParallel gateway Marked by a “+” icon, parallel gateways indicate a point where parallel tasks are run.\nParallel gateways run jobs in parallel.\nParallel joins You can join parallel tasks with another parallel gateway. This joins the variables from both branches to process variable context. Note that parallel joins have performance costs, so be mindful of using them, especially in large tasks. To learn more, read Tune BPMN performance.\nParallel joins join variable context, but have performance costs.","service-tasks#Service tasks":"In BPMN, an activity is work performed within a business process.\nOn the Rhize platform, most activities are tasks, work that cannot be broken down into smaller levels of detail. Tasks are drawn with rectangles with rounded corners.\nℹ️ Besides tasks, you can also use call activities, processes which call and invoke other processes. A service task uses some service. In Rhize workflows, service tasks include Calls to the GraphQL API (and REST APIs), data source reads and writes, and JSON manipulation. These service tasks come with templates.\nAs with Gateways and events, service task are marked by their icons.\nService tasks have a gear icon marker\nTo add a service task, select the change icon (“wrench”), then select Service Task. Use Templates to structure the service task call and response.\nThe service task templates are as follows\nJSONata transform Transform JSON data with a JSONata expression. For detailed examples, read The Rhize Guide to JSONata.\nCall parameters Description Input Input data for the transform Transform The transform expression Max Payload size Number. If the response length exceeds this number of characters, Rhize throws an error. Besides the call parameters, the JSONata task has following additional fields:\nParameter Description Input response The name of the variable to add to the process variable context GraphQL Query Run a GraphQL query\nCall parameters Description Query body GraphQL query expression Variables JSON. Variables for the GraphQL query. Connection Timeout Number. Time in milliseconds to establish a connection. Max Payload size Number. If the response length exceeds this number of characters, Rhize throws an error. Besides the call parameters, the Query task has following additional fields:\nParameter Description Input response Optional JSONata expression to map to the process variable context. For GraphQL operations, use this only to map values. Rely on GQL filters to limit the payload. Headers Additional headers to send in the request GraphQL Mutation Run a GraphQL mutation\nCall parameters description Mutation body GraphQL Mutation expression Variables JSON. Variables for the GraphQL query. Connection Timeout Number. Time in milliseconds to establish a connection. Max Payload size Number. If the response length exceeds this number of characters, Rhize throws an error. Besides the call parameters, the mutation task has following additional fields:\nParameter Description Input response Optional JSONata expression to map to the process variable context. For mutations, use this only to map values. Use the mutation call to limit the payload. Headers Additional headers to send in the request Call REST API HTTP call to a REST API service.\nCall parameters Description Method Type One of GET, POST, PATCH, PUT, DELETE Verification Boolean. Whether verify the Certificate Authority provided in the TLS certificate. URL The target URL URL Parameters JSON. The key-value pairs to be used as query parameters in the URL HTTP Headers JSON. The key-value pairs to be used as request headers Connection Timeout Number. Time in milliseconds to establish a connection. Max Payload size Number. If the response length exceeds this number of characters, Rhize throws an error. Besides the call parameters, the REST task has following additional fields:\nParameter Description Input response Optional JSONata expression to map to the process variable context Headers Additional headers to send in the request JSON schema Validate that a payload conforms to a configured JSON schema. For example, you can validate that data.arr contains an array of numbers and that userID contains a string of certain length.\nCall Parameters Description Schema A JSON schema. You can also create one from a JSON file with a tool such as JSON to JSON schema Variable Optional. Key of specific variable to validate (default checks all variables in The Schema task has the following output that you can define as a variable:\nResponse mapping Description Valid The boolean output of the schema evaluation. True if schema is valid. Validation error A string that reports the validation errors if the schema is invalid. Read Datasource Read values from topics of a datasource (for example, an OPC-UA server)\nCall parameters Description Data source The ID of the datasource Data JSON or JSONata expression. Topics and values to write to Max Payload size Number. If the response length exceeds this number of characters, Rhize throws an error. Besides the call parameters, the data source task has following additional fields:\nParameter Description Input response The variable name to store the response in process variable context Headers Additional headers to send in the request Write Datasource Write values to topics of a datasource.\nCall parameters Description Data source The ID of the datasource Data JSON or JSONata expression. Topics and values to write to Max Payload size Number. If the response length exceeds this number of characters, Rhize throws an error. Besides the call parameters, the data source task has following additional fields:\nParameter Description Input response The variable name to store the response in process variable context Headers Additional headers to send in the request ","variables-and-expressions#Variables and expressions":"As data passes and transforms from one element to another, variables remain in the process variable context. You can access these variables through JSONata expressions."},"title":"BPMN elements"},"/how-to/bpmn/create-workflow/":{"data":{"":"This guide provides a quick overview of the major features of the Rhize BPMN engine and interface, with links to detailed guides for specific topics. For a reference of all BPMN elements and their parameters, refer to BPMN elements.\nThe Rhize BPMN UI provides a graphical interface to transform and standardize data flows across systems. Such process orchestration has many uses for manufacturing. For example, you can write a BPMN workflow to do any of the following:\nAutomatically ingest data from ERP and SCADA systems, then transform and store the payloads in the standardized ISA-95 representation Coordinate tasks across various systems, creating a layer for different data and protocols to pass through Calculate derived values from the data that is exchanged to perform functions such as waste calculation and process control. ℹ️ Rhize BPMN workflows conform to the visual grammar described in the OMG standard for Business Process Model and Notation. Each process is made of events (circles), activities (rectangles), gateways (diamonds), and flows (arrows). Some elements are extended for Rhize-specific features, such as service tasks that call the GraphQL API. Some elements from the standard are unused and thus do not appear in the UI. ","access-process-variable-context#Access process variable context":"As data passes through the nodes of a workflow, the nodes share access to a variable space. Nodes can access these variables, create new variables, and mutate existing ones. This overall variable object is called process variable context.\nWhen working with variables, keep the following in mind:\nAccess the root variable context through $..\nThis follows the conventions of JSONata. For details and examples, read Use JSONata.\nAccess nested properties with dot notation.\nFor example, the following is a reference to the first item in the orders object in the variable context:\n$.orders[] You can store a node’s output in a variable.\nMany output fields offer a way to create a variable. For example, the JSON schema field has two variables that you can name, one that outputs a boolean based on whether the input is valid, and another that outputs the error string if the variable is invalid.\nYou can access these variables in later nodes (unless you mutate them).\nVariables.\nIf you direct output to a variable that already exists, the new value overwrites the old one. This behavior can be used to manage the overall memory footprint of a workflow.\nThe maximum context size is configurable.\nBy default, the process variable context has a maximum size of 1MB. When an activity outputs data, the output is added to the process variable context. When variable size gets large, you have multiple strategies to reduce its size (besides mutating variables). For ideas, refer to Tune BPMN performance.\nYou can trace variable context.\nFor details, refer to the Debug guide.","control-flows#Control flows":"As data passes through a workflow, you might need to conditionally direct it to specific tasks, transformations, and events. For this, Rhize has gateways, represented as diamonds.\nExclusive gateway. Represented by an X symbol, exclusive gateways create decision branches based on whether a condition is true. While you can use JSONata conditionals to control flows within tasks, exclusive gateways are the most common and visually understandable way to execute conditional steps.\nTo use an exclusive gateway:\nCreate an arrow from the control start to an exclusive gateway (diamond with X symbol). Use arrows to create outgoing conditions to new tasks or events. Leave the default condition blank. In all other arrows, use the Condition field to write a boolean expression. This gateway creates a job order only if its material state is ready. Parallel gateways Represented by a + (plus sign) symbol, parallel gateways execute multiple tasks at the same time. To use a parallel gateway:\nSelect a gateway. Use the wrench sign to change its condition to parallel. Use arrows to create parallel conditions. When the workflow runs, each parallel branch executes at the same time.\nSimultaneously add a record to the database and send an alert ","examples#Examples":"Rhize has a repository of templates that you can import and use in your system. Use these to explore how the key functionality works. Rhize BPMN templates","request-and-send-data#Request and send data":"Workflows often exchange data between Rhize and one or more external systems. The BPMN activity task templates provide multiple ways to communicate with internal and external systems, and pass data over different protocols. Additionally, message events provide templates to publish and subscribe to the Rhize broker.\nEach template has a set of parameters to configure it. To use a template:\nSelect the activity (rectangle) element. Select Template and then choose the template you want. Configure the template according to its Task parameters. Interact with the Rhize API Use GraphQL tasks to query and change data in your manufacturing knowledge graph. For example:\nA scheduling workflow could use the Query task to find all JobResponses whose state is COMPLETED. An ingestion workflow might use a Mutation task to update new jobResponse data that was published from a SCADA system. You can also use JSONata in your GraphQL payloads to dynamically add values at runtime. For details about how to use the Rhize API, read the Guide to GraphQL.\nInteract with external systems To make HTTP requests to external systems, use the REST task. For example, you might send a POST with performance values to an ERP system, or use a GET operation to query test results.\nℹ️ Besides REST, you can use this template to interact with any HTTP API. Publish and subscribe Besides HTTP, workflows can also publish and subscribe messages over MQTT, NATS, and OPC UA.\nA workflow that evaluates a message and throws a if the payload meets a certain condition message To publish and subscribe to the Rhize broker:\nSelect a start (thin circle) or intermediate (double-line) circle. Select the wrench icon. Select the message event (circle with an envelope). Configure the message topic and body according to the Event parameters. If using an Intermediate throw event, name the variable BODY. To listen and publish to an edge device:\nCreate a data source. In your workflow, select the task. And choose the Data source template. Configure the Data Source task parameters. The strategy you choose to send and receive message data depends on your architectural setup. Generally, data-source messages come from level-1 and level-2 devices on the edge, and messages published to the Rhize broker come from any NATS, MQTT, or OPC UA client. The following diagram shows some common ways to interact with messages through BPMN.","reuse-workflows#Reuse workflows":"BPMN workflows are composable, where each element can be reused by others. For example, you might create a workflow that calculates common statistics, or one that makes a specified call to an external system. Using call activities other workflows can reuse the workflow.\nAn example of a main workflow calling a function. Template To reuse a workflow:\nDrag the task element (rectangle) into the workflow. Select the wrench icon. Select Call Activity. Configure it according to the call activity parameters. ","transform-and-calculate#Transform and calculate":"As the data in a workflow passes from start node to end node, it often undergoes some secondary processing. Mew properties might be added, some data might be filtered, or a node might create a set of derived values from the original input. For example, you might use calculate statistics, or transform a message payload into a format to be received by the Rhize API or an external system.\nAnnotated and truncated version of mapping an external event to the operationEvent definition To calculate and transform data, BPMN nodes can interpret the JSONata expression language. For details, read the complete Rhize guide to JSONata.","trigger-workflows#Trigger workflows":"You have multiple ways to trigger a start condition.\nAPI triggers come from a GraphQL call to the Rhize DB. Message triggers subscribe to a topic on the Rhize broker and run whenever an event is published to that topic. Rule triggers subscribe to the topics on a remote data source, such as an edge OPC-UA server or MQTT broker. Timer triggers start according to some schedule (once or repeating). To learn more, read Trigger workflows."},"title":"Overview: orchestrate processes"},"/how-to/bpmn/debug-workflows/":{"data":{"":"Errors come in two categories: expected and unexpected. The Rhize BPMN engine has ways to handle both.\nA robust workflow should have built-in logic to anticipate errors. For unexpected issues, Rhize also creates a trace for each workflow, which you can use to observe the behavior and performance of the workflow at each element as it executes sequentially. You can also use debug flags and variables to trace variable context as it transforms across the workflow.","strategies-to-debug#Strategies to debug":"For detailed debugging, you can use an embedded instance of Grafana Tempo to inspect each step of the workflow, node by node. To debug on the fly, you may also find it useful to use customResponse and intermediate message throws to print variables and output at different checkpoints.\nDebug from the API calls When you first test or run a workflow, consider starting the testing and debugging process from an API trigger. All API triggers return information about the workflow state (for example COMPLETED or ABORTED). With the createAndRunBpmnSync operation, you can also use the customResponse to provide information from the workflow’s variable context. For details of how this works, read the guide to triggering workflows.\nFor example, consider a workflow that has two nodes, a Message throw event and a REST task.\nWhen the message completes, the user writes Message sent into customResponse as an output variable. When the REST task completes, the response is saved into customResponse. So the jobState property reports on the overall workflow status, and customResponse serves as a checkpoint to report the state of each node execution. You can also request the dataJSON field, which reports the entire variable context at the last node. Now imagine that the user has started the workflow from the API and receives this response:\n{ \"data\": { \"createAndRunBpmnSync\": { \"jobState\": \"ABORTED\", \"customResponse\": \"message sent\", \"traceID\": \"993ee32af9522f5b35b4ec80f4ff58a8\" } } } Note how ABORTED indicates the workflow failed somewhere. Yet, the value of customResponse must have been set after the message event executed. So the problem is likely with the REST node.\nYou could also use a similar strategy with intermediate message events. However, while customResponse and messages are undoubtedly useful debugging methods, they are also limited— the BPMN equivalents of printf() debugging. For full-featured debugging, use the traceID to explore the workflow through Tempo.\nDebug in Tempo ℹ️ The instructions here provide the minimum about using Tempo as a tool. To discover the many ways you can filter your BPMN traces for debugging and analysis, refer to the official documentation. Rhize creates a unique ID and trace for each workflow that runs. This ID is reported as the traceID in the createAndRunBPMN mutation operation. Within this trace, each node is instrumented, with spans emitted at every input and output along each node of execution. With the trace ID, you can find the workflow run in Tempo and follow the behavior.\nTo inspect a workflow in Tempo:\nGo to your Grafana instance. Select Explore and then Tempo. From the TraceQL tab, enter the traceID and query. Alternatively, use the Search tab with the bpmn-engine to find traces for all workflows. Screenshot of a compact view of spans for a BPMN process in Tempo Each workflow instance displays spans that trace the state of each node at its start, execution, and end states. When debugging, you are likely interested in the spans that result in ABORTED. To inspect the errors:\nSelect the nodes with errors. Use the events property to inspect for exceptions. For example, this REST task failed because the URL was invalid.\nA detailed view of an error for a BPMN process in Tempo Also note the names of the spans in the previous two screenshots. Names that convey semantic information it easier to find specific nodes and easier to understand and follow the overall workflow. Well-named nodes make debugging easier. This is one of the reasons we recommend always following a set of naming conventions when you author BPMN workflows.\nAdding the debug flag For granular debugging, it also helps to trace the variable context as it passes from node to node. To facilitate this, Rhize provides a debugging option that you can pass in multiple ways:\nFrom an API call with the debug:true argument. In the process variable context, by setting __traceDebug: true In the BPMN service configuration by setting OpenTelemetry.defaultDebug to true When the debugging variable is set, Tempo reports the entire variable context in the Span Attributes at the end of each node.\nThe process variable context at the end of a node in a BPMN workflow. ","strategies-to-handle-errors#Strategies to handle errors":"All error handling likely uses some conditional logic. The workflow author anticipates the error and then writes some logic to conditionally handle it. However, you have many ways to handle conditions. When deciding how to direct flows, consider both the context of the error and overall readability of your diagram. This section describes some key strategies.\nGateways Use exclusive gateways for any type of error handling. For example, you might define a normal range for a value, then send alerts for when the value falls outside of this range. If it makes sense, these error branches also might flow into an early end event.\nDownload this workflow from BPMN templates JSON schema validation Validation can greatly limit the scope of possible errors. To validate your JSON payloads, use the JSON schema task.\nThe JSON schema task outputs a boolean value that indicates whether the input conforms to the schema that you set. You can then set a condition based on whether this valid variable is true, and create logic to handle errors accordingly. For example, this schema requires that the input variables include a property arr whose value is an array of numbers.\n{ \"$schema\": \"http://json-schema.org/draft-07/schema#\", \"title\": \"Generated schema for Root\", \"type\": \"object\", \"properties\": { \"arr\": { \"type\": \"array\", \"items\": { \"type\": \"number\" } } }, \"required\": [\"arr\"] } In a production workflow, you might use this exact schema to validate the input for a function that calculates statistics (perhaps choosing a different variable name).\nA conditional that branches when the JSON schema task receives invalid input. Download the template JSONata conditions Besides the logical gateway, it may make sense to use JSONata ternary expressions in one of the many parameters that accepts JSONata expressions. For example, this expression creates one message body if valid is true and another if not:\n= { \"message\": $.valid ? \"payload is valid\" : \"Invalid payload\" } Check JSONata output If a field has no value, JSONata outputs nothing. For example, the following expression outputs only {\"name\": \"Rhize\"}, because no $err field exists.\nExpressionOutput =( $name := \"Rhize\"; { \"name\": $name, \"error\": $err } ) { \"name\": \"Rhize\" } \u003cbutton class=“hextra-code-copy-btn hx-group/copybtn hx-transition-all active:hx-opacity-50 hx-bg-primary-700/5 hx-border hx-border-black/5 hx-text-gray-600 hover:hx-text-gray-900 hx-rounded-md hx-p-1.5 dark:hx-bg-primary-300/10 dark:hx-border-white/10 dark:hx-text-gray-400 dark:hover:hx-text-gray-50” title=“Copy code”\n\u003cdiv class=\"copy-icon group-[.copied]/copybtn:hx-hidden hx-pointer-events-none hx-h-4 hx-w-4\"\u003e\u003c/div\u003e \u003cdiv class=\"success-icon hx-hidden group-[.copied]/copybtn:hx-block hx-pointer-events-none hx-h-4 hx-w-4\"\u003e\u003c/div\u003e You can use this behavior to direct flows. For example, an exclusive gateway may have a condition such as $exists(err) that flows into an error-handling condition.\nCreate event logging To persist error handling, you can set gateways that flow to mutation tasks that use the addEvent operation. The added event may be a successful operation, an error, or both, creating a record of events emitted in the workflow that are stored in your manufacturing knowledge graph. This strategy increases the observability of errors and facilitates future analysis. It may also be useful when combined with the debugging strategies described in the next section."},"title":"Handle errors and debug"},"/how-to/bpmn/learning-resources/":{"data":{"":"Here are some links to supplemental tools and material to help you build BPMN workflows in Rhize:\nBPMN templates. A repository of BPMN workflows that you can download and run yourself. bpmn.io. Open source rendering toolkits and editors for BPMN 2.0. You can use the bpmn-js for local offline building. Rhize Youtube channel. Includes demos of BPMN. 📝 OMG BPMN standard. The standard on which the Rhize BPMN engine and UI is based. vscode-language-jsonata. A VS code extension to interactively pipe JSONata expressions together, in the literate-programming style of Jupyter notebooks. "},"title":"BPMN learning resources"},"/how-to/bpmn/naming-conventions/":{"data":{"":"","bpmn-nodes#BPMN Nodes":" ℹ️ These are recommendations. Your organization may adapt the conventions to its needs. Each BPMN workflow has an ID, as does each node in the workflow. Rhize recommends adopting a set of conventions about how you name these elements. Standardizing BPMN names across an environment has multiple benefits:\nConsistent workflow naming conventions help you filter and find workflows in the process list. Well-named nodes make the workflow behavior easier to understand at a glance. These names provide discovery and context when debugging and tracing a workflow. The following list describes our default naming conventions.\nBPMN processes Name BPMN Processes according to the following convention:\n\u003cINVOCATION_TYPE\u003e_\u003cCONTRACT_ID\u003e_\u003cPURPOSE\u003e\nWhere:\n\u003cINVOCATION_TYPE\u003e describes how the BPMN is expected to be triggered\n\u003cCONTRACT_ID\u003e Links to the ID supplied in the sequence diagram (if applicable)\n\u003cPURPOSE\u003e describes what the workflow does\nInvocationTypes Description NATS Invoked when a message is received in the Rhize NATS broker API Expects to be called from the API using createAndRunBPMNSync or createAndRunBPMN mutations RULE Invocation is expected from the rule engine FUNC Internal functions to be invoked as helpers Examples:\nNATS_ProcessOrderV1TransformAndPersist NATS_PLMMaterialMasterV2TransformPersistAndPublish RULE_ChangeOEEStatusOfCNCEquipment API_WST04_GetNextLibreBatchNumber API_WAT01_CloseOpenJobOrders BPMN Nodes Name nodes in a workflow according to the following convention:\n\u003cTYPE\u003e_\u003cSUB_TYPE\u003e_\u003cDETAIL\u003e Where:\n\u003cTYPE\u003e is the type of node \u003cSUB_TYPE\u003e further categorizes the node \u003cDETAIL\u003e describes the node behavior. Start Events For start events, use the name to describe each trigger according to the following convention:\nSTART_\u003cSUB_TYPE\u003e_\u003cDESCRIPTION\u003e\nFor message starts, include the topics. For timers, indicate the frequency.\nSubType Description API Manual start via API MSG Message Start TIMER Timer start Examples:\nSTART_MSG_InboundOrders START_API START_TIMER_EveryTenMinutes Query task Runs a GraphQL .\nPrefix: Q.\nSubTypes Description GET Get query. Expected to return one item QUERY Query operation. May return multiple items AGG Aggregate query Examples:\nQ_GET_OperationsScheduleByOperationId Q_QUERY_JobOrdersByOperationsRequestId Mutation task Runs a GraphQL mutation.\nPrefix: M\nSubTypes Description ADD Adds a new record UPDATE Updates existing UPSERT Updates existing or adds new if not found DELETE Deletes a record Examples:\nM_UPSERT_ProcessOrder M_ADD_UnitOfMeasure JSONata transform task Prefix: J\nSubType Description INIT Initialising some new variables INDEX Updating an index variable MAP Mapping one entity to another TRANSFORM Updates existing or adds new if not found DIFF Calculating the difference between entities CLEANUP Deletes the record CALC Performing some calculation Examples:\nJ_INDEX_CreateLoop1 J_TRANSFORM_PO_to_JO J_INIT_ProcessingLimits REST task REST nodes should also indicate the target system and endpoints, according to the following naming convention:\nREST_\u003cSYSTEM\u003e_\u003cSUB_TYPE\u003e_\u003cENDPOINT\u003e_\u003cDETAIL\u003e\nWhere:\nSYSTEM abbreviates the system being called, for example SAP. SubType Description GET Initialising some new variables POST Updating an index variable PUT Mapping one entity to another PATCH Updates existing or adds new if not found DELETE Calculating the difference between entities Examples:\nREST_SAP_POST_bill-material-bom REST_EWM_GET_serial-number-by-object-v1_RingSerialNumbers Gateway Prefix = GW\nSubType Description X_SPLIT Exclusive gateway split X_JOIN Parallel gateway join P_SPLIT Parallel gateway split P_JOIN Parallel gateway join Examples:\nGW_X_SPLIT_DifferenceLoop01 GW_P_JOIN_DifferenceLoop01 Sequence Flows Only name sequence flows that have conditions. If a sequence flow carries a condition, indicate the condition in the naming as follows:\nF_\u003cCONDITION_DESCRIPTION\u003e\nExamples:\nF_NoNewEquipment F_AbandonFlagTrue End Events If a workflow has multiple end events, indicate their number according to this convention:\nEND_\u003cNUMBER\u003e Examples:\nEND_01 END_02 ","bpmn-processes#BPMN processes":"","response-field-naming#Response Field Naming":"When nodes generate a response, give the response field the same name as its node."},"title":"Naming conventions"},"/how-to/bpmn/trigger-workflows/":{"data":{"":"You also have multiple ways to start, or trigger, a BPMN workflow. The best choice of trigger depends on the context of the event and the system that initiates the workflow.\nAPI triggers come from a GraphQL call to the Rhize DB. Message triggers subscribe to a topic on the Rhize broker and run whenever an event is published to that topic. Rule triggers subscribe to the topics on a remote data source, such as an edge OPC-UA server or MQTT broker. Timer triggers start according to some schedule (once or repeating). ","rule-based-triggers#Rule-based triggers":"Rule-based triggers subscribe to tag changes from a data source and trigger when the rule change condition is met. Typically, users choose this workflow trigger when they want to orchestrate processes originating from level-1 and level-2 systems.\nTo add a data source and create a rule based trigger, refer to Turn values into events.","start-a-workflow-from-an-api#Start a workflow from an API":"No matter the start event, all workflows can be triggered by an API call. However, if the workflow uses the default blank start event, you must trigger it through API call. For example, an API trigger may originate from a custom frontend application or a developer’s local machine.\nThe Rhize API has two mutations to start workflows. Both must specify the workflow ID as an argument in the request body. Each run for a workflow returns a unique ID that you can use for debugging.\nSynchronous and asynchronous API triggers To start BPMN workflows, Rhize has two API operations:\ncreateAndRunBPMNSync starts a workflow and waits for the process to complete or abort (synchronous). createAndRunBpmn starts a workflow and does not wait for the response (asynchronous). Use the synchronous operation if you want to receive information about the result of the workflow in the call response. On the other hand, the asynchronous operation frees up the call system to do more work, no matter whether the workflow runs correctly.\nBoth operations have similar call syntax. For example, compare the syntax for these calls:\nSynchronousAsync mutation sychronousCall{ createAndRunBpmnSync(id: \"API_demo_custom_response\") { id jobState customResponse } } mutation asyncCall{ createAndRunBpmn(id: \"API_demo_custom_response\") { id jobState customResponse } } The responses for these calls have two differences:\nFor synchronous calls, the returned JobState should be a finished value (such as COMPLETED or ABORTED). Asynchronous calls likely return an in-progress status, such as RUNNING. Synchronous calls can request the dataJSON field to report the entire variable context at the final node. Only the synchronous call receives data in the customResponse. For details, refer to the next section. customResponse The customResponse is a special variable to return data in the response to clients that run createAndRunBPMNSync operations.\nYou can set the customResponse in any element that has an Output or Input response parameter. It can use any data from the process variable context, including variables added on the fly.\nFunctionally, only the last value of the customResponse is returned to the client that sent the response. However, you can use conditional branches and different end nodes to add error handling. For example, this workflow returns Workflow ran correctly if the call variables include the message CORRECT and an error message in all other cases.\nDownload this workflow from BPMN templates Additional properties for workflow calls API operations can also include parameters to add variables to the process variable context and to specify a workflow version.\nTo add variables, use the variables input argument. Note that the variables are accessible from their root object. For example, a workflow would access the following string value at $.input.message:\n\"variables\": \"{\\\"input\\\":{\\\"message\\\":\\\"CORRECT\\\"}}\" } To specify a version, use the version property. For example, this input instructs Rhize to run version 3 of the API_demoCallToRemoteAPI workflow:\n{ \"createAndRunBpmnSyncId\": \"API_demoCallToRemoteAPI\", \"version\": \"3\" } If the version property is empty, Rhize runs the active version of the workflow (if an active version exists).","start-from-a-message#Start from a message":"The message start event subscribes to a topic on the Rhize broker. Whenever a message is published to this topic, the workflow is triggered. The Rhize broker can receive messages published over MQTT, NATS, and OPC UA.\nFor example, this workflow subscribes to the topic material/stuff. Whenever a message is published to the topic, it evaluates whether the quantity is in the correct threshold. If the quantity is correct, it uses the mutation service task to add a material lot. If incorrect, it sends an alert back to the broker.\nDownload this workflow as a BPMN template. Note that, for a workflow to run from a message start event, the workflow must be enabled.","timer-triggers#Timer triggers":"Timer triggers run according to a configured time or interval. For example, a timer trigger may start a workflow each day, or once at a certain time.\nTo use timer triggers, use the timer start event. As with message start events, the workflow must be enabled for it to run."},"title":"Trigger workflows"},"/how-to/bpmn/tune-performance/":{"data":{"":"This page documents some tips to debug BPMN workflows and improve their performance.\nManufacturing events can generate a vast amount of data. And a BPMN workflow can have any number of logical flows and data transformations. So an inefficient BPMN process can introduce performance degradations.","look-for-inefficient-execution-logic#Look for inefficient execution logic":"When you first write a workflow, you may use some logical flows slow down execution time. If a process seems slow, look for these places to refactor performance.\nAvoid parallel joins Running processes in parallel can increase the workflow’s complexity. Parallel joins in particular can also increase memory usage of the NATS service.\nWhere possible, prefer exclusive branching and sequential execution. When a task requires concurrency, keep the amount of data processed and the complexity of the tasks to the minimum necessary.\nControl wildcards in message start events BPMN message start tasks can start on any topic or data source. However, performance varies with the events that the start task subscribes to.\nSubscribing to multiple wildcards can especially drag performance. To avoid a possible performance hit, try to subscribe to an exact topic, or limit subscriptions to a single wildcard.\nAvoid loops A BPMN process can loop back to a previous task node to repeat execution. This process can also increase execution time. If a process with a loop is taking too long to execute, consider refactoring the loop to process the variables as a batch in JSONata tasks.","manage-the-process-context-size#Manage the process context size":" ℹ️ The max size of the process variable context comes from the default max payload size of NATS Jetstreams. To increase this size, change your NATS configuration. By default, the size of the process variable context is 1MB. If the sum size of all variables exceeds this limit, the BPMN process fails to execute.\nBe mindful of variable output Pay attention the overall size of your variables, especially when outputting to new variables. For example, imagine an initial JSON payload, data, that is 600KB. If a JSONata task slightly modifies and outputs it to a new variable, data2, the process variable context will exceed 1MB and the BPMN process will exit.\nTo work around this constraint, you can save memory by mutating variables. That is, instead of outputting a new variable, you can output the transformed payload to the original variable name.\nDiscard unneeded data from API responses Additionally, in service tasks that call APIs, use the Response Transform Expression to minimize the returned data to only the necessary fields. Rhize stores only the output of the expression, and discards the other part of the response. This is especially useful in service tasks that Call a REST API, since you cannot precisely specify the fields in the response (as you can with a GraphQL query).\nIf you still struggle to find what objects create memory bottlenecks, use a tool to observe their footprint, as documented in the next section.\nObserve payload size Each element in a BPMN workflow passes, evaluates, or transforms a JSON body. Any unnecessary fields occupy unnecessary space in the process variable context. However, it’s hard for a human to reason about the size of the inflight payload without a tool to provide measurements and context.\nIt’s easier to find places to reduce the in-flight payload size if you can visualize its memory footprint. We recommend the JSON site analyzer, which presents a flame graph of the memory used by the objects in a JSON data structure.\nThe material lot object is dominating the size of this JSON payload. This a good place to start looking for optimizations.","use-the-jsonata-book-extension#Use the JSONata book extension":"A BPMN process performs better when the JSONata transformations are precise. A strategy to debug and minimize necessary computation is to break transformations into smaller steps.\nIf you use Visual Studio Code, consider the jsonata-language extension. Similar to a Jupyter notebook, the extension provides an interactive environment to write JSONata expressions and pass the output from one expression into the input of another. Besides its benefit for monitoring performance, we have used it to incrementally build complex JSONata in a way that we can document and share (in the style of literate programming)."},"title":"Tune BPMN performance"},"/how-to/bpmn/use-jsonata/":{"data":{"":"JSONata is a query language to filter, transform, and create JSON objects. Rhize BPMN workflows use JSONata expressions to transform JSON payloads as they pass through workflow nodes and across integrated systems. In BPMN workflows, JSONata expressions have some essential functions:\nMap data. Moving values from one data structure to another. Calculate data. Receiving values as input and create new data from them. Create logical conditions. Generate values to feed to gateways to direct the flow of the BPMN. This guide details how to use JSONata in your Rhize environment and provides some examples relevant to manufacturing workflows. For the full details of the JSONata expression language, read the Official JSONata documentation.","jsonata-examples#JSONata examples":"These snippets provide some examples of JSONata from manufacturing workflows. To experiment with how they work, copy the data and expression into a JSONata exerciser and try changing values.\nFilter for items that contain This expression returns the ID of all equipmentActual items that are associated with a specified job response JR-4. It outputs the IDs as an array of strings in a new custom object.\nThis is a minimal example of how you can use JSONata to transform data into new representations. Such transformation is a common prerequisite step for post-processing and service interoperability.\n$.data.queryJobResponse[`id`=\"JR-4\"].( {\"associatedEquipment\": equipmentActual.id} ) InputOutput { \"data\": { \"queryJobResponse\": [ { \"id\": \"JR-1\", \"data\": [ { \"value\": 100 } ], \"equipmentActual\": [ { \"id\": \"hauler\" }, { \"id\": \"actuator-121\" } ] }, { \"id\": \"JR-4\", \"data\": [ { \"value\": \"101.8\" } ], \"equipmentActual\": [ { \"id\": \"actuator-132\" }, { \"id\": \"actuator-133\" } ] } ] } } { \"associatedEquipment\": [ \"actuator-132\", \"actuator-133\" ] } Find actual associated with high values This expression finds all job responses whose value exceeds 100. It outputs the matching job response IDs along with the associated equipment actual used in the job.\nIn production, you may use a similar analysis to isolate all resource actuals associated with an abnormal production outcome.\n$map($.data.queryJobResponse, function($v){ $number($v.data.value) \u003e 102 ? {\"jobResponseId\": $v.id, \"EquipmentActual\": $v.equipmentActual} } ) InputOutput { \"data\": { \"queryJobResponse\": [ { \"id\": \"JR-1\", \"data\": [ { \"value\": 100 } ], \"equipmentActual\": [ { \"id\": \"hauler\" }, { \"id\": \"actuator-121\" } ] }, { \"id\": \"JR-5\", \"data\": [ { \"value\": 103.2 } ], \"equipmentActual\": [ { \"id\": \"actuator-122\" }, { \"id\": \"actuator-13\" } ] }, { \"id\": \"JR-2\", \"data\": [], \"equipmentActual\": [ { \"id\": \"actuator-13\" } ] }, { \"id\": \"JR-4\", \"data\": [ { \"value\": \"101.8\" } ], \"equipmentActual\": [ { \"id\": \"actuator-132\" }, { \"id\": \"actuator-133\" } ] }, { \"id\": \"JR-3\", \"data\": [], \"equipmentActual\": [ { \"id\": \"actuator-091\" } ] }, { \"id\": \"JR-12\", \"data\": [], \"equipmentActual\": [] }, { \"id\": \"JR-123\", \"data\": [], \"equipmentActual\": [ { \"id\": \"actuator-121\" } ] }, { \"id\": \"JR-6\", \"data\": [], \"equipmentActual\": [] }, { \"id\": \"JR-8\", \"data\": [ { \"value\": \"96.7\" } ], \"equipmentActual\": [ { \"id\": \"actuator-091\" } ] }, { \"id\": \"JR-9\", \"data\": [], \"equipmentActual\": [] }, { \"id\": \"JR-10\", \"data\": [ { \"value\": \"105.0\" } ], \"equipmentActual\": [ { \"id\": \"actuator-12\" } ] }, { \"id\": \"JR-7\", \"data\": [ { \"value\": \"103.2\" } ], \"equipmentActual\": [ { \"id\": \"actuator-12\" } ] } ] } } [ { \"jobResponseId\": \"JR-5\", \"EquipmentActual\": [ { \"id\": \"actuator-122\" }, { \"id\": \"actuator-13\" } ] }, { \"jobResponseId\": \"JR-10\", \"EquipmentActual\": [ { \"id\": \"actuator-12\" } ] }, { \"jobResponseId\": \"JR-7\", \"EquipmentActual\": [ { \"id\": \"actuator-12\" } ] } ] Map event to operations event This function takes data from an external weather API and maps it onto the operationsEvent ISA-95 object. It takes the earliest value from the event time data as the start, and last value as the end. If no event data exists, it outputs a message.\nAlthough this example uses data that is unlikely to be a source of a real manufacturing event, the practice of receiving data from a remote API and mapping it to ISA-95 representation is quite common. In production, you may perform a similar operation to map an SAP schedule order to an operationsSchedule, or the results from a QA service to the testResults object.\n( $count(events[0]) \u003e 0 ? events.{ \"id\":id, \"description\":title, \"hierarchyScope\":{ \"id\":\"Earth\", \"label\": Earth, \"effectiveStart\": $sort(geometry.date)[0] }, \"category\":categories.title, \"recordTimestamp\": $sort(geometry.date)[0], \"effectiveStart\": $sort(geometry.date)[0], \"effectiveEnd\": $sort(geometry.date)[$count(geometries.date)-1], \"source\": sources.id \u0026 \" \" \u0026 sources.url, \"operationsEventDefinition\": { \"id\": \"Earth event\", \"label\": \"Earth event\" } } : {\"message\":\"No earth events lately\"} ) InputOutput Long JSON { \"title\": \"EONET Events\", \"description\": \"Natural events from EONET.\", \"link\": \"https://eonet.gsfc.nasa.gov/api/v3/events\", \"events\": [ { \"id\": \"EONET_6516\", \"title\": \"Ubinas Volcano, Peru\", \"description\": null, \"link\": \"https://eonet.gsfc.nasa.gov/api/v3/events/EONET_6516\", \"closed\": null, \"categories\": [ { \"id\": \"volcanoes\", \"title\": \"Volcanoes\" } ], \"sources\": [ { \"id\": \"SIVolcano\", \"url\": \"https://volcano.si.edu/volcano.cfm?vn=354020\" } ], \"geometry\": [ { \"magnitudeValue\": null, \"magnitudeUnit\": null, \"date\": \"2024-05-06T00:00:00Z\", \"type\": \"Point\", \"coordinates\": [ -70.8972, -16.345 ] } ] }, { \"id\": \"EONET_6513\", \"title\": \"Iceberg D28A\", \"description\": null, \"link\": \"https://eonet.gsfc.nasa.gov/api/v3/events/EONET_6513\", \"closed\": null, \"categories\": [ { \"id\": \"seaLakeIce\", \"title\": \"Sea and Lake Ice\" } ], \"sources\": [ { \"id\": \"NATICE\", \"url\": \"https://usicecenter.gov/pub/Iceberg_Tabular.csv\" } ], \"geometry\": [ { \"magnitudeValue\": 208.00, \"magnitudeUnit\": \"NM^2\", \"date\": \"2024-02-16T00:00:00Z\", \"type\": \"Point\", \"coordinates\": [ -33.27, -51.88 ] }, { \"magnitudeValue\": 208.00, \"magnitudeUnit\": \"NM^2\", \"date\": \"2024-03-01T00:00:00Z\", \"type\": \"Point\", \"coordinates\": [ -32.82, -51.09 ] }, { \"magnitudeValue\": 208.00, \"magnitudeUnit\": \"NM^2\", \"date\": \"2024-03-07T00:00:00Z\", \"type\": \"Point\", \"coordinates\": [ -30.95, -51.21 ] } ] }, { \"id\": \"EONET_6515\", \"title\": \"Sheveluch Volcano, Russia\", \"description\": null, \"link\": \"https://eonet.gsfc.nasa.gov/api/v3/events/EONET_6515\", \"closed\": null, \"categories\": [ { \"id\": \"volcanoes\", \"title\": \"Volcanoes\" } ], \"sources\": [ { \"id\": \"SIVolcano\", \"url\": \"https://volcano.si.edu/volcano.cfm?vn=300270\" } ], \"geometry\": [ { \"magnitudeValue\": null, \"magnitudeUnit\": null, \"date\": \"2024-04-28T00:00:00Z\", \"type\": \"Point\", \"coordinates\": [ 161.36, 56.653 ] } ] } ] } [ { \"id\": \"EONET_6516\", \"description\": \"Ubinas Volcano, Peru\", \"hierarchyScope\": { \"id\": \"Earth\", \"effectiveStart\": \"2024-05-06T00:00:00Z\" }, \"category\": \"Volcanoes\", \"recordTimestamp\": \"2024-05-06T00:00:00Z\", \"effectiveStart\": \"2024-05-06T00:00:00Z\", \"effectiveEnd\": \"2024-05-06T00:00:00Z\", \"source\": \"SIVolcano https://volcano.si.edu/volcano.cfm?vn=354020\", \"operationsEventDefinition\": { \"id\": \"Earth event\", \"label\": \"Earth event\" } }, { \"id\": \"EONET_6513\", \"description\": \"Iceberg D28A\", \"hierarchyScope\": { \"id\": \"Earth\", \"effectiveStart\": \"2024-02-16T00:00:00Z\" }, \"category\": \"Sea and Lake Ice\", \"recordTimestamp\": \"2024-02-16T00:00:00Z\", \"effectiveStart\": \"2024-02-16T00:00:00Z\", \"effectiveEnd\": \"2024-03-07T00:00:00Z\", \"source\": \"NATICE https://usicecenter.gov/pub/Iceberg_Tabular.csv\", \"operationsEventDefinition\": { \"id\": \"Earth event\", \"label\": \"Earth event\" } }, { \"id\": \"EONET_6515\", \"description\": \"Sheveluch Volcano, Russia\", \"hierarchyScope\": { \"id\": \"Earth\", \"effectiveStart\": \"2024-04-28T00:00:00Z\" }, \"category\": \"Volcanoes\", \"recordTimestamp\": \"2024-04-28T00:00:00Z\", \"effectiveStart\": \"2024-04-28T00:00:00Z\", \"effectiveEnd\": \"2024-04-28T00:00:00Z\", \"source\": \"SIVolcano https://volcano.si.edu/volcano.cfm?vn=300270\", \"operationsEventDefinition\": { \"id\": \"Earth event\", \"label\": \"Earth event\" } } ] Calculate summary statistics These functions calculate statistics for an array of numbers. Some of the output uses built-in JSONata functions, such as $max(). Others, such as the ones for median and standard deviation, are created in the expression.\nYou might use statistics such as these to calculate metrics on historical or streamed data.\n( $mode := function($arr) { ( $uniq := $distinct($arr); $counted := $map($uniq, function($v){ { \"value\": $v, \"count\": $count($filter($arr, function($item) { $item = $v })) } }); $modes := $filter($counted, function($item) { $item.count = $max($counted.count) }); $sort($modes.value) ) }; $stdPop := function($arr) { ( $variance := $map($arr, function($v, $i, $a) { $power($v - $average($a), 2) }); $sum($variance) / $count($arr) ~\u003e $sqrt() ) }; $median := function($arr) { ( $sorted := $sort($arr); $length := $count($arr); $mid := $floor($length / 2); $length % 2 = 0 ? $median := ($sorted[$mid - 1] + $sorted[$mid]) / 2 : $median := $sorted[$mid] ) }; { \"std_population\": $stdPop($.data.arr), \"mean\": $average($.data.arr), \"median\": $median($.data.arr), \"mode\": $mode($.data.arr), \"max\": $max($.data.arr), \"min\": $min($.data.arr) } ) InputOutput { \"data\": { \"arr\": [ 1, 1, 6, 2, 3, 32, 4, 5, 5, 3, 3, 6, 6 ] } } { \"std_population\": 7.72071677084591, \"mean\": 5.923076923076923, \"median\": 4, \"mode\": [ 3, 6 ], \"max\": 32, \"min\": 1 } Select random item This expression randomly selects an item from the plant’s array of available equipment, and then adds that item as the equipmentRequirement for a segment associated with a specific job order.\nYou might use randomizing functions for scheduling, quality control, and simulation.\n( $randomChoice := function($a) { ( $selection := $random() * ($count($a)+1) ~\u003e $floor(); $a[$selection] )}; { \"segmentRequirement\": { \"workRequirement\": {\"id\": $.PO}, \"equipmentRequirements\":[$randomChoice($.available)], \"id\": \"Make widget\" } } ) InputOutput { \"available\":[\"line_1\",\"line_2\",\"line_3\",\"line_4\",\"line_5\"], \"PO\":\"po-123\" } { \"segmentRequirement\": { \"workRequirement\": { \"id\": \"po-123\" }, \"equipmentRequirements\": [ \"line_2\" ], \"id\": \"Make widget\" } } Recursively find child IDs This function uses recursion and a predefined set of naming rules to find (or generate) a set of child IDs for an entity. The n value determines how many times it’s called.\nMany payloads in manufacturing have nested data. Recursive functions such as the following provide a concise means of traversing a set of subproperties.\n( $next := function($x, $y) {$x \u003e 1 ? ( $namingRules := \"123456789ABCDFGHJKLMNOPQRSTUVWXYZ\"; $substring($y[-1],-1) = \"Z\" ? $next($x - 1, $append($y, $y[-1] \u0026 '1')) : $next($x - 1, $append( $y, $substring($y[-1],0,$length($y[-1])-1) \u0026 $substring($substringAfter($namingRules,$substring($y[-1],-1)),0,1) )) ) : $y}; { \"children\": $next(n, [nextId]) } ) InputOutput { \"n\":10, \"nextId\": \"molten-widet-X2FCS\" } { \"children\": [ \"molten-widet-X2FCS\", \"molten-widet-X2FCT\", \"molten-widet-X2FCU\", \"molten-widet-X2FCV\", \"molten-widet-X2FCW\", \"molten-widet-X2FCX\", \"molten-widet-X2FCY\", \"molten-widet-X2FCZ\", \"molten-widet-X2FCZ1\", \"molten-widet-X2FCZ2\" ] } ","use-jsonata-in-rhize#Use JSONata in Rhize":"JSONata returns the final value of its expression as output. This output can be of any data type that JSON supports. Generally, we recommend outputting a JSON object with the keys and values of the data you want to subsequently work with.\nIn practice, creating an expression usually follows these steps:\nBegin with an =. Embed the expression in parenthesis. At the top of expression, write your logic, variables, and functions. At the bottom of the expression, create a JSON object whose keys are names you configure and whose values are derived from your logic. For example:\nexpressionoutput =( $logic := \"Hello\" \u0026 \" \" \u0026 \"World\"; { \"output\": $logic } ) { \"output\": \"Hello World\" } Begin each expression with a = Note that the previous expression begins with the equals sign, =. This character instructs Rhize to parse the subsequent data as JSONata (as opposed to raw JSON or some other data structure).\nAccess root variable context with $. To access the root of the entire BPMN variable space, use the dollar character followed by a dot, $.. For example, this expression accesses all IDs for an equipmentClass object from the root variable context, $..\n$.equipmentClass.id InputOutput { \"equipmentClass\": [ { \"id\": \"Vessel-A012\", \"description\": \"Stock Solution Vessel\", \"effectiveStart\": \"2023-05-24T09:58:00Z\", \"equipmentClassProperties\": [ { \"id\": \"Volume\", \"description\": \"Vessel Volume\" } ] }, { \"id\": \"Vessel-A013\", \"description\": \"Stock Solution Vessel\" } ] } [ \"Vessel-A012\", \"Vessel-A013\" ] \u003cbutton class=“hextra-code-copy-btn hx-group/copybtn hx-transition-all active:hx-opacity-50 hx-bg-primary-700/5 hx-border hx-border-black/5 hx-text-gray-600 hover:hx-text-gray-900 hx-rounded-md hx-p-1.5 dark:hx-bg-primary-300/10 dark:hx-border-white/10 dark:hx-text-gray-400 dark:hover:hx-text-gray-50” title=“Copy code”\n\u003cdiv class=\"copy-icon group-[.copied]/copybtn:hx-hidden hx-pointer-events-none hx-h-4 hx-w-4\"\u003e\u003c/div\u003e \u003cdiv class=\"success-icon hx-hidden group-[.copied]/copybtn:hx-block hx-pointer-events-none hx-h-4 hx-w-4\"\u003e\u003c/div\u003e JSONata in BPMN elements JSONata can be used in many Rhize BPMN elements Particularly, the JSONata service task exists to receive input and pass it to another element or system.\nThough JSONata tasks are the most common use of JSONata, you can use the = prefix to declare an expression in many other fields. Parameters that accept expressions include API payloads, message payloads, and flow conditions.\nTo review the full list of elements and fields that accept JSONata, read the BPMN element reference.\nJSONata version Many implementations of JSONata exist. Rhize uses a custom Go implementation for high performance and safe calculation."},"title":"Use JSONata"},"/how-to/bpmn/variables/":{"data":{"":"Rhize designates some variable names for a special purpose in BPMN workflow. This list these special variables is as follows:\nVariable Purpose BODY The name of the variable as Input in Intermediate message throws. The value of this variable is the payload sent to the Rhize message broker. customResponse A value to report at the end of a synchronous API call to trigger a workflow. On completion, the call reports whatever the value was in the customResponse field of the GraphQL response. For details, read Trigger workflows. __traceDebug If true at the start of the workflow, the BPMN workflow reports the variable context at each node as spans in Tempo. "},"title":"Special variables"},"/how-to/gql/":{"data":{"":" Overview: the Rhize APIHow to query your manufacturing knowledge graph QueryA guide to the three GraphQL operations in Rhize FilterHow to filter a GraphQL call to a subset of manufacturing items. MutateA guide to adding, creating, and deleting data in the Rhize DB SubscribeA guide to using GraphQL to subscribe to changes in the database. "},"title":"Use GraphQL"},"/how-to/gql/call-the-graphql-api/":{"data":{"":"","call-syntax#Call syntax":"The following sections show you the essential features to make a query.\nAuthenticate To authenticate your requests, pass a bearer token as an Authorization header. Be sure to preface the value with the word Bearer :\nFor an overview of how Rhize handles token exchange, read About OpenID connect.\nRequest body By default, all GraphQL operations have the following structure:\nDefine the operation type (one of, query, mutation, or subscription).\nName the query anything you want. This example builds a query called myCustomName:\nquery myCustomName { #operations will go here } In curly brackets, define the operation you want to query.\nIn parenthesis next to the operation, add the arguments to pass. This example uses the getEquipment operation, and its arguments specify which item of equipment to get.\nquery myCustomName { getEquipment(id: \"Kitchen_mixer_b_01\") { #Fields go here } } Within the operation, define the fields you want to return. This example queries for the equipment ID and the person who created the entity.\nquery myCustomName { getEquipment(id: \"Kitchen_mixer_b_01\") { id _createdBy } } As you might expect, the request returns only these fields for the equipment named Kitchen_mixer_b_01.\n{ \"data\": { \"getEquipment\": { \"id\": \"Kitchen_mixer_b_01\", \"_createdBy\": \"john_snow\" } } } Request exactly the data you want A major benefit of GraphQL is that you can modify queries to return only the fields you want. You can join data entities in a single query and query for entity relationships in the same way that you would for entity attributes.\nUnlike calls to a REST API, where the server-side code defines what a response looks like, GraphQL calls instruct the server to return only what is specified. Furthermore, you can query diverse sets of data in one call, so you can get exactly the entities you want without calling multiple endpoints, as you would in REST, or composing queries with complex recursive joins, as you would in SQL. Besides precision, this also brings performance benefits to minimize network calls and their payloads.\nFor example, this expands the fields requested from the previous example. Besides id and _createdBy, it now returns the description, unique ID, and version information about the requested equipment item:\nRequestResponse query ExampleQuery { queryEquipment(filter: { id: { eq: \"Kitchen_mixer_b_0 1\" } }) { id _createdBy versions { iid description } activeVersion { iid description } } } { \"data\": { \"queryEquipment\": [ { \"id\": \"Kitchen_mixer_b_01\", \"_createdBy\": \"john_snow\", \"versions\": [ { \"iid\": \"0xcc701\", \"description\": \"First generation of the mixer 9000\" }, { \"iid\": \"0xcc71a\", \"description\": \"Second generation (in testing)\" } ], \"activeVersion\": { \"iid\": \"0xcc701\", \"description\": \"First generation of the mixer 9000\" } } ] } } You can also add multiple operations to one call. For example, this query requests all data sources and all persons:\nRequestResponse query peopleAndDataSources { queryPerson { id label } queryDataSource { id } } { \"data\": { \"queryPerson\": [ { \"id\": \"235\", \"label\": \"John Ramirez\" }, { \"id\": \"234\", \"label\": \"Jan Smith\" } ], \"queryDataSource\": [ { \"id\": \"x44_mqtt\" }, { \"id\": \"x45_opcUA\" } ] } } ","operations#Operation types":"In a manufacturing operation, all event data is interrelated. To make these relations explorable, Rhize stores data in a special-purpose graph database designed to represent all levels of the manufacturing process. This database is enforced by our ISA-95 schema, the most comprehensive data representation of ISA-95 in the world.\nRhize exposes this database through a GraphQL API. Unlike REST, GraphQL requires only one endpoint, and you can define exactly the data that you return for each operation.\nIf you are a customer, the best way to learn both GraphQL and ISA-95 modelling is to use the Apollo Explorer for our schema. However, for newcomers to GraphQL, the flexibility may look overwhelming. These topics introduce the basics of how to use GraphQL with Rhize’s custom database.\nOnce you learn how to explore the API, you’ll find that the interface is more comfortable and discoverable than a comparable OpenAPI (Swagger) document—and that’s before considering the improvements GraphQL brings to precision, performance, and developer experience.\nOperation types In GraphQL, an operation is a request to the server. Rhize supports three types of operations:\nQueries return data and subsets of data. Mutations change the data on the server side. Subscriptions notify about data changes in real time. For details and examples, refer to their specific documentation pages.","shortcuts-for-more-expressive-requests#Shortcuts for more expressive requests":"The following sections provide some common ways to reduce boilerplate and shorten the necessary coding for a call.\nMake input dynamic with variables The preceding examples place the query input as inline arguments. Often, calls to production systems separate these arguments out as JSON variables.\nVariables add dynamism to your requests, which serves to make them more reusable. For example:\nIf you build a low-code reporting application, you could use variables to change the arguments based on user input. In a BPMN event orchestration, you could use variables to make a GraphQL call based on a previous JSONata filter. Refer to the example, Write ERP material definition to DB. For example, this query places the ID of the resource that it requests as an inline variable:\nquery myCustomName { getEquipment(id: \"Kitchen_mixer_b_01\") { _createdBy } } Instead, you can pass this argument as a variable. This requires the following changes:\nIn the argument for your query, name the variable and state its type. This instructs the query to receive data from outside of its context:\n## Name variable and type query myCustomName ($getEquipmentId: String) { ## operations go here. } In the operation, pass the variable as a value in the argument. In this example, add the variable as a value to the id key like this:\nquery GetEquipment($getEquipmentId: String) { ## pass variable to one or more operations getEquipment(id: $getEquipmentId) { ## fields go here } } In a separate variables section of the query, define the JSON object that is your variable:\n{ \"getEquipmentId\": \"Kitchen_mixer_b_01\" } QueryMutation query GetEquipment($getEquipmentId: String) { getEquipment(id: $getEquipmentId) { _createdBy } } Variables:\n{ \"getEquipmentId\": \"Kitchen_mixer_b_01\" } The preceding example is minimal, but the use of variables to parameterize arguments also applies to complex object creation and filtering. For example, this mutation uses variables to create an array of Persons:\nmutation AddPerson($input: [AddPersonInput!]!) { addPerson(input: $input) { person { id } } } Variables:\n{ \"input\": [ {\"id\": \"234\", \"label\":\"Jan Smith\"}, {\"id\": \"235\", \"label\": \"John Ramirez\"} ] } To learn more, read the official GraphQL documentation on Variables.\nTemplate requested fields with fragments Along with variables, you can use fragments to reduce repetitive writing.\nFragments are common fields that you use when querying an object. For example, imagine you wanted to make queries to different equipment objects for their id, label, _createdBy, and versions[] properties. Instead of writing these fields in each operation, you could define them in a fragment, and then refer to that fragment in each specific operation or query.\nTo use a fragment:\nDefine them with the fragment keyword, declaring its name and object. ## name ## object fragment CommonFields on Equipment Include the fragment in the fields for your operation by prefacing its name with three dots: ...CommonFields For example:\nQueryResponse ## Define common fields fragment CommonFields on Equipment{ id label _createdBy versions { id } } ## Use them in your query. query kitchenEquipment { getEquipment(id: \"Kitchen_mixer_b_02\") { ...CommonFields } } Variables:\n{ \"data\": { \"getEquipment\": { \"id\": \"Kitchen_mixer_b_02\", \"label\": \"Kitchen mixer B02\", \"_createdBy\": \"admin@rhize.com\", \"versions\": [] } } } "},"title":"Overview: the Rhize API"},"/how-to/gql/filter/":{"data":{"":"","combine-filters-with-and-or-not#Combine filters with \u003ccode\u003eand\u003c/code\u003e, \u003ccode\u003eor\u003c/code\u003e, \u003ccode\u003enot\u003c/code\u003e":"Filters limit an operation to a subset of resources. You can use filters to make operations more precise, remove unneeded items from a payload, and reduce the need for secondary processing.\nTo use a filter, specify it in the operation’s argument. Most fields in an object can serve as a filter. ℹ️ This page provides a detailed guide of how to use the filters, with examples. For a bare reference of filters and data types, refer to the GraphQL type reference. Filter by property The following sections show some common scalar filters, filters that work on string, dateTime, and numeric values. These filters return only the resources that have some specified property or property range.\nbetween dates The between property returns items within time ranges for a specific property. This query returns job responses that started between January 01, 2023 and January 05, 2023.\nquery { queryJobResponse( filter: { effectiveStart: { between: { min: \"2023-01-01\", max: \"2023-01-05\" } } } ) { id effectiveStart } } has property The has keyword returns results only if an item has the specified field. For example, this query returns only equipment items that have been modified at least once.\nQueryResponse query QueryEquipment { queryEquipment(filter: {has: _modifiedOn}) { id _createdOn _modifiedOn } } { \"data\": { \"queryEquipment\": [ { \"id\": \"AN-19670-Equipment-1\", \"_createdOn\": \"2023-12-24T16:50:45Z\", \"_modifiedOn\": \"2024-01-23T20:06:30Z\" }, { \"id\": \"AN-19670-Equipment-2\", \"_createdOn\": \"2023-12-24T18:16:35Z\", \"_modifiedOn\": \"2024-01-23T20:06:35Z\" }, // more items ] } } To filter for items that have multiple properties, include the fields in an array. This query returns equipment objects that both have been modified and have next versions:\nquery QueryEquipment { queryEquipment(filter: {has: [_modifiedOn, nextVersion]}) { nextVersion _modifiedOn } } in this subset The in keyword filters for objects that have properties with specified values. For example, this query returns material lots that have material definitions that are either dough or cookie_unit.\nquery{ queryMaterialLot @cascade { id materialDefinition(filter: {id: {in: [\"dough\", \"cookie_unit\"]}}) { id } } } } regexp The regexp keyword searches for matches using the RE2 regular expression engine.\nFor example, this query uses a regular expression in its variables to filter for items that begin with either Kitchen_ or Cooling_ (case insensitive):\nquery getEquipment($filter: EquipmentFilter) { aggregateEquipment(filter: $filter) { count } queryEquipment(filter: $filter) { id } } Variables\n{ \"EquipmentFilter\": { \"id\": { \"regexp\": \"/|Kitchen_.*|Cooling_.*/i\" } }, } ⚠️ The regexp filters can have performance costs. After you refine a query filter to return exactly what you need, consider ways to simplify the regular expression or, if possible, use a different filter. Combine filters with and, or, not To filter by multiple properties, use the and, or, and not, operators. GraphQL syntax uses infix notation, so: “a and b” is a, and: { b }, “a or b or c” is a, or: { b, or: c }, and “not” is a prefix (not:).\nthis and that property The and operator filters for objects that include all specified properties.\nFor example, this query returns equipment objects that match two properties:\nThe effectiveStart must be the 1st and the 10th of January, 2024. It must have a non-null nextVersion. The and function is implicit unless you are searching on the same field. So this filter has an implied and:\nquery{ queryEquipment(filter: { effectiveStart: { between: {min: \"2024-01-01\", max: \"2024-01-10\"} } has: nextVersion } ) { effectiveStart id nextVersion } } ℹ️ This preceding filter syntax is a shorter equivalent to and: {has: nextVersion}. One or more properties The or operator filters for objects that have at least one of the specified properties. For example, you can take the preceding query and modify it so that it returns objects that have an effective start between the specified range or a nextVersion property (inclusive).\nqueryEquipment(filter: { effectiveStart: { between: {min: \"2024-01-01\", max: \"2024-01-10\"} } or: {has: nextVersion} } ) not these properties The not operator filters for objects that do not contain the specified property. For example, you can take the preceding query and modify it so that it returns objects that have an effective start between the specified range and do not have a nextVersion property:\nqueryEquipment(filter: { effectiveStart: { between: {min: \"2024-01-01\", max: \"2024-01-10\"} } not: {has: nextVersion} } ) To modify this to include both objects within the range and objects that do not have a nextVersion, use or with not:\nor: { not: {has: nextVersion} } This list of filters The and and or operators accept lists of filters. For example, this query filters for equipment objects whose id matches A, B, or C:\nqueryEquipment (filter: { or: [ { id: { eq: \"A\" } }, { id: { eq: \"B\" } }, { id: { eq: \"C\" } }, ] }) ","filter-by-property#Filter by property":"","use-directives#Use directives":"Rhize offers query directives, special instructions about how to look up and return values in a query. These directives can extend your filtering to look at nested properties or to conditionally display a field.\nAll directives begin with the @ sign.\nCascade The @cascade directive filters for certain nodes within a query. Use it to filter requested resources by a nested sub-property, similar to a WHERE clause in SQL.\n@cascade is not as performant as flatter queries. Consider using it only after you’ve exhausted other query structures to return the data you want. For example, this query filters for job responses with an ID of 12341, and then filters that set for only the items that have a data.properyLabel field with a value of INSTANCE ID.\nQueryResponse query QueryJobResponse($filter: JobResponseFilter, $propertyLabel: String) { queryJobResponse(filter: $filter) @cascade(fields:[\"data\"]){ id iid data(filter: { label: { anyoftext: $propertyLabel } }) { id iid label value } } } Variables:\n{ \"filter\": { \"id\": { \"alloftext\": \"12341\" } }, \"propertyLabel\": \"INSTANCE ID\" } Avoid using @cascade with the order argument The order argument returns only the first 1000 records of the query. If a record matches the @cascade filter but comes after these first 1000 records, the API does not return it.\nFor example, this query logic works as follows:\nReturn the first 1000 records of equipment as ordered by effectiveStart. From these 1000 records, return only the equipment items that are part of parentEquipment1. query($filter: EquipmentFilter){ queryEquipment (filter: { order: {desc:effectiveStart}) @cascade{ id isPartOf (filter: {id:{eq:\"parentEquipment1\"}}) { id } } } This behavior can be surprising and undesirable, so avoid @cascade with the order argument.\nInclude The @include directive returns a field only if its variable is true.\nFor example, when includeIf is true, this query omits specified values for versions.\nQueryResponse query($includeIf: Boolean!) { queryEquipment { id versions @include(if: $includeIf) { id } } } Change to true to include versions fields.\n{ \"includeIf\": false } "},"title":"Filter"},"/how-to/gql/mutate/":{"data":{"":"","add#\u003ccode\u003eadd\u003c/code\u003e":"","deep-mutations#Deep mutations":"You can perform deep mutations at multiple levels. Deep mutations don’t alter linked objects but can add nested new objects or link to existing objects.\nFor example, this mutation creates a new version of equipment, and associates a new item of equipment with it. Both the equipmentVersion and the equipment did not exist in the database.\nmutation AddEquipmentVersion($addEquipmentVersionInput2: [AddEquipmentVersionInput!]!) { addEquipmentVersion(input: $addEquipmentVersionInput2) { equipmentVersion { id equipment { id } } } } Variables:\n\"addEquipmentVersionInput2\": { \"id\": \"widget_machine_version_1\", \"version\": \"1\", \"versionStatus\": \"DRAFT\", \"equipment\": { \"id\": \"widget_maker_1\", \"label\": \"Widget maker 1\" } } } You can confirm that the record and its nested property exists with a get query. If the preceding operation succeeded, this query returns both the new Widget Maker and its corresponding version:\nQueryResponse query{ getEquipment(id: \"widget_maker_1\") { id versions{ id version } } } { \"addEquipmentVersionInput2\": { \"id\": \"widget_machine_version_1\", \"version\": \"1\", \"versionStatus\": \"DRAFT\", \"equipment\": { \"id\": \"widget_maker_1\", \"label\": \"Widget maker 1\" } } } To update an existing nested object, use the update mutation for its type.","delete#\u003ccode\u003edelete\u003c/code\u003e":" 🎥Watch:Add manufacturing data through GraphQL Mutations change the database in someway by creating, updating, or deleting a resource. You might use a mutation to update a personnel class, or in to a BPMN workflow that automatically creates records of incoming material lots.\nRhize supports the following ways to change the API.\nadd ℹ️ The add operation corresponds to the Process verb defined in Part 5 of the ISA-95 standard. Mutations that start with add create a resource on the server.\nFor example, this mutation adds one more items of equipment. To add multiple, send the variable as an array of objects, rather than a single object. The numUids property reports how many new objects were created.\nMutationcreate 1 Create many mutation AddEquipment($input: [AddEquipmentInput!]!) { addEquipment(input: $input) { equipment { id label } numUids } } { \"input\": { \"id\": \"Kitchen_mixer_a_20\", \"label\": \"Kitchen mixer A11\" } } { \"input\": [{ \"id\": \"Kitchen_mixer_b_01\", \"label\": \"Kitchen mixer A11\" },{ \"id\": \"Kitchen_mixer_b_02\", \"label\": \"Kitchen mixer A12\" }, ] } upsert Many add operations support upserting, which update or insert (create). That is, if the object already exists, the operation will update it with the additional fields. If the object doesn’t exist, the operation will create it.\nBesides general UX convenience, upsert is useful when data comes from multiple sources and in no guaranteed order, like from multiple streams from the message broker.\nTo enable upsert, set the upsert: argument to true:\naddEquipment(input: $input, upsert: true) update Mutations that start with update change something in an object that already exists. The update operations can use filters.\nℹ️ The update operation corresponds to the Change verb defined in Part 5 of the ISA-95 standard. For example, this operation updates the description for a specific version of an equipment item.\nmutation updateMixerVersion( $updateEquipmentVersionInput2: UpdateEquipmentVersionInput!){ updateEquipmentVersion(input: $updateEquipmentVersionInput2) { equipmentVersion { description id } } } Variables:\n{ \"updateEquipmentVersionInput2\": { \"filter\": {\"iid\":\"0xcc701\"}, \"set\": { \"description\": \"Second generation of the mixer 9000\" } } } delete ⚠️ Be careful! Without a Database backup, deleted items cannot be recovered. Mutations that start with delete remove a resource from the database. The delete operations can use filters.\nℹ️ The delete operation corresponds to the Cancel verb defined in Part 5 of the ISA-95 standard. For example, this operation deletes a unit of measure:\nmutation deleteUoM($filter: UnitOfMeasureFilter!){ deleteUnitOfMeasure(filter: $filter) { numUids } } Variables:\n{ \"filter\": { \"id\": { \"eq\": \"example unit of measure\" } } } ","update#\u003ccode\u003eupdate\u003c/code\u003e":""},"title":"Mutate"},"/how-to/gql/query/":{"data":{"":"","aggregate#\u003ccode\u003eAggregate\u003c/code\u003e data from multiple resources":"A query returns one or more resources from the database. Whether you want to investigate manufacturing processes or build a custom report, a good query is likely the foundation of your workflow.\nMost queries start with these three verbs, each of which indicates the resources to return.\nget for a single resource query for multiple resources aggregate for calculations on arrays ℹ️ These operations correspond to the Get verb defined in Part 5 of the ISA-95 standard. query multiple resources Queries that start with query return an array of objects. For example, a custom dashboard may use queryEquipmentVersion to create a page that displays all active versions of equipment that are running in a certain hierarchy scope.\nFor example, this query returns the ID of all pieces of equipment.\nquery allEquipment{ queryEquipment { id } } Query specified IDs To filter your query to a specific set of items, use the filter argument with the requested IDs.\nThe least verbose way to filter is to specify the requested items’ iid (their unique database addresses) in an array: For example, this query returns only equipment with an iid of 0xf9b49 or 0x102aa5.\nquery ExampleQuery { queryEquipment(filter: { iid: [\"0xf9b49\", \"0x102aa5\"] }) { iid id } } If you don’t have the precise iid, you can use one of the string filters.\nget single resource Queries that start with get return one object. A common use of get is to explore all data related to a particular object. For example, in a custom dashboard, you may use getDataSource to make a custom page that reports a specified data source.\nTypically, the argument specifies the resource by either its human-readable ID (id) or its unique address in the database (iid).\nFor example, this query gets the iid, _createdBy, and versions for the equipment item Kitchen_mixer_b_01:\nquery mixerCheck { getEquipment(id: \"Kitchen_mixer_b_01\") { iid _createdBy versions{ id } } } Aggregate data from multiple resources Operations that start with aggregate provide aggregated statistics for a specified set of items.\nThe syntax and filtering for an aggregate operation is the same as for a query operation. However, rather than returning items, the aggregate operation returns one or more computed statistics about these items. For example, you might use an aggregate query to create a summary report about a set of process segments within a certain time frame.\nThis request returns the count of all Equipment items that match a certain filter:\nquery countItems($filter: EquipmentFilter) { aggregateEquipment(filter: $filter) { count } } ","filter-queries#Filter queries":"Rhize also has many queries to filter or return subsets of items. To learn how to filter, read Use query filters.","get#\u003ccode\u003eget\u003c/code\u003e single resource":"","query#\u003ccode\u003equery\u003c/code\u003e multiple resources":"","sort-and-paginate#Sort and paginate":"A query can take arguments to order and paginate your results.\nℹ️ Without an order parameter, a query returns items without any default or guaranteed order. Order Ordered queries return only the first 1000 records of the ordered field. This behavior might exclude records that you expect, especially if you combine order with a @cascade filter in a nested field. The order argument works with any property whose type is Int, Float, String, or DateTime. For example, this query sorts Person objects by ID in ascending alphabetical order:\nquery{ queryPerson(order:{ asc: id}) { id } } And this orders by the Person’s effectiveStart date in descending chronological order.\nquery{ queryPerson(order:{ desc: effectiveStart}) { id effectiveStart } } Paginate with offset The offset argument specifies what item to start displaying results from, and the first argument specifies how many items to show.\nFor example, this skips the five most recent Person items (as measured by effectiveStart), and then displays the next 10:\nquery{ queryPerson(order:{ desc: effectiveStart }, offset: 5, first: 10 ) { id effectiveStart } } "},"title":"Query"},"/how-to/gql/subscribe/":{"data":{"":"The operations for a subscription are similar to the operations for a query. But rather than providing information about the entire item, the purpose of subscriptions is to notify about real-time changes to a manufacturing resource.\nℹ️ These operations correspond to the SyncGet verb defined in Part 5 of the ISA-95 standard. This example query subscribes to changes in a specified set of workResponses, reporting only their id and effective end time.\nsubscription GetWorkResponse($getWorkResponseId: String) { getWorkResponse(id: $getWorkResponseId){ jobResponses { effectiveEnd } } } Try to minimize the payload for subscription operations. Additionally, you need to subscribe only to changes that persist to the knowledge graph. For general event handling, it’s often better to use a BPMN workflow that subscribes to a NATS, MQTT, or OPC UA topic."},"title":"Subscribe"},"/how-to/kpi-service/":{"data":{"":" 📝The KPI service is currently provided through a separate Helm chart. If you want to use for your Rhize installation, get in touch. The KPI service records equipment-centric metrics related to the manufacturing operation. To use it, you must:\nRecord machine state data using the rule pipeline. Persist this data to a time-series database. "},"title":"Use the KPI service"},"/how-to/kpi-service/about-kpi-service/":{"data":{"":" 📝The KPI service is currently provided through a separate Helm chart. If you want to use for your Rhize installation, get in touch. Key Performance Indicators (KPIs) in manufacturing are metrics to help monitor, assess, and optimize the performance of various aspects of your production process.\nRhize has an optional KPI service that queries process values persisted to a time-series database and then calculates various KPIs. Rhize’s implementation of work calendars is inspired by ISO/TR 22400-10, a standard on KPIs in operations management.","supported-kpis#Supported KPIs":"The service supports all KPIs described by the ISO/TR 22400-10, along with some other useful KPIs:\nActualProductionTime ActualUnitSetupTime ActualSetupTime ActualUnitDelayTime ActualUnitDownTime TimeToRepair ActualUnitProcessingTime PlannedShutdownTime PlannedDownTime PlannedBusyTime Availability GoodQuantity ScrapQuantity ReworkQuantity ProducedQuantityMachineOrigin ProducedQuantity Effectiveness EffectivenessMachineOrigin QualityRatio OverallEquipmentEffectiveness ActualCycleTime ActualCycleTimeMachineOrigin ","what-the-service-does#What the service does":"sequenceDiagram actor U as User participant K as KPI Service participant TSDB as Time Series Database U-\u003e\u003eK: Query KPI in certain interval K-\u003e\u003eTSDB: Query State Records TSDB-\u003e\u003eK: Response: State records K-\u003e\u003eTSDB: Query Quantity Records TSDB-\u003e\u003eK: Response: Quantity records K-\u003e\u003eTSDB: Query JobResponse Records TSDB-\u003e\u003eK: Response: JobResponse records K--\u003e\u003eTSDB: (Optional:) Query Planned Downtime Records TSDB--\u003e\u003eK: Response: Downtime Records K--\u003e\u003eTSDB: (Optional:) Query Shift Records TSDB--\u003e\u003eK: Response: Downtime Records K-\u003e\u003eK: Calculate KPIs K-\u003eU: Response: KPI Result The KPI service provides an interface in the graph database for the user to query a list of pre-defined KPIs on a piece of equipment in the equipmentHierarchy within a certain time interval. The service then queries the time-series database for all state changes, produced quantities, and job response data. With the returned data, the service calculates the KPI value and returns it to the user."},"title":"About KPI Service and overrides"},"/how-to/kpi-service/configure-kpi-service/":{"data":{"":" 📝The KPI service is currently provided through a separate Helm chart. If you want to use for your Rhize installation, get in touch. This guide shows you how to configure the time-series you need for the KPI service. It does not suggest how to persist these values.\nTo learn how the KPI service works, read About KPI service. Example use cases include OEE and various performance metrics.","prerequisites#Prerequisites":"Before you start, ensure you have the following:\nThe KPI service installed An equipmentHierarchy is configured ","procedure#Procedure":"In short, to configure the KPI Service, the procedure works as follows:\nPersist machine state records to the EquipmentState table Persist quantity records to the QuantityLog table Persist job response data to the JobOrderState table (Optional) Configure the calendar service to record planned downtime events and shift records to time series. Refer to Use work calendars ","record-job-response-records#Record job response records":"Job response records persist to JobOrderState and are used to identify the current planned cycle time of each part produced from the machine. When an operation starts, a record is created setting the planned cycle time. When the operation is finished, another record is created to reset the planned cycle time to 0.\nJobOrderState table schema SchemaStart operationEnd operation CREATE TABLE IF NOT EXISTS JobOrderState( EquipmentId SYMBOL, JobOrderId SYMBOL, PlanningCycleTime FLOAT, -- Number of seconds per produced part time TIMESTAMP ) TIMESTAMP(time) PARTITION BY MONTH DEDUP UPSERT KEYS(time, EquipmentId, JobOrderId); [ { \"EquipmentId\": \"Machine A\", \"JobOrderId\": \"Order001\", \"PlanningCycleTime\": 100, \"time\": \"2024-04-02T14:32:21.947000Z\" } ] [ { \"EquipmentId\": \"Machine A\", \"JobOrderId\": \"Order001\", \"PlanningCycleTime\": 0, \"time\": \"2024-04-02T14:59:58.947000Z\" } ] ","record-machine-states#Record machine states":"Every time an equipment changes state, it is persisted to the time-series table EquipmentState.\nEquipmentState table schema SchemaExample CREATE TABLE IF NOT EXISTS EquipmentState( EquipmentId SYMBOL, ISO22400State VARCHAR, -- ADOT, AUST, ADET, APT time TIMESTAMP ) TIMESTAMP(time) PARTITION BY MONTH DEDUP UPSERT KEYS(time, EquipmentId); ℹ️ This table shows a QuestDB specific schema. You may also add additional columns as required.\nTo use the service for another time-series DB, get in touch.\n[ { \"EquipmentId\": \"Machine A\", \"ISO22400State\": \"ADET\", \"PackMLState\": \"Held\", \"time\": \"2024-03-28T13:13:47.814086Z\", } ] ℹ️ This record includes an additional field, PackMLState, to show that additional data can also be recorded. ","record-quantity-records#Record quantity records":"You can persist two categories of quantity records:\n(Optional) Values generated by the machine. Final produced quantities (these should be categorised into Good, Scrap, and Rework). QuantityLog table schema Schema Machine example User Example CREATE TABLE IF NOT EXISTS QuantityLog( EquipmentId SYMBOL, Origin SYMBOL, -- Machine, User QtyType SYMBOL, -- Delta, RunningTotal (running total not currently supported) ProductionType SYMBOL, -- Good, Scrap, Rework Qty FLOAT, time TIMESTAMP ) TIMESTAMP(time) PARTITION BY MONTH DEDUP UPSERT KEYS(time, EquipmentId, Origin, QtyType, ProductionType); [ { \"EquipmentId\": \"Machine A\", \"Origin\": \"Machine\", \"QtyType\": \"Delta\", \"ProductionType\": \"Unknown\", \"Qty\": 6, \"time\": \"2024-03-28T09:30:34.000325Z\" } ] [ { \"EquipmentId\": \"Machine A\", \"Origin\": \"User\", \"QtyType\": \"Delta\", \"ProductionType\": \"Good\", \"Qty\": 10, \"time\": \"2024-03-28T09:30:34.000325Z\" }, { \"EquipmentId\": \"Machine A\", \"Origin\": \"User\", \"QtyType\": \"Delta\", \"ProductionType\": \"Scrap\", \"Qty\": 2, \"time\": \"2024-03-28T09:30:34.000325Z\" }, { \"EquipmentId\": \"Machine A\", \"Origin\": \"User\", \"QtyType\": \"Delta\", \"ProductionType\": \"Rework\", \"Qty\": 1, \"time\": \"2024-03-28T09:30:34.000325Z\" } ] "},"title":"Configure the KPI service"},"/how-to/kpi-service/query-kpi-service/":{"data":{"":" 📝The KPI service is currently provided through a separate Helm chart. If you want to use for your Rhize installation, get in touch. The KPI service offers a federated GraphQL interface to query KPI values. This guide provides information on the different querying interfaces.","additional-filters#Additional Filters":"Some KPI Queries provide additional filters that are not mentioned in the preceding examples:\nignorePlannedDownTime (default: false) - Ignores planned down time events. For example if a state change happens while in the planned downtime calendar state, by default it is ignored. If ignorePlannedDowntime = true, the underlying state change is still returned. ignorePlannedShutdownTime (default: false). Similar to ignorePlannedDowntime except with planned shutdown calendar events. onlyIncludeActiveJobResponses (default: false) - if set to true will adjust the time interval of the KPI query to only be whilst a job response is active. For example if a user queries a KPI between 00:00 - 23:59 but there are only active job responses from 08:00-19:00, the query time range would be adjusted to 08:00-19:00. ","federated-queries#Federated Queries":"The KPI service extends the equipment, work schedule, work request, job order, and job response GraphQL entities with a KPI object. This makes KPIs easier to query.\nQuery Equipment Extending the equipment type allows the equipment ID to be inferred from parent equipment type\nQueryResponse query:\nquery QueryEquipment($startDateTime: DateTime!, $endDateTime: DateTime!, $kpi: [KPI!], $ignorePlannedDownTime: Boolean, $ignorePlannedShutdownTime: Boolean) { queryEquipment { id kpi(startDateTime: $startDateTime, endDateTime: $endDateTime, kpi: $kpi, ignorePlannedDownTime: $ignorePlannedDownTime, ignorePlannedShutdownTime: $ignorePlannedShutdownTime) { name from to error value units } } } input:\n{ \"startDateTime\": \"2024-09-01T06:00:00Z\", \"endDateTime\": \"2024-09-01T14:00:00Z\", \"kpi\": [\"ActualProductionTime\"], \"ignorePlannedDownTime\": false, \"ignorePlannedShutdownTime\": false } { \"data\": { \"queryEquipment\": [ { \"id\": \"Machine A\", \"kpi\": [ { \"name\": \"ActualProductionTime\", \"from\": \"2024-09-01T06:00:00Z\", \"to\": \"2024-09-01T14:00:00Z\", \"error\": null, \"value\": 27000, \"units\": \"seconds\" } ] }, { \"id\": \"Machine B\", \"kpi\": [ { \"name\": \"ActualProductionTime\", \"from\": \"2024-09-01T06:00:00Z\", \"to\": \"2024-09-01T14:00:00Z\", \"error\": null, \"value\": 27000, \"units\": \"seconds\" } ] } ] } } Query JobResponse Extending the job response type allows:\nstartDateTime to be inferred from jobResponse.startDateTime endDateTime to be inferred from jobResponse.endDateTime equipmentIds to be inferred from jobResponse.equipmentActual.EquipmentVersion.id QueryResponse query:\nquery QueryJobResponse($kpi: [KPI!], $ignorePlannedDownTime: Boolean, $ignorePlannedShutdownTime: Boolean, $filter: KPIFilter) { queryJobResponse { id startDateTime endDateTime equipmentActual { id equipmentVersion { id } } kpi(kpi: $kpi, ignorePlannedDownTime: $ignorePlannedDownTime, ignorePlannedShutdownTime: $ignorePlannedShutdownTime, filter: $filter) { name from to error value units } } } input:\n{ \"kpi\": [ \"ActualProductionTime\" ], \"ignorePlannedDownTime\": false, \"ignorePlannedShutdownTime\": false } { \"data\": { \"queryJobResponse\": [ { \"id\": \"Order A1.JobResponse 1\", \"startDateTime\": \"2024-09-01T08:00:00Z\", \"endDateTime\": \"2024-09-01T14:00:00Z\", \"equipmentActual\": [ { \"id\": \"Machine A.2024-09-01T08:00:00Z\", \"equipmentVersion\": { \"id\": \"Machine A\" } } ], \"kpi\": [ { \"name\": \"ActualProductionTime\", \"from\": \"2024-09-01T08:00:00Z\", \"to\": \"2024-09-01T14:00:00Z\", \"error\": null, \"value\": 27000, \"units\": \"seconds\" } ] } ] } } Query Job Order, Work Request, and Work Schedule Extending the Job order, Work Request, and Work Schedule entities makes it possible to recursively query all of the attached job responses:\nflowchart TD WorkSchedule --\u003e WorkRequests WorkRequests --\u003e JobOrders JobOrders --\u003e JobResponses Imagine that from the data from example 2 has this hierarchy:\nflowchart TD WorkScheduleA --\u003e WorkRequestA WorkScheduleA --\u003e WorkRequestB WorkRequestA --\u003e OrderA1 WorkRequestA --\u003e OrderA2 WorkRequestB --\u003e OrderB1 WorkRequestB --\u003e OrderC1 Querying KPI on workSchedule A combines all results for order A1, A2, B1 and C1:\nQueryResponse query:\nquery QueryWorkSchedule($kpi: [KPI!], $ignorePlannedDownTime: Boolean, $ignorePlannedShutdownTime: Boolean, $filter: KPIFilter) { queryWorkSchedule { id kpi(kpi: $kpi, ignorePlannedDownTime: $ignorePlannedDownTime, ignorePlannedShutdownTime: $ignorePlannedShutdownTime, filter: $filter) { name from to error value units } } } input:\n{ \"kpi\": [ \"ActualProductionTime\" ], \"ignorePlannedDownTime\": false, \"ignorePlannedShutdownTime\": false } { \"data\": { \"queryWorkSchedule\": [ { \"id\": \"WorkScheduleA\", \"kpi\": [ { \"name\": \"ActualProductionTime\", \"from\": \"2024-09-01T08:00:00Z\", \"to\": \"2024-09-02T06:00:00Z\", \"error\": null, \"value\": 108000, \"units\": \"seconds\" } ] } ] } } ","root-level-queries#Root level queries":"The KPI service offers two root-level queries:\nGetKPI() GetKPIByShift() GetKPI() The GetKPI() query is the base-level KPI Query. You can use it to input an equipment ID or hierarchy-scope ID, a time range, and a list of desired KPIs. The result is a single KPI object per requested KPI.\nGetKPI() - Definition queryresponse query:\nquery GetKPI($filterInput: KPIFilter!, $startDateTime: DateTime!, $endDateTime: DateTime!, $kpi: [KPI!], $ignorePlannedDownTime: Boolean, $ignorePlannedShutdownTime: Boolean) { GetKPI(filterInput: $filterInput, startDateTime: $startDateTime, endDateTime: $endDateTime, kpi: $kpi, ignorePlannedDownTime: $ignorePlannedDownTime, ignorePlannedShutdownTime: $ignorePlannedShutdownTime) { name to from error value units } } input:\n{ \"filterInput\": { \"equipmentIds\": [\"MachineA\", \"MachineB\"], \"hierarchyScopeId\": \"Enterprise1.SiteA.Line1\" }, \"startDateTime\": \"2024-09-01T00:00:00Z\", \"endDateTime\": \"2024-09-01T18:00:00Z\", \"kpi\": [\"ActualProductionTime\",\"Availability\", \"GoodQuantity\", \"ProducedQuantity\", \"Effectiveness\", \"QualityRatio\", \"ActualCycleTime\", \"OverallEquipmentEffectiveness\"], \"ignorePlannedDownTime\": false, \"ignorePlannedShutdownTime\": false, \"onlyIncludeActiveJobResponses\": false } { \"data\": { \"GetKPI\": [ { \"name\": \"ActualProductionTime\", \"to\": \"2024-09-01T18:00:00Z\", \"from\": \"2024-09-01T00:00:00Z\", \"error\": null, \"value\": 0, \"units\": \"seconds\" }, { \"name\": \"ActualUnitDelayTime\", \"to\": \"2024-09-01T18:00:00Z\", \"from\": \"2024-09-01T00:00:00Z\", \"error\": null, \"value\": 0, \"units\": \"seconds\" }, { \"name\": \"PlannedDownTime\", \"to\": \"2024-09-01T18:00:00Z\", \"from\": \"2024-09-01T00:00:00Z\", \"error\": null, \"value\": 0, \"units\": \"seconds\" }, { \"name\": \"Availability\", \"to\": \"2024-09-01T18:00:00Z\", \"from\": \"2024-09-01T00:00:00Z\", \"error\": null, \"value\": 0, \"units\": \"%\" }, { \"name\": \"GoodQuantity\", \"to\": \"2024-09-01T18:00:00Z\", \"from\": \"2024-09-01T00:00:00Z\", \"error\": null, \"value\": 0, \"units\": \"units\" }, { \"name\": \"ProducedQuantity\", \"to\": \"2024-09-01T18:00:00Z\", \"from\": \"2024-09-01T00:00:00Z\", \"error\": null, \"value\": 0, \"units\": \"units\" }, { \"name\": \"Effectiveness\", \"to\": \"2024-09-01T18:00:00Z\", \"from\": \"2024-09-01T00:00:00Z\", \"error\": null, \"value\": 100, \"units\": \"%\" }, { \"name\": \"QualityRatio\", \"to\": \"2024-09-01T18:00:00Z\", \"from\": \"2024-09-01T00:00:00Z\", \"error\": null, \"value\": 100, \"units\": \"%\" }, { \"name\": \"ActualCycleTime\", \"to\": \"2024-09-01T18:00:00Z\", \"from\": \"2024-09-01T00:00:00Z\", \"error\": null, \"value\": 0, \"units\": \"seconds per unit\" }, { \"name\": \"OverallEquipmentEffectiveness\", \"to\": \"2024-09-01T18:00:00Z\", \"from\": \"2024-09-01T00:00:00Z\", \"error\": null, \"value\": 0, \"units\": \"%\" } ] } } Example 1. Imagine a scenario where Machine A produces parts at a planned cycle time of 10-seconds per part. The order starts at 09:00 and finishes at 12:00 with 30 minutes of unplanned downtime in between (this could be from loading materials, unplanned maintenance, switching tools, and so on). After the operation finishes, the user has registered 800 Good parts and 200 scrap parts. The tables in time series appear as follows:\nEquipmentstateQuantityLogJobOrderState EquipmentId ISO22400State time Machine A APT 2024-09-03T09:00:00Z Machine A ADET 2024-09-03T10:30:00Z Machine A APT 2024-09-03T11:00:00Z Machine A ADOT 2024-09-03T12:00:00Z EquipmentId Origin QtyType ProductionType Qty time Machine A User Delta Good 800 2024-09-03T12:00:00Z Machine A User Delta Scrap 200 2024-09-03T12:00:00Z EquipmentId JobOrderId PlanningCyleTime time Machine A Order A 10 2024-09-03T09:00:00Z Machine A NONE 0 2024-09-03T12:00:00Z Calling this KPI Query appears as follows:\nQueryResponse query:\nquery GetKPI($filterInput: KPIFilter!, $startDateTime: DateTime!, $endDateTime: DateTime!, $kpi: [KPI!], $ignorePlannedDownTime: Boolean, $ignorePlannedShutdownTime: Boolean) { GetKPI(filterInput: $filterInput, startDateTime: $startDateTime, endDateTime: $endDateTime, kpi: $kpi, ignorePlannedDownTime: $ignorePlannedDownTime, ignorePlannedShutdownTime: $ignorePlannedShutdownTime) { name to from error value units } } input:\n{ \"filterInput\": { \"equipmentIds\": [\"MachineA\"] }, \"startDateTime\": \"2024-09-03T09:00:00Z\", \"endDateTime\": \"2024-09-03T12:00:00Z\", \"kpi\": [\"ActualProductionTime\",\"Availability\", \"GoodQuantity\", \"ProducedQuantity\", \"Effectiveness\", \"QualityRatio\", \"ActualCycleTime\", \"OverallEquipmentEffectiveness\"] } { \"data\": { \"GetKPI\": [ { \"_comment\": \"This is the total time spent in APT\", \"name\": \"ActualProductionTime\", \"to\": \"2024-09-01T18:00:00Z\", \"from\": \"2024-09-01T00:00:00Z\", \"error\": null, \"value\": 9000, \"units\": \"seconds\" }, { \"_comment\": \"This is the total time spent in ADET\", \"name\": \"ActualUnitDelayTime\", \"to\": \"2024-09-01T18:00:00Z\", \"from\": \"2024-09-01T00:00:00Z\", \"error\": null, \"value\": 1800, \"units\": \"seconds\" }, { \"_comment\": \"This is the total time spent in PDOT\", \"name\": \"PlannedDownTime\", \"to\": \"2024-09-01T18:00:00Z\", \"from\": \"2024-09-01T00:00:00Z\", \"error\": null, \"value\": 0, \"units\": \"seconds\" }, { \"_comment\": \"This is APT/PBT\", \"name\": \"Availability\", \"to\": \"2024-09-01T18:00:00Z\", \"from\": \"2024-09-01T00:00:00Z\", \"error\": null, \"value\": 83.3333333, \"units\": \"%\" }, { \"_comment\": \"This is the total recorded good quantity\", \"name\": \"GoodQuantity\", \"to\": \"2024-09-01T18:00:00Z\", \"from\": \"2024-09-01T00:00:00Z\", \"error\": null, \"value\": 800, \"units\": \"units\" }, { \"_comment\": \"This is the total quantity produced in the order\", \"name\": \"ProducedQuantity\", \"to\": \"2024-09-01T18:00:00Z\", \"from\": \"2024-09-01T00:00:00Z\", \"error\": null, \"value\": 1000, \"units\": \"units\" }, {\"_comment\": \"This is (ProducedQuantity * PlannedCycleTime)/APT\", \"name\": \"Effectiveness\", \"to\": \"2024-09-01T18:00:00Z\", \"from\": \"2024-09-01T00:00:00Z\", \"error\": null, \"value\": 111.111111, \"units\": \"%\" }, { \"_comment\": \"This is GoodQuantity/ProducedQuantity\", \"name\": \"QualityRatio\", \"to\": \"2024-09-01T18:00:00Z\", \"from\": \"2024-09-01T00:00:00Z\", \"error\": null, \"value\": 80, \"units\": \"%\" }, { \"_comment\": \"This is APT/ProducedQuantity\", \"name\": \"ActualCycleTime\", \"to\": \"2024-09-01T18:00:00Z\", \"from\": \"2024-09-01T00:00:00Z\", \"error\": null, \"value\": 10.8, \"units\": \"seconds per unit\" }, { \"_comment\": \"This is Availability * Effectiveness * QualityRatio\", \"name\": \"OverallEquipmentEffectiveness\", \"to\": \"2024-09-01T18:00:00Z\", \"from\": \"2024-09-01T00:00:00Z\", \"error\": null, \"value\": 74.074, \"units\": \"%\" } ] } } GetKPIByShift() The GetKPIByShift() query is another base-level KPI Query. It is similar to GetKPI(), but rather than returning a single result per KPI query, it also accepts WorkCalendarEntryProperty IDs to filter against and return a result for each instance of a shift.\nGetKPIByShift() - Definition QueryResponse query:\nquery GetKPIByShift($filterInput: GetKPIByShiftFilter!, $startDateTime: DateTime!, $endDateTime: DateTime!, $kpi: [KPI!], $ignorePlannedDownTime: Boolean, $ignorePlannedShutdownTime: Boolean, $groupByShift: Boolean, $groupByEquipment: Boolean, $onlyIncludeActiveJobResponses: Boolean) { GetKPIByShift(filterInput: $filterInput, startDateTime: $startDateTime, endDateTime: $endDateTime, kpi: $kpi, ignorePlannedDownTime: $ignorePlannedDownTime, ignorePlannedShutdownTime: $ignorePlannedShutdownTime, groupByShift: $groupByShift, groupByEquipment: $groupByEquipment, OnlyIncludeActiveJobResponses: $onlyIncludeActiveJobResponses) { name equipmentIds shiftsContained from to error value units } } input:\n{ \"filterInput\": { \"shiftFilter\": [ { \"propertyName\": \"Shift Name\", \"eq\": \"Morning\" } ], \"equipmentIds\": [\"Machine A\", \"Machine B\"], \"hierarchyScopeId\": \"Enterprise1.SiteA.Line1\" }, \"startDateTime\": \"2024-09-01T00:00:00Z\", \"endDateTime\": \"2024-09-03T18:00:00Z\", \"kpi\": [\"ActualProductionTime\", \"OverallEquipmentEffectiveness\"], \"ignorePlannedDownTime\": false, \"ignorePlannedShutdownTime\": false, \"onlyIncludeActiveJobResponses\": false, \"groupByShift\": false, \"groupByEquipment\": true } { \"data\": { \"GetKPIByShift\": [ { \"name\": \"ActualProductionTime\", \"equipmentIds\": [\"Machine A\", \"Machine B\"], \"shiftsContained\": [\"Shift.Sunday.Morning\"], \"from\": \"2024-09-01T09:00:00Z\", \"to\": \"2024-09-01T17:00:00Z\", \"error\": null, \"value\": 0, \"units\": \"seconds\" }, { \"name\": \"ActualProductionTime\", \"equipmentIds\": [\"Machine A\", \"Machine B\"], \"shiftsContained\": [\"Shift.Monday.Morning\"], \"from\": \"2024-09-02T00:00:00Z\", \"to\": \"2024-09-02T17:00:00Z\", \"error\": null, \"value\": 0, \"units\": \"seconds\" }, { \"name\": \"ActualProductionTime\", \"equipmentIds\": [\"Machine A\", \"Machine B\"], \"shiftsContained\": [\"Shift.Tuesday.Morning\"], \"from\": \"2024-09-03T00:00:00Z\", \"to\": \"2024-09-03T17:00:00Z\", \"error\": null, \"value\": 0, \"units\": \"seconds\" }, { \"name\": \"OverallEquipmentEffectiveness\", \"equipmentIds\": [\"Machine A\", \"Machine B\"], \"shiftsContained\": [\"Shift.Sunday.Morning\"], \"from\": \"2024-09-01T09:00:00Z\", \"to\": \"2024-09-01T17:00:00Z\", \"error\": null, \"value\": 0, \"units\": \"%\" }, { \"name\": \"OverallEquipmentEffectiveness\", \"equipmentIds\": [\"Machine A\", \"Machine B\"], \"shiftsContained\": [\"Shift.Monday.Morning\"], \"from\": \"2024-09-02T00:00:00Z\", \"to\": \"2024-09-02T17:00:00Z\", \"error\": null, \"value\": 0, \"units\": \"%\" }, { \"name\": \"OverallEquipmentEffectiveness\", \"equipmentIds\": [\"Machine A\", \"Machine B\"], \"shiftsContained\": [\"Shift.Tuesday.Morning\"], \"from\": \"2024-09-03T00:00:00Z\", \"to\": \"2024-09-03T17:00:00Z\", \"error\": null, \"value\": 0, \"units\": \"%\" }, ] } } Example 2 Following on from Example 1. Machine A exists on a production line alongside Machine B, they both produce parts with a planned cycle time of 10 seconds per part and runs on the same shift pattern. The work calendar service is configured with 3 distinct daily shifts:\nMorning (06:00-14:00) Afternoon (14:00 - 22:00) Night (22:00-06:00) Which results in the following tables:\nEquipmentStateQuantityLogCalendar EquipmentId ISO22400State time Machine A APT 2024-09-01T06:00:00Z Machine B APT 2024-09-01T06:00:00Z Machine A ADET 2024-09-01T10:30:00Z Machine B ADET 2024-09-01T10:30:00Z Machine A APT 2024-09-01T11:00:00Z Machine B APT 2024-09-01T11:00:00Z Machine A ADOT 2024-09-01T14:00:00Z Machine B ADOT 2024-09-01T14:00:00Z Machine A APT 2024-09-01T14:00:00Z Machine B APT 2024-09-01T14:00:00Z Machine A ADET 2024-09-01T17:30:00Z Machine B ADET 2024-09-01T17:30:00Z Machine A APT 2024-09-01T18:00:00Z Machine B APT 2024-09-01T18:00:00Z Machine A ADOT 2024-09-01T22:00:00Z Machine B ADOT 2024-09-01T22:00:00Z Machine A APT 2024-09-01T22:00:00Z Machine B APT 2024-09-01T22:00:00Z Machine A ADET 2024-09-02T04:00:00Z Machine B ADET 2024-09-02T04:00:00Z Machine A APT 2024-09-02T04:30:00Z Machine B APT 2024-09-02T04:30:00Z Machine A ADOT 2024-09-02T06:00:00Z Machine B ADOT 2024-09-02T06:00:00Z Machine A APT 2024-09-02T06:00:00Z Machine B APT 2024-09-02T06:00:00Z Machine A ADET 2024-09-02T10:30:00Z Machine B ADET 2024-09-02T10:30:00Z Machine A APT 2024-09-02T11:00:00Z Machine B APT 2024-09-02T11:00:00Z Machine A ADOT 2024-09-02T14:00:00Z Machine B ADOT 2024-09-02T14:00:00Z Machine A APT 2024-09-02T14:00:00Z Machine B APT 2024-09-02T14:00:00Z Machine A ADET 2024-09-02T18:30:00Z Machine B ADET 2024-09-02T18:30:00Z Machine A APT 2024-09-02T19:00:00Z Machine B APT 2024-09-02T19:00:00Z Machine A ADOT 2024-09-02T22:00:00Z Machine B ADOT 2024-09-02T22:00:00Z Machine A APT 2024-09-02T22:00:00Z Machine B APT 2024-09-02T22:00:00Z Machine A ADET 2024-09-03T04:30:00Z Machine B ADET 2024-09-03T04:30:00Z Machine A APT 2024-09-03T05:00:00Z Machine B APT 2024-09-03T05:00:00Z Machine A ADOT 2024-09-03T06:00:00Z Machine B ADOT 2024-09-03T06:00:00Z EquipmentId Origin QtyType ProductionType Qty time Machine A User Delta Good 800 2024-09-01T14:00:00Z Machine A User Delta Scrap 200 2024-09-01T14:00:00Z Machine B User Delta Good 700 2024-09-01T14:00:00Z Machine B User Delta Scrap 300 2024-09-01T14:00:00Z Machine A User Delta Good 900 2024-09-01T22:00:00Z Machine A User Delta Scrap 100 2024-09-01T22:00:00Z Machine B User Delta Good 950 2024-09-01T22:00:00Z Machine B User Delta Scrap 50 2024-09-01T22:00:00Z Machine A User Delta Good 999 2024-09-01T06:00:00Z Machine A User Delta Scrap 1 2024-09-01T06:00:00Z Machine B User Delta Good 900 2024-09-01T06:00:00Z Machine B User Delta Scrap 100 2024-09-01T06:00:00Z Machine A User Delta Good 850 2024-09-02T14:00:00Z Machine A User Delta Scrap 150 2024-09-02T14:00:00Z Machine B User Delta Good 800 2024-09-02T14:00:00Z Machine B User Delta Scrap 200 2024-09-02T14:00:00Z Machine A User Delta Good 700 2024-09-02T22:00:00Z Machine A User Delta Scrap 300 2024-09-02T22:00:00Z Machine B User Delta Good 750 2024-09-02T22:00:00Z Machine B User Delta Scrap 250 2024-09-02T22:00:00Z Machine A User Delta Good 600 2024-09-02T06:00:00Z Machine A User Delta Scrap 400 2024-09-02T06:00:00Z Machine B User Delta Good 750 2024-09-02T06:00:00Z Machine B User Delta Scrap 250 2024-09-02T06:00:00Z EquipmentId JobOrderId PlanningCyleTime time Machine A Order A1 10 2024-09-01T06:00:00Z Machine B Order A2 10 2024-09-01T06:00:00Z Machine A NONE 0 2024-09-01T14:00:00Z Machine B NONE 0 2024-09-01T14:00:00Z Machine A Order B1 10 2024-09-01T14:00:00Z Machine B Order B2 10 2024-09-01T14:00:00Z Machine A NONE 0 2024-09-01T22:00:00Z Machine B NONE 0 2024-09-01T22:00:00Z Machine A Order C1 10 2024-09-01T22:00:00Z Machine B Order C2 10 2024-09-01T22:00:00Z Machine A NONE 0 2024-09-02T06:00:00Z Machine B NONE 0 2024-09-02T06:00:00Z Machine A Order D1 10 2024-09-02T06:00:00Z Machine B Order D2 10 2024-09-02T06:00:00Z Machine A NONE 0 2024-09-02T14:00:00Z Machine B NONE 0 2024-09-02T14:00:00Z Machine A Order E1 10 2024-09-02T14:00:00Z Machine B Order E2 10 2024-09-02T14:00:00Z Machine A NONE 0 2024-09-02T22:00:00Z Machine B NONE 0 2024-09-02T22:00:00Z Machine A Order F1 10 2024-09-02T22:00:00Z Machine B Order F2 10 2024-09-02T22:00:00Z Machine A NONE 0 2024-09-03T06:00:00Z Machine B NONE 0 2024-09-03T06:00:00Z EquipmentId WorkCalendarDefinitionID WorkCalendarDefinitionEntryId EntryType time Machine A ShiftCalendar ShiftCalendar.Sunday.Morning START 2024-09-01T06:00:00Z Machine B ShiftCalendar ShiftCalendar.Sunday.Morning START 2024-09-01T06:00:00Z Machine A ShiftCalendar ShiftCalendar.Sunday.Morning END 2024-09-01T14:00:00Z Machine B ShiftCalendar ShiftCalendar.Sunday.Morning END 2024-09-01T14:00:00Z Machine A ShiftCalendar ShiftCalendar.Sunday.Afternoon START 2024-09-01T14:00:00Z Machine B ShiftCalendar ShiftCalendar.Sunday.Afternoon START 2024-09-01T14:00:00Z Machine A ShiftCalendar ShiftCalendar.Sunday.Afternoon END 2024-09-01T22:00:00Z Machine B ShiftCalendar ShiftCalendar.Sunday.Afternoon END 2024-09-01T22:00:00Z Machine A ShiftCalendar ShiftCalendar.Sunday.Night START 2024-09-01T22:00:00Z Machine B ShiftCalendar ShiftCalendar.Sunday.Night START 2024-09-01T22:00:00Z Machine A ShiftCalendar ShiftCalendar.Sunday.Night END 2024-09-02T06:00:00Z Machine B ShiftCalendar ShiftCalendar.Sunday.Night END 2024-09-02T06:00:00Z Machine A ShiftCalendar ShiftCalendar.Monday.Morning START 2024-09-02T06:00:00Z Machine B ShiftCalendar ShiftCalendar.Monday.Morning START 2024-09-02T06:00:00Z Machine A ShiftCalendar ShiftCalendar.Monday.Morning END 2024-09-02T14:00:00Z Machine B ShiftCalendar ShiftCalendar.Monday.Morning END 2024-09-02T14:00:00Z Machine A ShiftCalendar ShiftCalendar.Monday.Afternoon START 2024-09-02T14:00:00Z Machine B ShiftCalendar ShiftCalendar.Monday.Afternoon START 2024-09-02T14:00:00Z Machine A ShiftCalendar ShiftCalendar.Monday.Afternoon END 2024-09-02T22:00:00Z Machine B ShiftCalendar ShiftCalendar.Monday.Afternoon END 2024-09-02T22:00:00Z Machine A ShiftCalendar ShiftCalendar.Monday.Night START 2024-09-02T22:00:00Z Machine B ShiftCalendar ShiftCalendar.Monday.Night START 2024-09-02T22:00:00Z Machine A ShiftCalendar ShiftCalendar.Monday.Night END 2024-09-03T06:00:00Z Machine B ShiftCalendar ShiftCalendar.Monday.Night END 2024-09-03T06:00:00Z You can run this query in multiple ways:\ngroupByEquipment = false and groupByShift = false - returns a separate result per shift instance per equipment QueryResponse query:\nquery GetKPIByShift($filterInput: GetKPIByShiftFilter!, $startDateTime: DateTime!, $endDateTime: DateTime!, $kpi: [KPI!], $ignorePlannedDownTime: Boolean, $ignorePlannedShutdownTime: Boolean, $groupByShift: Boolean, $groupByEquipment: Boolean, $onlyIncludeActiveJobResponses: Boolean) { GetKPIByShift(filterInput: $filterInput, startDateTime: $startDateTime, endDateTime: $endDateTime, kpi: $kpi, ignorePlannedDownTime: $ignorePlannedDownTime, ignorePlannedShutdownTime: $ignorePlannedShutdownTime, groupByShift: $groupByShift, groupByEquipment: $groupByEquipment, OnlyIncludeActiveJobResponses: $onlyIncludeActiveJobResponses) { name equipmentIds shiftsContained from to error value units } } input:\n{ \"filterInput\": { \"shiftFilter\": [ { \"propertyName\": \"Shift Name\", \"eq\": \"Morning\" } ], \"equipmentIds\": [\"Machine A\", \"Machine B\"], }, \"startDateTime\": \"2024-09-01T00:00:00Z\", \"endDateTime\": \"2024-09-03T18:00:00Z\", \"kpi\": [\"ActualProductionTime\"], \"ignorePlannedDownTime\": false, \"ignorePlannedShutdownTime\": false, \"onlyIncludeActiveJobResponses\": false, \"groupByShift\": false, \"groupByEquipment\": false } { \"data\": { \"GetKPIByShift\": [ { \"name\": \"ActualProductionTime\", \"equipmentIds\": [\"Machine A\"], \"shiftsContained\": [\"Shift.Sunday.Morning\"], \"from\": \"2024-09-01T06:00:00Z\", \"to\": \"2024-09-01T14:00:00Z\", \"error\": null, \"value\": 27000, \"units\": \"seconds\" }, { \"name\": \"ActualProductionTime\", \"equipmentIds\": [\"Machine B\"], \"shiftsContained\": [\"Shift.Sunday.Morning\"], \"from\": \"2024-09-01T06:00:00Z\", \"to\": \"2024-09-01T14:00:00Z\", \"error\": null, \"value\": 27000, \"units\": \"seconds\" }, { \"name\": \"ActualProductionTime\", \"equipmentIds\": [\"Machine A\"], \"shiftsContained\": [\"Shift.Monday.Morning\"], \"from\": \"2024-09-02T06:00:00Z\", \"to\": \"2024-09-02T14:00:00Z\", \"error\": null, \"value\": 27000, \"units\": \"seconds\" }, { \"name\": \"ActualProductionTime\", \"equipmentIds\": [\"Machine B\"], \"shiftsContained\": [\"Shift.Monday.Morning\"], \"from\": \"2024-09-02T06:00:00Z\", \"to\": \"2024-09-02T14:00:00Z\", \"error\": null, \"value\": 27000, \"units\": \"seconds\" } ] } } groupByEquipment = true and groupByShift = false - returns a separate result per shift instance containing all equipment QueryResponse query:\nquery GetKPIByShift($filterInput: GetKPIByShiftFilter!, $startDateTime: DateTime!, $endDateTime: DateTime!, $kpi: [KPI!], $ignorePlannedDownTime: Boolean, $ignorePlannedShutdownTime: Boolean, $groupByShift: Boolean, $groupByEquipment: Boolean, $onlyIncludeActiveJobResponses: Boolean) { GetKPIByShift(filterInput: $filterInput, startDateTime: $startDateTime, endDateTime: $endDateTime, kpi: $kpi, ignorePlannedDownTime: $ignorePlannedDownTime, ignorePlannedShutdownTime: $ignorePlannedShutdownTime, groupByShift: $groupByShift, groupByEquipment: $groupByEquipment, OnlyIncludeActiveJobResponses: $onlyIncludeActiveJobResponses) { name equipmentIds shiftsContained from to error value units } } input:\n{ \"filterInput\": { \"shiftFilter\": [ { \"propertyName\": \"Shift Name\", \"eq\": \"Morning\" } ], \"equipmentIds\": [\"Machine A\", \"Machine B\"], }, \"startDateTime\": \"2024-09-01T00:00:00Z\", \"endDateTime\": \"2024-09-03T18:00:00Z\", \"kpi\": [\"ActualProductionTime\"], \"ignorePlannedDownTime\": false, \"ignorePlannedShutdownTime\": false, \"onlyIncludeActiveJobResponses\": false, \"groupByShift\": false, \"groupByEquipment\": true } { \"data\": { \"GetKPIByShift\": [ { \"name\": \"ActualProductionTime\", \"equipmentIds\": [\"Machine A\", \"Machine B\"], \"shiftsContained\": [\"Shift.Sunday.Morning\"], \"from\": \"2024-09-01T06:00:00Z\", \"to\": \"2024-09-01T14:00:00Z\", \"error\": null, \"value\": 54000, \"units\": \"seconds\" }, { \"name\": \"ActualProductionTime\", \"equipmentIds\": [\"Machine A\", \"Machine B\"], \"shiftsContained\": [\"Shift.Monday.Morning\"], \"from\": \"2024-09-02T06:00:00Z\", \"to\": \"2024-09-02T14:00:00Z\", \"error\": null, \"value\": 54000, \"units\": \"seconds\" } ] } } groupByEquipment = true and groupByShift = true - groups shifts and equipment together QueryResponse query:\nquery GetKPIByShift($filterInput: GetKPIByShiftFilter!, $startDateTime: DateTime!, $endDateTime: DateTime!, $kpi: [KPI!], $ignorePlannedDownTime: Boolean, $ignorePlannedShutdownTime: Boolean, $groupByShift: Boolean, $groupByEquipment: Boolean, $onlyIncludeActiveJobResponses: Boolean) { GetKPIByShift(filterInput: $filterInput, startDateTime: $startDateTime, endDateTime: $endDateTime, kpi: $kpi, ignorePlannedDownTime: $ignorePlannedDownTime, ignorePlannedShutdownTime: $ignorePlannedShutdownTime, groupByShift: $groupByShift, groupByEquipment: $groupByEquipment, OnlyIncludeActiveJobResponses: $onlyIncludeActiveJobResponses) { name equipmentIds shiftsContained from to error value units } } input:\n{ \"filterInput\": { \"shiftFilter\": [ { \"propertyName\": \"Shift Name\", \"eq\": \"Morning\" } ], \"equipmentIds\": [\"Machine A\", \"Machine B\"], }, \"startDateTime\": \"2024-09-01T00:00:00Z\", \"endDateTime\": \"2024-09-03T18:00:00Z\", \"kpi\": [\"ActualProductionTime\"], \"ignorePlannedDownTime\": false, \"ignorePlannedShutdownTime\": false, \"onlyIncludeActiveJobResponses\": false, \"groupByShift\": true, \"groupByEquipment\": true } { \"data\": { \"GetKPIByShift\": [ { \"name\": \"ActualProductionTime\", \"equipmentIds\": [\"Machine A\", \"Machine B\"], \"shiftsContained\": [\"Shift.Sunday.Morning\",\"Shift.Monday.Morning\"], \"from\": \"2024-09-01T06:00:00Z\", \"to\": \"2024-09-01T14:00:00Z\", \"error\": null, \"value\": 108000, \"units\": \"seconds\" } ] } } "},"title":"Query the KPI service"},"/how-to/model/":{"data":{"":"You have multiple ways to update your production models:\nUse the UI to define it for an individual unit or class POST a batch over the GraphQL API Use BPMN as a filter receive an incoming ERP document and map into the system The trade offs are usually upfront configuration time, number of items to add, and level of automation. Adding an item over the UI requires no programming skill, but you can only add only one unit at a time. Creating a BPMN process to listen for an event and automatically map new units brings the highest automation, but it requires upfront investment to write and test the BPMN workflow.\nCreate objects from the UIHow to create manufacturing objects from the Rhize UI. Master definitions and fieldsA reference of all manufacturing data objects and properties that you can create in the Rhize UI "},"title":"Define production models"},"/how-to/model/create-objects-ui/":{"data":{"":"To make a production object visible to the Rhize data hub, you must define it as a data model. Along with its API, Rhize also has a graphical interface to create and update objects in your role-based equipment hierarchy.\nOften, one object references another: for example, a piece of equipment may belong to an equipment class, have a unit of measure as a property, and be associated with process segment. These associations form nodes and edges in your knowledge graph, so the more information relationships that you accurately create, the better.","example-create-oven-class-and-instance#Example: create oven class and instance":"AG holdings is a fictional enterprise that makes product called Alleman Brownies. These brownies are produced in its UK site, AG_House, specifically in the brownie_kitchen_1 work center of the south_wing area.\nThe brownie_kitchen_1 kitchen has oven_123, an instance of the Oven class. This equipment item also has a data source that gives temperature readings, which are published to a dashboard.\nHere’s how you could use the Rhize UI to model this.\nℹ️ If you are actively following to learn, make sure to use names that will easily identify the objects as example objects for testing. Model the equipment levels:\nFrom Master Data, select Equipment and enter AG_house as the name.\nGive it a description. Then for Equipment Level, choose Site.\nFrom the new AG_House object, create a sub-object with the + button.\nName the object south_wing and choose Area as its level.\nRepeat the preceding steps to make brownie_kitchen1 a work center in the south_wing.\nOnce complete, the hierarchy should look like this:\nModel the Oven equipment class:\nFrom Master Data, select Equipment Class. Give it a name that makes sense for your organization. Give it a description, such as Oven for baking. Add any additional properties. Create . Make it active by changing its version state. Create the associated data source:\nFrom Master Data, select Data Source. Add the source’s connection string and protocol, along with any credentials (to configure authentication, refer to Agent configuration). Select the Topics tab and add the label and data type. Create and make the version active. Now, create an instance of the Oven.\nFrom Master Data, select Equipment. Then create a sub-object for the brownie_kitchen1 work center.\nAdd its unique, globally identifiable ID and give it a description.\nFor Equipment Class, add the Oven class you just created.\nFor Equipment Level, select WorkUnit.\nCreate.\nAfter the object is successfully created, you can add the data source.\nFrom the Data Sources tab, select Link Data Sources. Select the data source you just created.\nOn success, your UI should now have an item equipment that is associated with an equipment level, equipment class, and data source. For a complete reference of all objects and properties that you can add through the UI, refer to the Master definitions and Fields.","general-procedure#General procedure":" From the UI, select the menu in the top corner. Select Master Data, then the object you want to configure. Create new (or the plus sign). Name the object according to your naming conventions. Fill in the other fields. For details, refer to the Master definitions and fields. You can create many different objects, all with their own parameters and associations. For that reason, a general procedure such as the preceding lacks any interesting detail.\nTo make the action more concrete, the next section provides an example to create plausible group of objects.","prerequisites#Prerequisites":"Ensure that you have the following:\nAccess to the Rhize UI Information about the equipment that you want to model "},"title":"Create objects from the UI"},"/how-to/model/master-definitions/":{"data":{"":"To make a production object visible to the Rhize data hub, you must define it as a data model.\nThese sections document all the objects that you can add through the UI, and the fields and properties that you can associate with them. All these models are based on the ISA-95 standard, mostly from Part 2, which describes the role-based equipment hierarchy.\nℹ️ For an introduction to the language of ISA-95, read How to speak ISA-95 For visual examples of how some of these models relate, look at our page of ISA-95 Diagrams. ","common-models#Common models":"Common models are data objects that can apply to different resources in your manufacturing process\nUnits of Measure A Unit of measure is a defined unit to consistently compare values, duration or quantities.\nYou can create units of measure in the UI and give them the following parameters:\nName Data type Data Sources A data source is a source of real-time data that is collected by the Rhize agent. For example, in a baking process, a data source might be an OPC UA server that sends readings from an oven thermometer.\nThe general fields for a data source are as follows:\nGeneral fields Description Connection string A string to specify information about the data source and the way to connect to it The Data source protocol Either MQTT or OPCUA username If needed, username for Agent authentication password If needed, password for Agent authentication certificate If needed, certificate for Agent authentication Additionally, each data source can have topics that Rhize should be able to subscribe to. Each topic has the following fields:\nTopic field Description Data type The data type Rhize expects to find when it receives data from that topic Deduplication key The field that NATS uses to de-duplicate messages from multiple data sources. Label The name of the topic on the side of the data source Description A freeform text field to add context Some data sources, such as OPC UA, have methods for RPC calls.\nHierarchy Scope The hierarchy scope represents the scope within which data information is exchanged. It provides a flexible way to group entities and data outside of the scope defined by the role-based equipment hierarchy.","global-object-fields#Global object fields":"All objects that you define must have a unique name. Additionally, most objects have the following fields:\nGlobal field Description Version The version of the object (and each version has a state) Description Freeform text to describe what the object does and help colleagues understand its role Version states Each version of an object can have the following states:\nDraft Active For review Deprecated ℹ️ When recording actual execution, what matters is version of the object, not its general definition. Thus, to add a class to an object, you must give that object a version first. ","operation-models#Operation models":"Operation models are data objects that describe manufacturing processes from the perspective of the level-4 (ERP) systems.\nProcess segment A process segment is a step in a manufacturing activity that is visible to a business process, grouping the necessary personnel, material, equipment, and physical assets. In a baking process, an example segment might be mixing.\nYou can associate specifications for:\nEquipment, Material, Personnel, and Physical Assets Along with the global object fields, a process-segment object has the following fields:\nGeneral fields Description Operations type One of: Inventory, maintenance, mixed, production, quality Definition type One of: Instance, Pattern Duration The expected duration Duration unit of measure The time unit of measure Hierarchy scope The hierarchy scope within which data is exchanged for this process segment You can add additional parameters for:\nName Value Unit of measure Operations Definition Operations Definitions describe how resources come together to manufacture product from the perspective of the level-4 (ERP) systems.\nThe operation model carries enough detail to plan the work at resolutions of hours and days. For more granularity, refer to work models.\nAlong with the global object fields, an operations-definition object has the following fields:\nGeneral fields Description Operation type One of: Inventory, maintenance, mixed, production, quality Hierarchy scope The hierarchy scope within which data is exchanged for this operations definition Operations event class An operations event class defines a class of operations events within some hierarchy.\nThe class has the following properties:\nVersion Operations event classes, defining one or more operations event classes that a version inherits properties from Operations event definition An operations event definition defines the properties that pertain to an event from the perspective of the level-4 (ERP) systems. Along with the event itself, it may have associated resources, such as material lots or physical assets received.\nAlong with the global object fields, an operations-event-definition object has the following fields:\nField Description Category A string that can be used to group the event Source The activity, function, task or phase that generated the event. Event type One of: Alert, an potentially significant event, such as a workflow trigger, that does not require notification Alarm, an event that requires notification Event, any other event that is not at the level of alarm or alert Operations event classes One or more operations event classes that a version definition inherits properties from. ","resource-models#Resource models":"Resource models are data objects that have a specific role in your role-based equipment hierarchy.\nEquipment class An equipment class is a grouping of equipment for a definite purpose. For example, in a baking process, an equipment class might be the category of all ovens, with properties such as maximum temperature and number of shelves.\nAlong with the Global properties, an equipment class can include an indefinite number of properties with the following fields:\nProperties Description Name Name of the property Description A freeform text to describe the property Unit of measure The property unit of measure Equipment A piece of equipment is a tool with a defined role in a process segment. For example, in a baking process, equipment might be a specific brownie oven.\nEquipment also might be part of hierarchy of levels, starting with Enterprise and ending with granular levels such as WorkUnit.\nAlong with the following fields, you can also connect an equipment item to a data source, add additional properties, and toggle it to be active or inactive.\nAlong with the global object fields, an equipment object has the following fields:\nGeneral equipment fields Description Equipment class The class of equipment that it belongs to. Equipment level Associated level for the equipment. One of: Enterprise, Site, Area, ProcessCell, Unit, ProductionLine, WorkCell, ProductionUnit, Warehouse, StorageZone, StorageUnit, WorkCenter, WorkUnit, EquipmentModule, ControlModule, Other Material Class A material class is a group of material with a shared purpose in the manufacturing process.\nAlong with the global object fields, a material-class object has the following fields:\nGeneral fields Description Assembly type Can be one of: Logical: the components of the material are not necessarily physically connectedPhysical: the components of the material are physically connected or in the same location Relationship Can be one of: Permanent, if a material that can’t be split from the production process Transient, for temporary material in assembly, such as a pallet Hierarchy scope The hierarchy scope that material class belongs to Includes properties of One or more material classes that a version inherits properties from Is assembled from Material classes that make this material Material classes may have an indefinite number of properties with parameters for the following fields:\nValue Unit of measure Material definition Materials are everything required to produce a finished good. They include raw materials, intermediate materials, and collections of parts.\nAlong with the global object fields, a material object has the following fields:\nGeneral Description Material class One or more material classes that a version inherits properties from Materials may have an indefinite number of properties with parameters for the following fields:\nValue Unit of measure Personnel Class A personnel class is a grouping of persons whose work shares a definite purpose in the manufacturing process. In a baking process, an example of a personnel class may be oven_operators.\nAlong with the global object fields, a personnel-class object has the following fields:\nGeneral fields Description Hierarchy scope The hierarchy scope within which this personnel exchanges data Person A person is a unique member of personnel class.\nAlong with the global object fields, a person object has the following fields:\nGeneral fields Description Name The name of the person Hierarchy scope The hierarchy scope that the person belongs to Inherit personnel classes One or more personnel classes that a version inherits properties from Operational location The associated Operational location Physical asset class A physical asset class is a class of physical assets.\nThe physical asset class has properties for:\nClassType Value Unit of measure Physical Asset A physical asset is portable or swappable equipment. In a baking process, a physical asset might be the laser jet printer which adds labels to the boxes (and could be used in many segments across the plant).\nIn many cases, your process may need to model only equipment, not physical assets.\nOperational Location Class An operational location class is a grouping of operational locations for a defined purpose. For example, in a baking process, an operational location class may be Kitchens\nAlong with the global object fields, an operational-location-class object has the following fields:\nGeneral fields Description Hierarchy scope The hierarchy scope within which this location exchanges data Inherit Operational location class One or more Operational location classes that a version inherits properties from Operational Location An operational location is where resources are expected to be located in a plant. For example, in a baking process, an operational location class may be northwing_kitchen_A\nAlong with the global object fields, an operational-location object has the following fields:\nGeneral fields Description Hierarchy scope The hierarchy scope within which this location exchanges data Operational location classes Zero or more operational location classes that a version inherits properties from Map view Where the location is on the map ","work-models#Work models":"Work models describe how the resources come together to manufacture product from the perspective of level-3 (MES) systems. As with Operations models, the steps in the process are called segments.\nThe work model carries enough detail to plan the work at resolutions of hours and minutes. For less granularity, refer to operations definitions.\nWork Master A work master is a template for a job order from the perspective of the level-3 (MES/MOM) systems. In a baking process, an example work master might be Brownie Recipe.\nAlong with the global object fields, a work-master object has the following fields:\nGeneral fields Description Workflow type One of: Inventory, maintenance, mixed, production, quality Workflow specification An associated BPMN workflow Work calendar Work calendars describe a set of rules for specific calendar entries, including duration, start and end dates, and times.\nThe general fields for a calendar duration are as follows:\nGeneral fields Description Description A description of the work calendar Hierarchy scope The hierarchy scope that defines scope of data exchanged for the calendar entries The work calendar can have properties with a description, value, and unit of measure.\nThe work calendar object can have one or more entries, which define the start, end, duration, and recurrence of a rule. The duration and recurrence attributes for a time-based rule are represented by the ISO 8601 standard. The attributes for an entry are as follows:\nEntry fields Description Description Freeform text that describes the entry Type One of: PlannedBusyTime, PlannedDownTime, and PlannedShutdown Start date and time When the entry starts End date and time When the entry finishes Recurrence time interval How often the entry repeats according to the repeating interval representation of IS0 8601 Duration rule How long the work calendar lasts, according to the Duration representation of IS0 8601. "},"title":"Master definitions and fields"},"/how-to/publish-subscribe/":{"data":{"":"For Rhize to listen to and handle manufacturing events, you need to connect a data source.\nTutorial: Trigger a workflow from a ruleFollow this tutorial to create a rule to run a workflow every time a data source changes. Connect data sourceConfigure a data source to publish topics for the Rhize platform. Track changes (CDC)Streaming data in and out of RHIZE "},"title":"Connect event data"},"/how-to/publish-subscribe/connect-datasource/":{"data":{"":"For Rhize to listen to and handle manufacturing events, you need to connect a data source.","next-steps#Next steps":"Now that you have the data source sending data you can:\nWrite a rule to Turn data into events that trigger workflows. Create a BPMN workflow to run on this trigger. You can also write a workflow that subscribes to data source directly through a message start event. ","prerequisites#Prerequisites":"To add a data source, you need the following:\nAccess to an MQTT or OPC UA server Credentials for this server, if necessary The URL and connection string for this server (Rhize will point to this) ","steps-to-connect#Steps to connect":"The process has two sides:\nSending topics from your MQTT, OPCUA, or NATS server to Rhize.\nIn Rhize, defining the data source and its associated objects.\nTo do this, you can create entities in the Rhize UI or through its GraphQL API.\nModel the data source in the Rhize UI Enter the Rhize UI and go to Master Data \u003e Data sources. Add the connection string, topics, and other necessary parameters. For details of what these fields mean, review the Data source object reference. Create and then change version state to Active. Now add the data source to its equipment (or, if it doesn’t exist model new equipment):\nSelect the equipment, then Data sources. If the equipment has properties bound to this data source, create the properties, then configure them as BOUND to the data source. Once active, Rhize reaches out to this data source and synchronizes the equipment properties to the bound topics."},"title":"Connect data source"},"/how-to/publish-subscribe/create-equipment-class-rule/":{"data":{"":"An equipment class rule triggers a BPMN workflow whenever a data source publishes a value that meets a specified threshold.\nImagine a scenario when an oven must be preheated every time a new order number is published to an MQTT edge device. You could automate this workflow with a rule that listens to messages published and evaluates a condition. If the condition evaluates to true, the rule triggers a BPMN workflow to preheat the oven.\n--- title: Rules trigger workflows from data-source changes --- flowchart LR A(Property changed?) --\u003e|yes| B{\"rule evaluates to true?\"} B --\u003e|no| C(do nothing) B --\u003e|\"yes (optional: pass variables)\"| D(Run BPMN workflow) The broad procedure to create a rule is as follows:\nIn the Rhize UI or through GraphQL, create models for the data source and its associated unit of measure, equipment, and equipment class. In the Rhize UI, write a BPMN workflow that is triggered when this data source changes and executes some business logic. In the equipment class, create a rule that triggers the workflow. The following sections describe how to do these steps in more detail.\nℹ️ This tutorial assumes a data source that exchanges messages over the MQTT protocol. ","create-a-rule#Create a rule":"Add a rule to an existing equipment class From the Rules tab of an equipment class version, create a new rule. Enter Run BPMN on Order Number for the name and confirm. Select rules_example_bpmn for the workflow specification. Add orderNumber as a trigger property. Add a trigger expression that evaluates to true or false. ℹ️ The rule runs the preceding workflow only if the expression evaluates to true. It’s common to compare the new value with the previous. In this case, we can compare the new order number to the previous by adding OrderNumber.current.value != OrderNumber.previous.value. Note that the root of the object path must match the ID of the equipment class property we set up earlier and all evaluations are case-sensitive.\nThe entire information that becomes available to the rule engine looks like this:\nJSON inputExpressionOutput { orderNumber: { current: { bindingType: \"BOUND\", description: \"bound prop\", equipmentClassProperty: { id: \"EQCLASS1.10.orderNumber\", iid: \"0x2a78\", label: \"orderNumber\" }, equipmentVersion: { equipment: { id: \"EQ1\", iid: \"0x1b\", label: \"EQ1\" }, id: \"EQ1\", iid: \"0x22\", version: \"2\" }, id: \"EQCLASS1.10.orderNumber\", label: \"orderNumber\", messageKey: \"ns=3;i=1003.1695170450000000000\", propertyType: \"DefaultType\", serverPicoseconds: 0, serverTimestamp: \"2023-09-20T00:40:50.028Z\", sourcePicoseconds: 0, sourceTimestamp: \"2023-09-20T00:40:50Z\", value: \"Order2\", valueUnitOfMeasure: { dataType: \"FLOAT\", id: \"FLOAT\", iid: \"0x28\" } }, previous: { bindingType: \"BOUND\", description: \"bound prop\", equipmentClassProperty: { id: \"EQCLASS1.10.orderNumber\", iid: \"0x2a78\", label: \"orderNumber\" }, equipmentVersion: { equipment: { id: \"EQ1\", iid: \"0x1b\", label: \"EQ1\" }, id: \"EQ1\", iid: \"0x22\", version: \"2\" }, id: \"EQCLASS1.10.orderNumber\", label: \"orderNumber\", messageKey: \"ns=3;i=1003.1695170440000000000\", propertyType: \"DefaultType\", serverPicoseconds: 0, serverTimestamp: \"2023-09-20T00:40:40.003Z\", sourcePicoseconds: 0, sourceTimestamp: \"2023-09-20T00:40:40Z\", value: \"Order1\", valueUnitOfMeasure: { dataType: \"FLOAT\", id: \"FLOAT\", iid: \"0x28\" } } } } `OrderNumber.current.value != OrderNumber.previous.value` True \u003cbutton class=“hextra-code-copy-btn hx-group/copybtn hx-transition-all active:hx-opacity-50 hx-bg-primary-700/5 hx-border hx-border-black/5 hx-text-gray-600 hover:hx-text-gray-900 hx-rounded-md hx-p-1.5 dark:hx-bg-primary-300/10 dark:hx-border-white/10 dark:hx-text-gray-400 dark:hover:hx-text-gray-50” title=“Copy code”\n\u003cdiv class=\"copy-icon group-[.copied]/copybtn:hx-hidden hx-pointer-events-none hx-h-4 hx-w-4\"\u003e\u003c/div\u003e \u003cdiv class=\"success-icon hx-hidden group-[.copied]/copybtn:hx-block hx-pointer-events-none hx-h-4 hx-w-4\"\u003e\u003c/div\u003e The expression evaluates to false, because the current and previous values differ.\nOptionally, pass information to the BPMN by adding a payload message. The message is an object with multiple keys.\nEnter orderNumber for the field name. Enter orderNumber.current.value for the JSON expression. Confirm by clicking the green tick icon. Create. From the General tab, change the equipment class version state to active. Creating an equipment class rule in the UI. Associate an equipment with a bound property The final steps to setting up a rule are to:\nCreate a new equipment version. Link it to a data source. Set up bound properties. Create an equipment and version From the Main Menu, navigate to Master Data \u003e Equipment. Select a piece of equipment. If none, create one called Line 1. From the General tab, Create. Link the version to the equipment class you created earlier (Pizza Line). Save the version to create it. A new equipment class and version created in the UI. Link a data source From the Data Sources tab, link the equipment version to the data source you created in the previous section. An equipment linked to a data source in the UI. Set up the bound property From the Properties tab, find a property that you want this equipment to inherit and select the binding icon. If you chose the property orderNumber, add the topic Oven/OrderNumber you added previously. An equipment property bound to a data source topic in the UI. ","prerequisites#Prerequisites":"Before you start, ensure you have the following:\nAccess your Rhize customer environment The Agent configured to listen for your data-source ID ","set-up-configure-equipment-and-workflows#Set up: configure equipment and workflows":"The setup involves modeling the objects associated with the rule.\nData source Data source topic Unit of measure BPMN Equipment class with bound properties Once you have these, you can create a rule and associate it with an actual equipment item.\nCreate a data source From the main menu, navigate to Master Data \u003e Data Sources. Create a new data source. The label (ID) must match the one specified in the configuration file for the libre-agent microservice. From the General tab, add a draft data source version. Select MQTT as the data-source protocol. Optionally, enter a connection string, such as mqtt://\u003cREMOTE_HOST\u003e:1883, that matches the one specified in the configuration file for the libre-agent microservice. Save the data source version to create it. A new data source and version created in the UI. Create a data source topic Navigate to the Topics tab. Add a new property (that is, a topic). Select STRING for the property data type (this assumes an order number is a string such as Order1). Select your preferred deduplication key. The default option, Message Value, is most appropriate for this scenario. For label, enter the exact topic name as it appears in the data source. Use a slash to access nested topics. For this example, all new order numbers are published to Oven/OrderNumber. Confirm by clicking the green tick icon. Navigate to the General tab and change the version state to active. A new data source topic created in the UI. Create a unit of measure From the Main Menu, navigate to Master Data \u003e Units of Measure. Add a new unit of measure. Enter Order Number for the unit name. Select STRING for the data type. A new unit of measure created in the UI. Creating a BPMN workflow A rule must trigger a BPMN workflow. Before setting up a rule, create its workflow. For this example, this 3-node BPMN is enough:\nNavigate to Workflows \u003e Process List. Import the BPMN. Save it. Set the version as active. A BPMN created in the UI. The BPMN has a Libre Jsonata Transform task that contains an expression \"Preheating oven for order number \" \u0026 $.orderNumber\" . The rule engine triggers this BPMN with a payload that includes the order number value, as follows:\n{ \"orderNumber\": \"Order1\" } Create an equipment class with bound properties Equipment class and version Navigate to Master Data \u003e Equipment Class. Create a new equipment class from the sidebar. The label might be Pizza Line, for example. From the General tab, Create a new Draft version. A new equipment class and version created in the UI. Equipment class property From the properties tab, create a new property. For type, select BOUND. For name, enter orderNumber. For UoM, select the unit of measure created earlier (Order Number). Confirm by clicking the green tick icon. A new equipment class property created in the UI. ","test-the-binding-and-the-rule#Test the binding and the rule":"Send a message to test that the value of the property orderNumber of the equipment Line 1 is bound to the topic Oven/OrderNumber.\nTest using an MQTT client For example, using MQTT Explorer:\nOpen MQTT Explorer and connect to the broker. The microservice Libre Agent (libre-agent) should immediately publish a message to indicate the data source topic Oven/OrderNumber has been set up successfully.\nThe Libre Agent has connected to the data source. Publish the string Order1 to the topic Oven/OrderNumber. A new order number was published to the data source. If the message has been received, a new topic, Oven, appears with its subtopic OrderNumber.\nIf there is an equipment property bound to this topic, a topic called MQTT/\u003cDATA_SOURCE_NAME\u003e/ValueChanged also appears. In addition, the published value should show in the column Expression of the equipment property orderNumber.\nThe bound property assumes the last value published to the data source. ℹ️ If this is the first message published to the topic, the rule will not be triggered because Rhize has no previous value to compare it the message value to. However, if you publish another order number, a new topic called Core will show up containing a subtopic called RuleTriggered to indicate that the rule has indeed been triggered. The rule engine has published a message to indicate that the equipment class rule has indeed been triggered. Confirm in execution in Tempo To confirm the intended BPMN was executed, navigate to Grafana (Tempo) and look for a trace containing the expected BPMN ID.\nGrafana shows a recent trace with the id of the target BPMN. ","video-example#Video example":" 🎥 Trigger BPMN. This video provides an example of creating a rule based on values for an OPC UA server in a baking process. "},"title":"Tutorial: Trigger a workflow from a rule"},"/how-to/publish-subscribe/track-changes/":{"data":{"":"You can use change data capture (CDC) to track data changes over time, including a mutation or drop in your database. Rhize’s CDC implementation can use Kafka, NATS, or a local file as a sink to store CDC updates streamed by Rhize’s Alpha leader nodes.\nWhen CDC is enabled, Rhize streams events for:\nAll set and delete mutations, except those that affect password fields Drop events. Live Loader events are recorded by CDC, but Bulk Loader events aren’t.\nCDC events are based on changes to Raft logs. So, if the sink is unreachable by the Alpha leader node, then Raft logs expand as events are collected on that node until the sink is available again.\nYou should enable CDC on all Rhize Alpha nodes to avoid interruptions in the stream of CDC events.","cdc-command-reference#CDC command reference":"The --cdc option includes several sub-options that you can use to configure CDC when running the dgraph alpha command:\nSub-option Example dgraph alpha command option Notes ca-cert --cdc \"ca-cert=/cert-dir/ca.crt\" Path and filename of the CA root certificate used for TLS encryption, required if Kafka endpoint requires TLS client-cert --cdc \"client-cert=/c-certs/client.crt\" Path and filename of the client certificate used for TLS encryption client-key --cdc \"client-cert=/c-certs/client.key\" Path and filename of the client certificate private key file --cdc \"file=/sink-dir/cdc-file\" Path and filename of a local file sink (alternative to Kafka sink) nats --cdc \"nats=nats://user:password@localhost:4222\" URL connection string to nats sink (alternative to Kafka sink) kafka --cdc \"kafka=kafka-hostname; sasl-user=tstark; sasl-password=m3Ta11ic\" Hostname(s) of the Kafka hosts. May require authentication using the sasl-user and sasl-password sub-options sasl-user --cdc \"kafka=kafka-hostname; sasl-user=tstark; sasl-password=m3Ta11ic\" SASL username for Kafka. Requires the kafka and sasl-password sub-options sasl-password --cdc \"kafka=kafka-hostname; sasl-user=tstark; sasl-password=m3Ta11ic\" SASL password for Kafka. Requires the kafka and sasl-username sub-options sasl-mechanism --cdc \"kafka=kafka-hostname; sasl-mechanism=PLAIN\" The SASL mechanism for Kafka (PLAIN, SCRAM-SHA-256 or SCRAM-SHA-512). The default is PLAIN ","cdc-data-format#CDC data format":"CDC events are in JSON format. Most CDC events look like the following example:\n{ \"key\": \"0\", \"value\": {\"meta\":{\"commit_ts\":5},\"type\":\"mutation\",\"event\":{\"operation\":\"set\",\"uid\":2,\"attr\":\"counter.val\",\"value\":1,\"value_type\":\"int\"}}} The Meta.Commit_Ts value (shown above as \"meta\":{\"commit_ts\":5}) will increase with each CDC event, so you can use this value to find duplicate events if those occur due to Raft leadership changes in your Dgraph Alpha group.\nMutation event examples A set mutation event updating counter.val to 10 would look like the following:\n{\"meta\":{\"commit_ts\":29},\"type\":\"mutation\",\"event\":{\"operation\":\"set\",\"uid\":3,\"attr\":\"counter.val\",\"value\":10,\"value_type\":\"int\"}} Similarly, a delete mutation event that removes all values for the Author.name field for a specified node would look like the following:\n{\"meta\":{\"commit_ts\":44},\"type\":\"mutation\",\"event\":{\"operation\":\"del\",\"uid\":7,\"attr\":\"Author.name\",\"value\":\"_STAR_ALL\",\"value_type\":\"default\"}} Drop event examples CDC drop events look like the following example event for “drop all”:\n{\"meta\":{\"commit_ts\":13},\"type\":\"drop\",\"event\":{\"operation\":\"all\"}} The operation field specifies which drop operation (attribute, type, specified data, or all data) is tracked by the CDC event.","enable-cdc-with-file-sink#Enable CDC with file sink":"To enable CDC and sink results to a local unencrypted file, start Dgraph Alpha with the --cdc command and the sub-option shown below, as follows:\ndgraph alpha --cdc \"file=local-file-path\" ","enable-cdc-with-kafka-sink#Enable CDC with Kafka sink":"Kafka records CDC events under the libre-cdc topic. The topic must be created before events are sent to the broker. To enable CDC and sink events to Kafka, start Dgraph Alpha with the --cdc command and the sub-options shown below, as follows:\ndgraph alpha --cdc \"kafka=kafka-hostname:port; sasl-user=tstark; sasl-password=m3Ta11ic\" If you use Kafka on the localhost without SASL authentication, you can just specify the hostname and port used by Kafka, as follows:\ndgraph alpha --cdc \"localhost:9092\" If the Kafka cluster to which you are connecting requires TLS, the ca-cert option is required. Note that this certificate can be self-signed.","enable-cdc-with-nats-jetstream-kv-store-sink#Enable CDC with NATS JetStream KV store sink":"To enable CDC and sink results to a NATS JetStream KV store, start Dgraph Alpha with the --cdc command and the sub-option shown below, as follows:\ndgraph alpha --cdc \"nats=nats://system:system@localhost:4222\" ","known-limitations#Known limitations":"CDC has the following known limitations:\nCDC events do not track old values that are updated or removed by mutation or drop operations; only new values are tracked CDC does not currently track schema updates You can only configure or enable CDC when starting Alpha nodes using the dgraph alpha command If a node crashes or the leadership of a Raft group changes, CDC might have duplicate events, but no data loss "},"title":"Track changes (CDC)"},"/how-to/work-calendars/":{"data":{"":"Work calendars represent planned periods of time in your operation, including shifts, planned shutdowns, or recurring stops for maintenance. The Rhize API represents calendars through the workCalendar entity, which has close associations with the equipment and hierarchy scope models.\nRhize also has a calendar service that periodically queries the Rhize DB for workCalendarDefinitions. If it finds active definitions for that period, the service creates the work calendar entries and persists the data to a time-series database\nℹ️ Rhize’s implementation of work calendars was inspired by ISO/TR 22400-10, a standard on KPIs in operations management. "},"title":"Use work calendars"},"/how-to/work-calendars/about-calendars-and-overrides/":{"data":{"":"Work calendars represent planned periods of time in your operation, including shifts, planned shutdowns, or recurring stops for maintenance. The Rhize API represents calendars through a workCalendar entity and this calendar’s associated definitions and entries. They provide helpful abstractions for activities such as scheduling and performance analysis.\nRhize has an optional calendar service that periodically queries the Rhize DB for workCalendarDefinitions. If it finds active definitions and equipment for that period, the service creates work calendar entries and persists the data to a time-series database. This topic explains how that calendar service works.\nℹ️ Rhize’s implementation of work calendars was inspired by ISO/TR 22400-10, a standard on KPIs in operations management. ","calendar-precedence#Calendar precedence":"You can use calendar entries to set different calendar states at different levels of a hierarchy. It is also possible for multiple shutdown periods to overlap in the same scope. If an equipment belongs to multiple scopes, the service needs a way to handle this ambiguity.\nTo prevent conflicts in these situations, Rhize has logic to determine calendar precedence.\nThe lowest hierarchy scope has precedence The lowest level of the hierarchy scope defines the calendar state for the equipment in this hierarchy. For example, imagine two scopes:\nScope A corresponds to an equipment line. Scope B, the child of Scope A, corresponds to equipment items in the line. If Scope A has planned downtime and Scope B does not. Then all the equipment in Scope B takes state defined by its associated work calendar entries. As Scope B is at a lower level, it has precedence.\nThe first start time, the last end time It might occur that multiple active work calendars overlap with the same state. For example, consider three scopes at the same hierarchy level.\nScope A has a planned downtime starting at 00:00 and ending at 12:00, Scope A2 has a planned downtime that starts at 01:00 and ends at 13:00. Scope A3 has a planned busy time that starts at 05:00 and ends at 06:00. If an equipment item belonged to all these scopes, Rhize would calculate its planned downtime as being from 00:00 to 13:00. The planned busy time is locked out, since another active entry type has already taken effect. For a technical overview of how this locking and unlocking of states works, read about the Semaphore pattern in computer science.","what-the-service-does#What the service does":" A simplified view of how the calendar service coordinates and exchanges data with the Rhize DB and a time-series DB The calendar service queries all active work calendar definitions at an interval designated in your service configuration. The service then checks for any active workCalendarDefinitionEntry items that start or end within that interval. If any exist, Rhize creates a workCalendarEntry with the start and end time.\nThe service then traverses all the calendar hierarchyScope entities (designated by the prefix WorkCalendar) and their equipmentHierarchy properties. Rhize checks each equipment item for any workCalendarEntries. If the scope includes active equipment, Rhize persists the entry to a time-series database.\nThe relationship between hierarchy scope, equipment, and calendars flowchart TD hs(Hierarchy scope) --\u003e|provides scope for| wc(Work calendar) e(Equipment) --\u003e|physical structure maps to| hs hs --\u003e |calendar structure maps to| e wc --\u003e|create calendar entries for active|e As the behavior in the previous section describes, the work calendar service coordinates data between three entities in your knowledge graph: hierarchy scope, equipment, and work calendar. These entities work together to configure work calendars and automate changes of equipment state.\nEquipment provides the physical hierarchy of the plant’s equipment, at levels that can be as small as a workUnit or as broad as the entire enterprise. Hierarchy scope creates a calendar hierarchy that the equipment hierarchy maps to. Work calendars and their associated definitions and entries have a hierarchy scope property, which Rhize uses to determine what the equipment state is. The Rhize service uses the hierarchy scope to establish calendar precedence. Then, it uses scope’s associated calendar states to automatically set the state of the equipment for each hierarchy. So, when you create a calendar, ensure that you configure these three objects.\nCalendar states The Rhize database and service has three calendar types:\nPlannedDowntime PlannedShutdown None, for events that are not considered in OEE calculations. "},"title":"About calendars and overrides"},"/how-to/work-calendars/create-work-calendar/":{"data":{"":"This guide shows you how to create a work calendar using the Rhize GraphQL API. As a calendar has associations with multiple other entities, the process involves a series of mutations to create associated data.\nTo learn how work calendars work, read About work calendars.","prerequisites#Prerequisites":"To use the work calendar service, ensure you have the following:\nThe calendar service installed A plan for how to organize and name your calendars according to equipment. ","procedure#Procedure":"In short, the procedure works as follows:\nAdd equipment that follows some hierarchy. Add hierarchy scopes for the calendar rules. These scopes should map to the equipment hierarchy. Add work calendar definitions. You can add these objects in the UI or through the GraphQL API. The following sections provide the requirements for each of these entities and examples of a mutation to create them.\nThe calendar service uses the relationships between equipment, hierarchy scope, and work calendars. Add equipment The first step is to add an equipment hierarchy.\nRequirements.\nFor an equipment calendar state to be recorded, it must have an active version. Example\nThis mutation adds multiple items of equipment in a batch. Note that some items, such as Equipment A, Equipment B, and Equipment C, have links to child equipment, as expressed in the isMadeUpOf property. These relationships express the equipment hierarchy.\nmutation addEquipment mutation AddEquipment($input: [AddEquipmentInput!]!, $upsert: Boolean) { addEquipment(input: $input, upsert: $upsert) { numUids } } { \"input\": [ { \"id\": \"Equipment A\", \"nextVersion\": \"2\", \"label\": \"Equipment A\", \"activeVersion\": { \"id\": \"Equipment A\", \"version\": \"1\", \"versionStatus\": \"ACTIVE\", \"equipment\": { \"id\": \"Equipment A\" } }, \"isMadeUpOf\": [ {\"id\": \"Equipment B\"}, {\"id\": \"Equipment D\"} ] }, { \"id\": \"Equipment B\", \"nextVersion\": \"2\", \"label\": \"Equipment B\", \"activeVersion\": { \"id\": \"Equipment B\", \"version\": \"1\", \"versionStatus\": \"ACTIVE\", \"equipment\": { \"id\": \"Equipment B\" } }, \"isMadeUpOf\": [ {\"id\": \"Equipment C\"} ] }, { \"id\": \"Equipment C\", \"nextVersion\": \"2\", \"label\": \"Equipment C\", \"activeVersion\": { \"id\": \"Equipment C\", \"version\": \"1\", \"versionStatus\": \"ACTIVE\", \"equipment\": { \"id\": \"Equipment C\" } }, \"isMadeUpOf\": [ {\"id\": \"Equipment Ca\"}, {\"id\": \"Equipment Cb\"} ] }, { \"id\": \"Equipment Ca\", \"nextVersion\": \"2\", \"label\": \"Equipment Ca\", \"activeVersion\": { \"id\": \"Equipment Ca\", \"version\": \"1\", \"versionStatus\": \"ACTIVE\", \"equipment\": { \"id\": \"Equipment Ca\" } } }, { \"id\": \"Equipment Cb\", \"nextVersion\": \"2\", \"label\": \"Equipment Cb\", \"activeVersion\": { \"id\": \"Equipment Cb\", \"version\": \"1\", \"versionStatus\": \"ACTIVE\", \"equipment\": { \"id\": \"Equipment Cb\" } } }, { \"id\": \"Equipment D\", \"nextVersion\": \"2\", \"label\": \"Equipment D\", \"activeVersion\": { \"id\": \"Equipment D\", \"version\": \"1\", \"versionStatus\": \"ACTIVE\", \"equipment\": { \"id\": \"Equipment D\" } } } ], \"upsert\": true } Add Hierarchy scope The hierarchy scope establishes the calendar hierarchy that the Rhize calendar service uses to establish calendar precedence.\nRequirements. A calendar hierarchy scope must have the following properties.\nA time zone. An ID that starts with the prefix WorkCalendar_ To associate equipment with the hierarchy scope, add the equipment items to the equipmentHierarchy. To create levels of calendar scope, add children, each of which can link to equipment.\nExample\nThis example adds a work-calendar hierarchy, WorkCalendar_PSDT, with associated children scopes. The scope and its children link to equipment created in the previous step through equipmentHierarchy.\nmutation addHierarchyScope mutation AddHierarchyScope($input: [AddHierarchyScopeInput!]!) { addHierarchyScope(input: $input) { numUids } } { \"input\": [ { \"effectiveStart\": \"2024-05-29T00:00:00Z\", \"id\": \"WorkCalendar_PSDT.Scope A\", \"label\": \"WorkCalendar_PSDT.Scope A\", \"timeZoneName\": \"Europe/London\", \"equipmentHierarchy\": { \"id\": \"Equipment A\", \"version\": \"1\" }, \"children\": [ { \"effectiveStart\": \"2024-05-29T00:00:00Z\", \"id\": \"WorkCalendar_PSDT.Scope A.Scope B\", \"label\": \"Scope B\", \"timeZoneName\": \"Europe/London\", \"equipmentHierarchy\": { \"id\": \"Equipment B\", \"version\": \"1\" }, \"children\": [ { \"effectiveStart\": \"2024-05-29T00:00:00Z\", \"id\": \"WorkCalendar_PSDT.Scope A.Scope B.Scope C\", \"label\": \"Scope C\", \"timeZoneName\": \"Europe/London\", \"equipmentHierarchy\": { \"id\": \"Equipment C\", \"version\": \"1\" }, } ] }, { \"effectiveStart\": \"2024-05-29T00:00:00Z\", \"id\": \"WorkCalendar_PSDT.Scope A.Scope D\", \"label\": \"Scope D\", \"timeZoneName\": \"Europe/London\", \"equipmentHierarchy\": { \"id\": \"Equipment D\", \"version\": \"1\" }, } ] }, { \"effectiveStart\": \"2024-05-29T00:00:00Z\", \"id\": \"WorkCalendar_PDOT.Scope A\", \"label\": \"WorkCalendar_PDOT.Scope A\", \"timeZoneName\": \"Europe/London\", \"equipmentHierarchy\": { \"id\": \"Equipment A\", \"version\": \"1\" }, \"children\": [ { \"effectiveStart\": \"2024-05-29T00:00:00Z\", \"id\": \"WorkCalendar_PDOT.Scope A.Scope B\", \"label\": \"Scope B\", \"timeZoneName\": \"Europe/London\", \"equipmentHierarchy\": { \"id\": \"Equipment B\", \"version\": \"1\" }, \"children\": [ { \"effectiveStart\": \"2024-05-29T00:00:00Z\", \"id\": \"WorkCalendar_PDOT.Scope A.Scope B.Scope C\", \"label\": \"Scope C\", \"timeZoneName\": \"Europe/London\", \"equipmentHierarchy\": { \"id\": \"Equipment C\", \"version\": \"1\" }, } ] }, { \"effectiveStart\": \"2024-05-29T00:00:00Z\", \"id\": \"WorkCalendar_PDOT.Scope A.Scope D\", \"label\": \"Scope D\", \"timeZoneName\": \"Europe/London\", \"equipmentHierarchy\": { \"id\": \"Equipment D\", \"version\": \"1\" }, } ] } ] } Create work calendar definition After you have created equipment and hierarchy scopes, create a workCalendarDefinition. The calendar service reads the entries to create records of machine states.\nRequirements: The work calendar definition must have the following:\nAn associated work calendar A label. Note that Rhize uses the to label to configure overrides. At least one entry that has at least these properties: Start date type (one of: PlannedDowntime, PlannedShutdown, and PlannedBusyTime). A recurrence time interval in the representation defined by the ISO 8601 standard You can optionally add properties to each entry to add additional context and information.\nNaming conventions\nWhen you name the ID, the recommended convention is {CalendarDefinition.Label}.{HierarchyScope.id}. This convention helps readers quickly understand which scopes and equipment its entries affect. Example\nThis addWorkCalendarDefinition mutation adds entries for planned downtime and shutdown time. Note that the calendar definitions link to a hierarchy scope defined in the previous step.\nmutation addWorkCalendarDefinition mutation AddWorkCalendarDefinition($input: [AddWorkCalendarDefinitionInput!]!, $upsert: Boolean) { addWorkCalendarDefinition(input: $input, upsert: $upsert) { numUids } } { \"input\": [ { \"id\": \"PDOT C.Scope C\", \"label\": \"PDOT C\", \"workCalendars\": [ { \"id\": \"PDOT C.Scope C\", \"label\": \"PDOT C.Scope C\" } ], \"hierarchyScope\": { \"id\": \"WorkCalendar_PDOT.Scope A.Scope B.Scope C\" }, \"entries\": [ { \"id\": \"PDOT C.Scope C.1\", \"label\": \"PDOT C.Scope C.1\", \"durationRule\": \"PT15M\", \"startRule\": \"2024-05-29T09:30:00Z\", \"recurrentTimeIntervalRule\": \"R/P1D\", \"properties\": [ { \"id\": \"PDOT C.Scope C.1.PropA\", \"label\": \"Prop A\", \"value\": \"1\" }, { \"id\": \"PDOT C.Scope C.1.PropB\", \"label\": \"Prop B\", \"value\": \"2\" } ], \"entryType\": \"PlannedDowntime\" }, { \"id\": \"PDOT C.Scope C.2\", \"label\": \"PDOT C.Scope C.2\", \"durationRule\": \"PT1H5M\", \"startRule\": \"2024-05-29T08:45:00Z\", \"recurrentTimeIntervalRule\": \"R/P1D\", \"properties\": [ { \"id\": \"PDOT C.Scope C.2.PropA\", \"label\": \"Prop A\", \"value\": \"3\" }, { \"id\": \"PDOT C.Scope C.2.PropB\", \"label\": \"Prop B\", \"value\": \"4\" } ], \"entryType\": \"PlannedDowntime\" } ] }, { \"id\": \"PSDT D.Scope D\", \"label\": \"PSDT D\", \"workCalendars\": [ { \"id\": \"PSDT D.Scope D\", \"label\": \"PSDT D.Scope D\" } ], \"hierarchyScope\": { \"id\": \"WorkCalendar_PSDT.Scope A.Scope D\" }, \"entries\": [ { \"id\": \"PSDT D.Scope D.1\", \"label\": \"PSDT D.Scope D.1\", \"durationRule\": \"PT1H\", \"startRule\": \"2024-05-29T13:00:00Z\", \"recurrentTimeIntervalRule\": \"R/P1D\", \"properties\": [ { \"id\": \"PSDT D.Scope D.1.PropA\", \"label\": \"Prop A\", \"value\": \"1\" }, { \"id\": \"PSDT D.Scope D.1.PropB\", \"label\": \"Prop B\", \"value\": \"2\" } ], \"entryType\": \"PlannedShutdown\" }, { \"id\": \"PSDT D.Scope D.2\", \"label\": \"PSDT D.Scope D.2\", \"durationRule\": \"PT2H\", \"startRule\": \"2024-05-29T12:00:00Z\", \"recurrentTimeIntervalRule\": \"R/P1D\", \"properties\": [ { \"id\": \"PSDT D.Scope D.2.PropA\", \"label\": \"Prop A\", \"value\": \"3\" }, { \"id\": \"PSDT D.Scope D.2.PropB\", \"label\": \"Prop B\", \"value\": \"4\" } ], \"entryType\": \"PlannedShutdown\" } ] }, { \"id\": \"PDOT D.Scope D\", \"label\": \"PDOT D\", \"workCalendars\": [ { \"id\": \"PDOT D.Scope D\", \"label\": \"PDOT D.Scope D\" } ], \"hierarchyScope\": { \"id\": \"WorkCalendar_PDOT.Scope A.Scope D\" }, \"entries\": [ { \"id\": \"PDOT D.Scope D.1\", \"label\": \"PDOT D.Scope D.1\", \"durationRule\": \"PT3H\", \"startRule\": \"2024-05-29T18:00:00Z\", \"recurrentTimeIntervalRule\": \"R/P1D\", \"properties\": [ { \"id\": \"PDOT D.Scope D.1.PropA\", \"label\": \"Prop A\", \"value\": \"1\" }, { \"id\": \"PDOT D.Scope D.1.PropB\", \"label\": \"Prop B\", \"value\": \"2\" } ], \"entryType\": \"PlannedDowntime\" }, { \"id\": \"PDOT D.Scope D.2\", \"label\": \"PDOT D.Scope D.2\", \"durationRule\": \"PT2H\", \"startRule\": \"2024-05-29T21:00:00Z\", \"recurrentTimeIntervalRule\": \"R/P1D\", \"properties\": [ { \"id\": \"PDOT D.Scope D.2.PropA\", \"label\": \"Prop A\", \"value\": \"3\" }, { \"id\": \"PDOT D.Scope D.2.PropB\", \"label\": \"Prop B\", \"value\": \"4\" } ], \"entryType\": \"PlannedDowntime\" } ] } ], \"upsert\": true } "},"title":"Create work calendars"},"/reference/":{"data":{"":"A collection of pages to look up values for schemas, definitions, and anything else related to using Rhize.\nService configurationA collection of pages to look up configuration parameters for various Rhize services. Observability metricsMetrics from the Rhize microservices, collected by Prometheus. Default URLs and local portsA list of the default ports for the various Rhize services GraphQL types and filtersA reference of the data types in the Rhize API and of the filters available for each type. GlossaryA list of terms relevant to Rhize, or that are frequently used in manufacturing contexts. "},"title":"Reference"},"/reference/default-ports/":{"data":{"":"After you install Rhize services, they are accessible, by default, on the following ports:\nService Default Port Admin UI localhost:3030 Grafana localhost:3001 Router localhost:4000 Keycloak localhost:8090 baas-alpha command line localhost:8080 ","urls#URLs":"When you create DNS records, Rhize recommends the following URLs:\nService Domain Admin UI rhize-ui.\u003cYOUR_DOMAIN\u003e Keycloak rhize-auth.\u003cYOUR_DOMAIN\u003e GraphQL rhize-api.\u003cYOUR_DOMAIN\u003e NATS rhize-mqtt.\u003cYOUR_DOMAIN\u003e Grafana rhize-grafana.\u003cYOUR_DOMAIN\u003e "},"title":"Default URLs and local ports"},"/reference/glossary/":{"data":{"":"The manufacturing industry has many specialized terms—and many abbreviations. This glossary is a reference of how Rhize defines terms used in this documentation.\nAgent The Rhize agent bridges your plant processes with the Rhize data hub. It collects data that is emitted during the manufacturing process and publishes it to the NATS message broker.Andon A system that alerts manufacturing teams about problems in processes or quality.Audit The Libre Audit log server is a service that provides a tamper-proof and immutable audit trail of all events that occur in the Rhize system.Business Process Model and Notation (BPMN) A standardized graphical notation used to represent business processes in an intuitive, visual way. Rhize's BPMN UI and engine provides a low-code interface for complex event handlingCatch event In BPMN, a catch event reacts to a trigger (compared to a throw event, which returns some result). Only start and intermediate events can catch.Computerized maintenance management system (CMMS) A Source of information about an organization's maintenance operationsData source A source of real-time data that is collected by the Rhize agent, such as an MQTT or OPC UA deviceDiscrete manufacturing A form a manufacturing where items are made one at a item, as opposed to in batches or continuouslyElectronic batch report (eBR) A detailed report for a batch in a pharmaceutical process, logging items such as the process segments, personnel, equipment, material substance and weight, and so onEnterprise resource planning (ERP) Software to manage business processes. In Rhize, ERP activities usually are named \"operations\", where lower-level activities are prefixed with \"Work\".Equipment A tool used in the production process. Equipment has a role in a process segment to convert material into a final goodEquipment actual Refer to resource actualEquipment class An equipment class is a grouping of equipment with a definite purposeEvent Discrete occurrences or notifications that represent changes or actions within a systemEvent orchestration A pattern in event-driven architecture where a central service, such as NATS, listens to events and decides how to handle them.Event-driven architecture (EDA) A software design pattern that revolves around the concept of events and event-driven messagingEvent-driven data updates When an event leads to an update in the real-time databaseFinished goods The final product. Material becomes finished goods when it requires no more processing or productionGenealogy The ability to trace the raw materials that were used across different levels of a manufacturing processHierarchy scope The scope of the exchanged information, such as a site or area, for which the information is relevant. The scope identifies associated instances of objects such as material, personnel, and equipmentInstance An identifiable member of a manufacturing class, like a material or process segment, that can be tracked and scheduledISO 22400 A standard on KPIs in manufacturing operations management.Laboratory information management system (LIMS) Software that tracks laboratory data, including samples, experiments, and workflowsManufacturing data hub (MDH) A centralized platform or infrastructure that serves as a hub for collecting, integrating, storing, and processing manufacturing data from various sources within an organization.Manufacturing execution system (MES) A system in a manufacturing process that tracks and documents the transformation of raw materials to finished goodsMaterial Everything required to produce a finished good. They include raw materials, intermediate materials, and collections of partsMaterial class A group of material with a shared purpose in the manufacturing processMaterial definition A representation of material with similar charecteristics. All material lots must have one associated material definition.Material lot A uniquely identifiable amount of material, either countable or weighableMaterial sublot Each separately identifiable unit of a material lot.Mutation A GraphQL operation that modifies some data in the database, usually creating, updating, or deleting a resourceNATS Connective infrastructure to address, discover, and exchange messages in a distributed system. Rhize uses NATS as the broker to publish and subscribe to event data, decoupling communication between services and equipment.Operational location Where resources are expected to be located in a plantOperations definition The resources required to perform an operation, including for production, quality, maintenance, and inventory, from the perspective of the level-4 enterprise control systems.Operations event definition A definition of the properties that pertain to an event from the perspective of the level-4 (ERP) systems.Overall Equipment Effectiveness (OEE) A measure of manufacturing productivity. OEE is commonly calculated as the product of three critical factors: availability, performance, and quality.Pattern (process segment) A process segment that creates a pattern or template for other patterns or process segments to followPersonnel class A grouping of persons whose work shares a definite purpose in the manufacturing processPhysical asset A physical piece of equipment with an IDProcess segment A step in a manufacturing activitity that is visible to a business process, grouping the necessary personnel, material, equipment, and physical assetsProcess variable context The context and variables that are passed between tasks in a BPMN workflowResource actual The specific resource used for a specific job. For example, an operations segment may require EquipmentA, but if this equipment is in maintainance at the time of a job, the Equipment actual might be its temporary replacement, EquipmentBRole-based equipment model A definition of equipment based on the level-3 and level-4 functions and activities that it performsRules engine A program that evaluates incoming data from a data source and converts significant changes into events to be processed by a BPMN workflowSerial discrete A type of discrete manufacturing where each item produced has a serial numberSink The destination of a data flow.Throw event In BPMN, a throw event returns some response (compared to a catch event, which reacts to some trigger). Only intermediate and end events can throw.Unit of measure A defined unit to consistently compare values, duration or quantitiesWarehouse management system A system to organize processes and policies of warehouses. It may include processes to manage and track inventory and exchange data.Work calendar A collection of work calendar entriesWork calendar entry Rules that define a work period, including its start date, duration, and interval.Work master A template for a job order, corresponding to an operations and process segment, from the perspective of the level-3 (MES/MOM) systems. "},"title":"Glossary"},"/reference/gql-types/":{"data":{"":"This page provides a reference of the data types enforced by the Rhize database schema, and of the filters that can apply to these types when you query, update, or delete a set of resources. For an extended guide, with examples, read Use query filters.\nℹ️ These filters are based on Rhize’s implementation of the Dgraph @search directives. ","data-types#Data types":"Every object in the Rhize schema has fields that are of one of the basic data types. From the other perspective, these data types define fields that compose manufacturing objects, objects defined precisely by ISA-95 and enforced by Rhize’s database schema.\nBasic types Every manufacturing object in the Rhize database is made of fields that are of one the following basic types. In official GraphQL terminology, these types are called scalar types.\nString: A sequence of characters. For example, machine_2\nInt: An integer number. For example, 2.\nFloat: A number that includes a fraction. For example, 2.25.\nBoolean: A field whose value is either true or false.\nEnum: A field whose values are restricted to a defined set. For example, versionState might be one of ACTIVE, APPROVED, FOR_REVIEW, DRAFT, or DEPRECATED\nid: A string representing a unique object within a defined object type.\niid: The object’s unique address in the database. For example, 0xf9b49.\nDateTime: A timestamp in RFC 3339 format.\nGeo: Geometry types for geo-spatial coordinates\nObject type The preceding basic types form the building blocks for Rhize’s manufacturing object schema, with data models corresponding to ISA-95.\nEach object is made of manufacturing-specific fields of one of the basic types. For example, the materialActual object has basic fields, including:\ndescription, a String. effectiveEnd, a DateTime The materialActual also has complex fields describing associated manufacturing objects. For example, its fields include the array of associated MaterialLot objects, the MaterialDefinition object, and so on. All objects in the database have relationships to other objects.\nℹ️ Metadata fields start with an underscore (_). For example, _createdOn reports the time when the object was created. ","read-more#Read more":" Rhize guide to GraphQL Dgraph @search directive. ","scalar-filters#Scalar filters":"Most objects have some fields that can be filters for a query or mutation. The filters that are available depend on the data type, but the behavior of the String filters corresponds closely to DateTime and Int filters.\nString filters String properties have the following filters:\nFilter Description Example argument eq Equals (exact match) (filter: {id: {eq: \"match\"}}) in From this match of arrays (filter: {id: {in: [\"dough\", \"cookie_unit\"]}}) lt,gt,le, ge between Less than, greater than, or between a lexicographic range (filter: {id: {lt: \"M\"} regexp A regular expression match using RE2 syntax (filter: {id: {regexp: \"/hello/i\"}}) anyoftext A match for any entered strings, separated by spaces (filter: {id: {anyoftext: \"100 MD\"}}) alloftext A match for all entered strings (filter: {id {alloftext: \"MD\"}}) Integers, floats, DateTimes Properties that have a type of Int, Float, or DateTime can be filtered by the following keywords.\nlt le eq in between ge gt Each keyword has the same behavior as described in string filters, only they operate on numerical rather than lexicographic values. ℹ️ While the dateTime type uses the RFC 3339 format, some string fields may use the ISO 8601 format. This depends on the object and customer requirement. For these fields, the string filters work as chronological filters too. Enum filters Properties of the type Enum can be filtered by the following:\nlt le eq in between ge gt Each keyword has the same behavior as described in string filters.\nBoolean filters Boolean filters can be either true or false.\nGeolocation filters Geolocation filters return objects within specified geographic coordinates. They return matches within the specified GeoJSON polygon.\nIf a geolocation field can act as a filter, then the filter can work in one of the following behaviors:\nFilter Description near Within the specified distance from the polygon within In the polygon coordinates Intersects In the intersection of two polygons "},"title":"GraphQL types and filters"},"/reference/observability-metrics/":{"data":{"":"Rhize uses Prometheus to monitor metrics from many of its microservices. For the Kubernetes cluster, Rhize runs the Prometheus operator and monitors the accumulated metrics in Grafana dashboards. Monitoring occurs granularly, on the levels of cluster, pod, and container.","available-rhize-microservice-metrics#Available Rhize microservice metrics":"Several common metrics appear between Rhize microservices:\ngo process http. Service Instrumented Prometheus Go Application process metrics HTTP metrics* Additional Audit Y Y Y BAAS Y Y Y BPMN Y Y Y Y Core Y Y NATS Y Y Y Y Router Y Tempo Y Y ℹ️ HTTP metrics are noted as promhttp. BAAS Additional metrics on BAAS are from dgraph. These include two categories: Badger and Dgraph.\nSample # HELP badger_disk_reads_total Number of cumulative reads by Badger # TYPE badger_disk_reads_total untyped badger_disk_reads_total 0 # HELP badger_disk_writes_total Number of cumulative writes by Badger # TYPE badger_disk_writes_total untyped badger_disk_writes_total 0 # HELP badger_gets_total Total number of gets # TYPE badger_gets_total untyped badger_gets_total 0 # HELP dgraph_alpha_health_status Status of the alphas # TYPE dgraph_alpha_health_status gauge dgraph_alpha_health_status 1 # HELP dgraph_disk_free_bytes Total number of bytes free on disk # TYPE dgraph_disk_free_bytes gauge dgraph_disk_free_bytes{dir=\"postings_fs\"} 1.0153562112e+10 # HELP dgraph_disk_total_bytes Total number of bytes on disk # TYPE dgraph_disk_total_bytes gauge dgraph_disk_total_bytes{dir=\"postings_fs\"} 1.0447245312e+10 BPMN BPMN has four metrics unique to it, shown in full in the sample below.\nSample # HELP bpmn_execution_commands The number of BPMN commands that have started executing # TYPE bpmn_execution_commands counter bpmn_execution_commands 1162 # HELP bpmn_instances_started BPMN Instances started but not necessarily completed # TYPE bpmn_instances_started counter bpmn_instances_started 166 # HELP bpmn_queue_CommandConsumerQueue Number of BPMN Commands currently waiting to be executed # TYPE bpmn_queue_CommandConsumerQueue gauge bpmn_queue_CommandConsumerQueue 0 # HELP bpmn_queue_StartOnNatsMessages Number of BPMN trigger messages received from NATS # TYPE bpmn_queue_StartOnNatsMessages gauge bpmn_queue_StartOnNatsMessages 0 NATS NATS has two categories of metrics:\ngnatsd jetstream Sample # HELP gnatsd_connz_in_bytes in_bytes # TYPE gnatsd_connz_in_bytes counter gnatsd_connz_in_bytes{server_id=\"nats-demo-0\"} 0 # HELP gnatsd_connz_in_msgs in_msgs # TYPE gnatsd_connz_in_msgs counter gnatsd_connz_in_msgs{server_id=\"nats-demo-0\"} 0 # HELP gnatsd_connz_limit limit # TYPE gnatsd_connz_limit gauge gnatsd_connz_limit{server_id=\"nats-demo-0\"} 1024 # HELP jetstream_server_jetstream_disabled JetStream disabled or not # TYPE jetstream_server_jetstream_disabled gauge jetstream_server_jetstream_disabled{cluster=\"nats-demo\",domain=\"\",is_meta_leader=\"false\",meta_leader=\"nats-demo-1\",server_id=\"nats-demo-0\",server_name=\"nats-demo-0\"} 0 # HELP jetstream_server_max_memory JetStream Max Memory # TYPE jetstream_server_max_memory gauge jetstream_server_max_memory{cluster=\"nats-demo\",domain=\"\",is_meta_leader=\"false\",meta_leader=\"nats-demo-1\",server_id=\"nats-demo-0\",server_name=\"nats-demo-0\"} 2.147483648e+09 # HELP jetstream_server_max_storage JetStream Max Storage # TYPE jetstream_server_max_storage gauge jetstream_server_max_storage{cluster=\"nats-demo\",domain=\"\",is_meta_leader=\"false\",meta_leader=\"nats-demo-1\",server_id=\"nats-demo-0\",server_name=\"nats-demo-0\"} 5.36870912e+10 Router All metrics provided by Router are unique to Apollo Router.\nSample # HELP apollo_router_cache_hit_count apollo_router_cache_hit_count # TYPE apollo_router_cache_hit_count counter apollo_router_cache_hit_count{kind=\"query planner\",service_name=\"router-demo\",storage=\"memory\"} 121802 # HELP apollo_router_cache_hit_time apollo_router_cache_hit_time # TYPE apollo_router_cache_hit_time histogram apollo_router_cache_hit_time_bucket{kind=\"query planner\",service_name=\"router-demo\",storage=\"memory\",le=\"0.001\"} 121802 apollo_router_cache_hit_time_bucket{kind=\"query planner\",service_name=\"router-demo\",storage=\"memory\",le=\"0.005\"} 121802 apollo_router_cache_hit_time_bucket{kind=\"query planner\",service_name=\"router-demo\",storage=\"memory\",le=\"0.015\"} 121802 Tempo Tempo has a few categories of metrics:\njaeger prometheus tempo tempodb. The Tempo documentation details what these metrics measure.\nSample # HELP jaeger_tracer_baggage_restrictions_updates_total Number of times baggage restrictions were successfully updated # TYPE jaeger_tracer_baggage_restrictions_updates_total counter jaeger_tracer_baggage_restrictions_updates_total{result=\"err\"} 0 jaeger_tracer_baggage_restrictions_updates_total{result=\"ok\"} 0 # HELP jaeger_tracer_baggage_truncations_total Number of times baggage was truncated as per baggage restrictions # TYPE jaeger_tracer_baggage_truncations_total counter jaeger_tracer_baggage_truncations_total 0 # HELP prometheus_remote_storage_exemplars_in_total Exemplars in to remote storage, compare to exemplars out for queue managers. # TYPE prometheus_remote_storage_exemplars_in_total counter prometheus_remote_storage_exemplars_in_total 0 # HELP prometheus_remote_storage_histograms_in_total HistogramSamples in to remote storage, compare to histograms out for queue managers. # TYPE prometheus_remote_storage_histograms_in_total counter prometheus_remote_storage_histograms_in_total 0 # HELP prometheus_remote_storage_samples_in_total Samples in to remote storage, compare to samples out for queue managers. # TYPE prometheus_remote_storage_samples_in_total counter prometheus_remote_storage_samples_in_total 0 # HELP tempo_distributor_ingester_clients The current number of ingester clients. # TYPE tempo_distributor_ingester_clients gauge tempo_distributor_ingester_clients 0 # HELP tempo_distributor_metrics_generator_clients The current number of metrics-generator clients. # TYPE tempo_distributor_metrics_generator_clients gauge tempo_distributor_metrics_generator_clients 0 # HELP tempo_distributor_push_duration_seconds Records the amount of time to push a batch to the ingester. # TYPE tempo_distributor_push_duration_seconds histogram tempo_distributor_push_duration_seconds_bucket{le=\"0.005\"} 0 tempo_distributor_push_duration_seconds_bucket{le=\"0.01\"} 0 # HELP tempodb_backend_hedged_roundtrips_total Total number of hedged backend requests. Registered as a gauge for code sanity. This is a counter. # TYPE tempodb_backend_hedged_roundtrips_total gauge tempodb_backend_hedged_roundtrips_total 0 # HELP tempodb_blocklist_poll_duration_seconds Records the amount of time to poll and update the blocklist. # TYPE tempodb_blocklist_poll_duration_seconds histogram tempodb_blocklist_poll_duration_seconds_bucket{le=\"0\"} 0 tempodb_blocklist_poll_duration_seconds_bucket{le=\"60\"} 2012 tempodb_blocklist_poll_duration_seconds_bucket{le=\"120\"} 2012 ","dashboards#Dashboards":"A number of Grafana dashboards are pre-configured for use with Prometheus metrics. All dashboards in Grafana use Prometheus as a data source.\nYou can download them from Rhize Dashboard templates.","metrics-configuration#Metrics configuration":"For services where metrics are disabled by default, some configuration steps may be required. In case you are experimenting locally, this document includes for both the cluster and for Docker.\nAfter you enable metrics, add them into the Prometheus configuration file by pointing to that service endpoint. For example:\n- job_name: 'rhize-application-monitoring' honor_timestamps: true scrape_interval: 15s scrape_timeout: 10s metrics_path: /metrics scheme: http static_configs: - targets: ['audit-demo.demo.svc.cluster.local:8084', 'baas-alpha.demo.svc.cluster.local:8080', 'bpmn-demo.demo.svc.cluster.local:8081', 'core-demo.demo.svc.cluster.local:8080', 'grafana-demo.demo.svc.cluster.local:3000', 'tempo.demo.svc.cluster.local:3100', 'nats-demo-headless.demo.svc.cluster.local:7777', 'router-demo.demo.svc.cluster.local:9090'] NATS While NATS has no available metrics endpoint, the cluster includes a NATS Prometheus exporter. NATS metrics are exposed through port 7777.\nCluster Since the cluster already includes the exporter, no further configuration is required. This endpoint should be connected through the NATS headless pod. For example:\nnats-demo-headless.demo.svc.cluster.local:7777\nDocker To get NATS metrics in Docker, use the exporter mentioned in the preceding section. The following is a sample docker-compose configuration for the exporter.\nservices: # -- Other services nats-exporter: image: natsio/prometheus-nats-exporter:latest container_name: nats-exporter command: \"-varz 'http://nats:5555'\" depends_on: - nats ports: - 7777:7777 Access NATS metrics at localhost:7777/metrics\nRouter Cluster To enable metrics, the Router Helm chart needs to have several options added or changed, as follows: For details, refer to the Official Apollo instructions.\nrouter: configuration: # -- Other configuration prior telemetry: metrics: prometheus: enabled: true listen: 0.0.0.0:9090 path: \"/metrics\" # -- Open container ports containerPorts: metrics: 9090 # -- Enable service monitor serviceMonitor: enabled: true You can connect to this endpoint through the router pod. For example:\nrouter-demo.demo.svc.cluster.local:9090\nDocker To enable Router metrics, modify its configuration file. For details, refer to the Official Apollo Instructions.\nThe following is an example configuration:\n# -- Other configuration prior telemetry: exporters: metrics: prometheus: enabled: true listen: 0.0.0.0:9090 path: /metrics This opens the metrics endpoint on port 9090. To view it externally, you must expose the port in docker-compose. Once the port is exposed, view the metrics at localhost:9090/metrics","metrics-endpoints#Metrics endpoints":"The service metrics have endpoints at the following ports:\nService Available Enabled Port Audit Y Y 8084 BAAS Y Y 8080 BPMN Y Y 8081 Core Y Y 8080 NATS Y 7777 Router Y 9090 Tempo Y Y 3100 NATS has an available endpoint through an exporter that is present on the cluster. Router has an available endpoint that is disabled by default."},"title":"Observability metrics"},"/reference/service-config/":{"data":{"":"The Rhize services have different configuration parameters, as documented in the following pages.\nTo configure services in Kubernetes, add your configs to an overrides file. For example, you might add your configuration to the core service through a command like this:\nhelm upgrade --install \u003cMY_CORE_INSTANCE\u003e -f ./\u003cCORE_OVERRIDES\u003e.yaml libremfg/core -n \u003cMY_RHIZE_NAMESPACE\u003e On successful deployment, this configuration is added to the Kubernetes ConfigMap.\nAgent configurationConfiguration parameters for the Rhize agent Audit configurationConfiguration for the Rhize audit BPMN configurationAuthentication types for the Rhize BPMN Calendar configurationConfiguration for the Rhize Calendar Service Core configurationConfiguration for the Rhize core "},"title":"Service configuration"},"/reference/service-config/agent-configuration/":{"data":{"":"","azure#\u003ccode\u003eAZURE\u003c/code\u003e":"The Rhize agent collects data that is emitted in the manufacturing process and makes this data visible in the Rhize system. It works by connecting to equipment or groups of equipment that run over protocols such as OPC UA.\nAs the communication bridge between the Rhize Data Hub and your plant, the agent has multiple functions:\nIt subscribes to tags and republishes the changes in NATS. It creates an interface for the BPMN engine to send reads and writes to a data source and its associated equipment. OPC UA authentication types When authenticating over OPC UA, Rhize supports the following authentication types:\nAuthentication type Behavior Anonymous Connects without any necessary credential. Username Authenticates through a username and password in the config file, or through a Kubernetes secret. Certificate Uses the certificate on disk specified in the OPCUA.CertFile and OPCUA.KeyFile configs. If no certificate exists and the config specifies the OPCUA.GenCert property as true, automatically generates one. logging Logs the configurations to the console.\nAttributes Description type Specifies the logging configuration type: json, multi, or console. (Default: console) Level Configures the level of logging: Trace, Debug, Info, Warn, Error, Fatal, Panic. Defaults to Trace. (Default: trace) libreDataStoreGraphQL Attribute Description GRAPHQL_URL The URL of the GraphQL endpoint to use for interacting with Rhize services. (Default: http://localhost:8080/graphql) NATS Attribute Description SERVER_URL The URL for connecting to the NATS server. (Default: nats://system:system@localhost:4222) OIDC Configurations for Keycloak authentication and connection with OpenID Connect.\nAttribute Description serverUrl The URL of the OpenID Connect server. (Default: http://localhost:8090) realm Identifies the authentication domain for which the authentication request is being made. client_id The unique identifier assigned to the client application by the OIDC server. client_secret Used to authenticate the client alongside the client ID when making confidential requests. username The username credentials to authenticate with the OIDC server. password The password credentials to authenticate with the OIDC server. OpenTelemetry Attribute Description serverUrl The URL of the OpenTelemetry server. (Default: localhost:4317) OPCUA Attribute Description DiscoveryUrl The URL to locate and connect to OPC UA servers on a network. (Default: opc.tcp://localhost:4840) Endpoint The URL of the OPC UA service server. (Default: opc.tcp://localhost:4840) Username The username credentials to authenticate with the OPC UA server. Password The password credentials to authenticate with the OPC UA server. Mode The operational mode of the OPC UA server/client. (Default: None) Policy The security measures for OPC UA server communication. (Default: None) Auth The authentication mechanisms and user access control. (Default: Anonymous) AppUri The application’s unique URI within the OPC UA system. (Default: opc.tcp://localhost:4840) BUFFERS Attribute Description ProtocolQueueType The type of queue used for buffering communication protocol data. (Default: 0) HEALTH Attribute Description PollInterval The frequency of scans for component status and health. (Default: 1000) SubscriptionTimeout The maximum duration to wait to receive updates from subscribed data sources. (Default: 60000) SubscriptionMaxCount The maximum number of concurrent subscriptions for monitoring. (Default: 5) MQTT Attribute Description Version The version of MQTT used: 5.0 or 3.1.1. (Default: 3.1.1) ClientId The ID used in the MQTT broker. (Default: mqtt-client) Endpoint The URL of the MQTT broker. (Default: mqtt://localhost:1883) Username The username credentials to authenticate with the MQTT broker. Password The password credentials to authenticate with the MQTT broker. DecomposeJSON Enables or disables JSON payload decomposition into individual data fields. (Default: false) TimestampField The field to search to return timestamp information. (Default: timestamp) RequestTimeout The maximum duration to wait to receive a response to an MQTT request from the broker. (Default: 10) DATASOURCE Attribute Description ID The source ID to retrieve payload data from. (Default: DS_0806) AZURE Attribute Description CLIENT_ID The ID used to securely authenticate Azure service access. CLIENT_SECRET The secret key associated with the client ID for authentication. TENANT_ID The ID of the Azure Active Directory tenant where the service is registered. SERVICEBUS_HOSTNAME The URL of the Azure Service Bus namespace used for Azure ecosystem communication. (Default: bsl-dev.servicebus.windows.net) ","buffers#\u003ccode\u003eBUFFERS\u003c/code\u003e":"","datasource#\u003ccode\u003eDATASOURCE\u003c/code\u003e":"","health#\u003ccode\u003eHEALTH\u003c/code\u003e":"","libredatastoregraphql#\u003ccode\u003elibreDataStoreGraphQL\u003c/code\u003e":"","logging#\u003ccode\u003elogging\u003c/code\u003e":"","mqtt#\u003ccode\u003eMQTT\u003c/code\u003e":"","nats#\u003ccode\u003eNATS\u003c/code\u003e":"","oidc#\u003ccode\u003eOIDC\u003c/code\u003e":"","opc-ua-authentication-types#OPC UA authentication types":"","opcua#\u003ccode\u003eOPCUA\u003c/code\u003e":"","opentelemetry#\u003ccode\u003eOpenTelemetry\u003c/code\u003e":""},"title":"Agent configuration"},"/reference/service-config/audit-configuration/":{"data":{"":"","influxdb#\u003ccode\u003einfluxdb\u003c/code\u003e":"","logging#\u003ccode\u003elogging\u003c/code\u003e":"","oidc#\u003ccode\u003eOIDC\u003c/code\u003e":"","opentelemetry#\u003ccode\u003eOpenTelemetry\u003c/code\u003e":"","pg#\u003ccode\u003epg\u003c/code\u003e":"Audit offers a secure and unchangeable record of all activities that happen within the Rhize system.\nlogging Logs the configuration to the console.\nAttributes Description type Specifies the logging configuration type: json, multi, or console. (Default: console) Level Configures the level of logging: Trace, Debug, Info, Warn, Error, Fatal, and Panic. (Default: Trace) OIDC Configurations for Keycloak authentication and connection with OpenID Connect.\nAttributes Description serverUrl The URL of the OpenID Connect server. (Default: http://localhost:8090) realm Identifies the authentication domain for which the authentication request is being made. client_id The unique identifier assigned to the client application by the OIDC server. client_secret Used to authenticate the client when making confidential requests. username The username credentials of the user who is attempting to authenticate with the OIDC server. password The password credentials of the user who is attempting to authenticate with the OIDC server. OpenTelemetry Attributes Description serverUrl The URL of the OpenTelemetry server. (Default: localhost:4317) storage Description Storage system for the configuration data. Value options include: influxdb and pg. influxdb A time-series database that is used in conjunction with Grafana designed for handling time-stamped data, such as metrics, events, and logs, that change over time.\nAttributes Description serverUrl The URL of the InfluxDB server. (Default: http://localhost:8086) token The authentication token to authenticate requests to the InfluxDB server. (Default: my-token) pg PostgreSQL is a general-purpose relational database management system that supports a wide range of features and data types.\nAttributes Description host The host name of the PostgreSQL database server to which the client application connects. (Default: dbname) user The username to authenticate with the PostgreSQL database server. password The password associated with the specified PostgreSQL user account. port The port number on which the PostgreSQL database server is listening for incoming connections. (Default: 5432) ","storage#\u003ccode\u003estorage\u003c/code\u003e":""},"title":"Audit configuration"},"/reference/service-config/bpmn-configuration/":{"data":{"":"","commandconsumer#\u003ccode\u003ecommandConsumer\u003c/code\u003e":"","graphqlsubscriber#\u003ccode\u003eGraphQLSubscriber\u003c/code\u003e":"","http#\u003ccode\u003ehttp\u003c/code\u003e":"","libredatastoregraphql#\u003ccode\u003elibreDataStoreGraphQL\u003c/code\u003e":"","logging#\u003ccode\u003elogging\u003c/code\u003e":"","nats#\u003ccode\u003eNATS\u003c/code\u003e":"","oidc#\u003ccode\u003eOIDC\u003c/code\u003e":"","opentelemetry#\u003ccode\u003eOpenTelemetry\u003c/code\u003e":"","restapi#\u003ccode\u003eRESTAPI\u003c/code\u003e":"","secret#\u003ccode\u003eSECRET\u003c/code\u003e":"The Rhize BPMN acts as the tailored engine for processing low-code workflows designed within the [BPMN UI]/how-to/bpmn/. The configurations manage the connection and data flow on the BPMN engine to the other Rhize microservices.\nhttp All HTTP configurations are measured in seconds.\nAttribute Description ReadHeaderTimeout Wait duration for the request header to be fully read before timing out. (Default: 10) ReadTimeout Wait duration for the entire request to be read before timing out. (Default: 15) WriteTimeout Wait duration for the entire response to be written before timing out. (Default: 10) IdleTimeout Wait duration for the next request while the connection is idle before timing out. (Default: 30) logging Logs the configurations to the console.\nAttributes Description type Specifies the logging configuration type: json, multi, or console. (Default: console) Level Configures the level of logging: Trace, Debug, Info, Warn, Error, Fatal, or Panic. (Default: Debug) libreDataStoreGraphQL Attribute Description GRAPHQL_URL The URL of the GraphQL endpoint to use for interacting with Rhize services. (Default: http://localhost:4000/) GRAPHQL_CA_FILE The file path of the CA certificate used for secure communication with the GraphQL endpoint. (Default: '') viewInstance Configuration for service viewing instances.\nAttribute Description grafana org: The organization ID for the Grafana instance (Default: 1). tempoUid: The UID for Tempo integration in Grafana (Default: libre-tempo). url: The URL of the Grafana instance (Default: http://localhost:3000). loki accessToken: The access token for authentication with Loki (Default: ''). url: The URL of the Loki instance (Default: http://localhost:3100). tempo accessToken: The access token for authentication with Tempo (Default: ''). url: The URL of the Tempo instance (Default: http://localhost:3200). commandConsumer Attribute Description threads The number of threads for command consumption. (Default: 3) GraphQLSubscriber Attribute Description GRAPHQL_URL The URL of the GraphQL endpoint for the GraphQLSubscriber. (Default: http://localhost:4000/) NATS Attribute Description CommandStreamReplicas The number of replicas for the command stream. (Default: 1) JobResponseKVMaxGB The maximum size (in gigabytes) for the job response key-value store. (Default: 2) JobResponseKVReplicas The number of replicas for the job response key-value store. (Default: 1) JobResponseKVTTLMinutes the “time-to-live” (in minutes) for job response key-values. (Default: 7) WorkflowSpecificationKVReplicas The number of replicas for the workflow specification key-value store. (Default: 1) serverUrl The URL for connecting to the NATS server. (Default: nats://system:system@localhost:4222) OIDC Configurations for Keycloak authentication and connection with OpenID Connect.\nAttribute Description serverUrl The URL of the OpenID Connect server. (Default: http://localhost:8090) realm Identifies the authentication domain for which the authentication request is being made. client_id The unique identifier assigned to the client application by the OIDC server. client_secret Used to authenticate the client alongside the client ID when making confidential requests. username The username credentials to authenticate with the OIDC server. password The password credentials to authenticate with the OIDC server. OpenTelemetry Attribute Description serverUrl The URL of the OpenTelemetry server. (Default: localhost:4317) defaultDebug Enables or disables default debug mode. (Default: false) RESTAPI Attribute Description PORT The port number for RestAPI connection. (Default: 8080) SECRET Attribute Description KEY The secret key used to connect to the BPMN client. ","viewinstance#\u003ccode\u003eviewInstance\u003c/code\u003e":""},"title":"BPMN configuration"},"/reference/service-config/calendar-configuration/":{"data":{"":"","calendar#\u003ccode\u003eCalendar\u003c/code\u003e":"","database#\u003ccode\u003eDatabase\u003c/code\u003e":"The Calendar Service handles polling work calendar definitions and generating work calendar entries in the graph and time series databases.\nlogging Logs the configuration to the console.\nAttributes Description type Specifies the logging configuration type: json, multi, or console. (Default: console) Level Configures the level of logging: Trace, Debug, Info, Warn, Error, Fatal, and Panic. (Default: Trace) NATS Message broker that drives Rhize’s event-based architecture.\nAttributes Description SERVER_URL The URL of the NATS server. (Default: nats://system:system@localhost:4222) OIDC Configurations for Keycloak authentication and connection with OpenID Connect.\nAttributes Description serverUrl The URL of the OpenID Connect server. (Default: http://localhost:8090) realm Identifies the authentication domain for which the authentication request is being made. client_id The unique identifier assigned to the client application by the OIDC server. client_secret Used to authenticate the client when making confidential requests. username The username credentials of the user who is attempting to authenticate with the OIDC server. password The password credentials of the user who is attempting to authenticate with the OIDC server. OpenTelemetry Attributes Description serverUrl The URL of the OpenTelemetry server. (Default: localhost:4317) samplingRate The sampling rate for traces. (Default: 1) libreDataStoreGraphQL Attribute Description GRAPHQL_URL The URL of the GraphQL endpoint to use for interacting with Rhize services. (Default: http://localhost:8080/graphql) GraphQLSubscriber Attribute Description GRAPHQL_URL The URL of the GraphQL endpoint for the GraphQLSubscriber. (Default: http://localhost:4000/) RESTAPI Attribute Description PORT The port number for RestAPI connection. (Default: 8080) Calendar Specific configuration options for the calendar service.\nAttribute Description QueryIntervalMinutes How often to poll the work calendar definitions (Default: 10) QUERY Query options specific to the calendar service\nAttribute Description hierarchyScopeRecursionDepth How deep to recurse through the hierarchy scope hierarchy (Default: 3) equipmentRecursionDepth How deep to recurse through the equipment hierarchy (Default: 3) Influx3 InfluxDB3 server options\nAttribute Description Database The name of the database to connect to (Default: Libre_calendar-service) Host The host of the Influx3 database (Default: http://localhost:8096) Organization The Influx3 Organization (Influx3 Cloud) (Default: Libre) TokenPrefix The prefix used in the authorization token (Token for Influx 3 cloud, Bearer for Influx3 Edge) (Default: Token) Token The authorization token to attach to any Influx3 requests Postgres Postgres server options\nAttribute Description Host The hostname of the Postgres database (Default: localhost) Port Those port of the Postgres database (Default: 5432) User The Postgres user name (Default: postgres) Password The Postgres instance password (Default: postgres) Database The database name for the Postgres instance (Default: Libre) Database Which database instance to use, either Postgres or Influx3 (Default: Influx3)","graphqlsubscriber#\u003ccode\u003eGraphQLSubscriber\u003c/code\u003e":"","influx3#\u003ccode\u003eInflux3\u003c/code\u003e":"","libredatastoregraphql#\u003ccode\u003elibreDataStoreGraphQL\u003c/code\u003e":"","logging#\u003ccode\u003elogging\u003c/code\u003e":"","nats#\u003ccode\u003eNATS\u003c/code\u003e":"","oidc#\u003ccode\u003eOIDC\u003c/code\u003e":"","opentelemetry#\u003ccode\u003eOpenTelemetry\u003c/code\u003e":"","postgres#\u003ccode\u003ePostgres\u003c/code\u003e":"","query#\u003ccode\u003eQUERY\u003c/code\u003e":"","restapi#\u003ccode\u003eRESTAPI\u003c/code\u003e":""},"title":"Calendar configuration"},"/reference/service-config/core-configuration/":{"data":{"":"","bpmn#\u003ccode\u003eBPMN\u003c/code\u003e":"","graphqlserver#\u003ccode\u003egraphQLServer\u003c/code\u003e":"","libredatastoregraphql#\u003ccode\u003elibreDataStoreGraphQL\u003c/code\u003e":"","logging#\u003ccode\u003elogging\u003c/code\u003e":"","nats#\u003ccode\u003eNATS\u003c/code\u003e":"","oidc#\u003ccode\u003eOIDC\u003c/code\u003e":"","opentelemetry#\u003ccode\u003eOpenTelemetry\u003c/code\u003e":"","secret#\u003ccode\u003eSECRET\u003c/code\u003e":"","timeseries#\u003ccode\u003eTimeSeries\u003c/code\u003e":"The Core service oversees data sources such as OPC-UA servers and manages the publication and subscription of topics within the NATS messaging system.\nlogging Logs the configuration to the console.\nAttributes Description type Specifies the logging configuration type: json, multi, or console. (Default: console) Level Configures the level of logging: Trace, Debug, Info, Warn, Error, Fatal, and Panic. (Default: Trace) NATS Message broker that drives Rhize’s event-based architecture.\nAttributes Description serverUrl The URL of the NATS server. (Default: nats://system:system@localhost:4222) replicas The number of replicas or instances of the NATS server to be deployed. (Default: 1) OIDC Configurations for Keycloak authentication and connection with OpenID Connect.\nAttributes Description serverUrl The URL of the OpenID Connect server. (Default: http://localhost:8090) realm Identifies the authentication domain for which the authentication request is being made. client_id The unique identifier assigned to the client application by the OIDC server. client_secret Used to authenticate the client when making confidential requests. username The username credentials of the user who is attempting to authenticate with the OIDC server. password The password credentials of the user who is attempting to authenticate with the OIDC server. OpenTelemetry Attributes Description serverUrl The URL of the OpenTelemetry server. (Default: localhost:4317) samplingRate The sampling rate for traces. (Default: 1) SECRET Attributes Description KEY The SECRET KEY used for authorization within Core. graphQLServer The server used to connect to the GraphQL playground.\nAttributes Description Port The port used within the URL that connects to the graphQLServer. (Default: 4001) libreDataStoreGraphQL Attributes Description GraphQLUrl The URL of the GraphQL endpoint to use for interacting with Rhize services. (Default: http://localhost:8080/graphql) BPMN Attributes Description GraphQLUrl The URL of the BPMN endpoint to use for interacting with Rhize services. (Default: http://localhost:8081) TimeSeries Attributes Description Enabled Enables the use of TimeSeries. (Default: false) "},"title":"Core configuration"},"/releases/":{"data":{"":"Read about new Rhize features and how to upgrade versions.\n3.0.3Release notes for v3.0.3 of the Rhize application 3.0.1Release notes for v3.0.1 of the Rhize application Rhize 3.0Notes for v3.0 of the Rhize Manufacturing Data Hub. A flexible architecture for workflow orchestration, message handling, standards-based modeling, and custom MES apps. ChangelogA log of all changes to the Rhize application "},"title":"Releases"},"/releases/3-0-1/":{"data":{"":"Release notes for version 3.0.1 of the Rhize application.\nThis release includes bug fixes, improvements for developer experience, and new relationships between entities in the ISA-95 schema. It also clears the way for the new, improved Work Calendar service (still in release candidacy).\nRelease date: 23 Jul 2024\n⚠️ This release includes a few minor breaking changes, review the upgrade instructions for details. ","changes-by-service#Changes by service":"The following sections document the changes this release brings to each service.\nAudit Fixes\nFix Postgres user tag query to use column name user_id instead of the user reserved keyword BPMN Features\nError boundary events now support call-activity tasks. If a task in the called process aborts, the error boundary captures that event. Support Custom Certificate authorities so that the BPMN service can run in networks with internal certificates authorities. Message events now support topics that use the + wildcard. Add configuration option to set __traceDebug flag on all BPMN executions. For details, read Debug workflows. Allow __traceDebug to be set from variable context. Add support for querying a range of work calendar entries for a Work Calendar Definition. When configured, calendar data persists to a timeseries DB. ⚠️Breaking . Work calendars now query for WorkCalendarDefinitions not workCalendars. This makes it possible to configure calendars in the UI. Work calendar execution is now based on Hierarchy Scope. Change\nReduce number of retries in BPMN recovery, speeding up response times when node does not exist. From the user’s perspective, the delay was particularly notable on API BPMN triggers when the specified workflow did not exist. Fixes\nCapture panics in JSONata go for better recovery and discovery. ⚠️Breaking . Limit BPMN users to only tokens with specific audience and scope. Workflow specifications not found in NATS now fall back to database lookups before failing. Fixes issue when spinning up new development containers. Include metadata fields for jobResponse. Fixes panic if node tried to access these fields. Respect debug flag for BPMNs triggered with createAndRunBpmn. Reuse expected Audience for clients to match documented client name. Do not create calendar entry for end events. Fix nil pointer error when aborting tasks without parents. Schema Features\n⚠️Breaking . Add id and label to stateTransitionInstance. Add menu URL for portal in UI (preparation for future work in UI). Add filter to query material actual by a material requirement and vice-versa. Link workCalendarEntries to equipment versions. Include Kafka in the enum values for data sources. Added optional InformationObjectTypeEnum to OperationsEventRecordEntry. Fixes\nAdd 0:N relationship from materialRequirements to materialActual. The segmentResponse entity can now have multiple jobResponses attached. Change segmentResponse relationship to JobResponse to be a 1:N relationship ⚠️Breaking . Change stateModelTransition from state to a relationship. Fix inverse relationship between operationseventDefinitionProperty and operationsEventProperty. ","checksums#Checksums":"When you install, check the container images against these checksums:\nBPMN engine: registry.gitlab.com/libremfg/bpmn-engine:v3.0.1 sha256:d5395f8a5a342c904367385c7996c1a8936a0eac498822191e0bd3e064dc72d3 Libre Core: registry.gitlab.com/libremfg/libre-core:v3.0.1 sha256:9c312df6c8c08e41221f6bb68e1d3609db51cf87cc5d6d85f644d452c4617509 Libre Agent: registry.gitlab.com/libremfg/libre-agent:v3.0.1 sha256:696a9e028ffbca5ca3820fde7223b018e88899719c7b4e784c2e86b921f61615 Libre Audit: registry.gitlab.com/libremfg/libre-audit:v3.0.1 sha256:718d57c7e7206450bca3337f682c4782cb5222b20ae24b47ed838b5f7f963db3 Libre Audit Postgres registry.gitlab.com/libremfg/libre-audit/postgres:v3.0.1 sha256:7629dde995db6b22020ab85c68c25be478cf54c7d6d64e5a30aab72d336432f8 Libre BaaS: registry.gitlab.com/libremfg/baas:v3.0.1 sha256:605513936d88fb09babc7d1b38410c32674d14b6a38db1363fd0e6fbf27aaec3 Libre router init: registry.gitlab.com/libremfg/libre-router-init:v3.0.1 sha256:32084bd47013974317aa39288c28686084500a8c51a5bfa0b2b135d5a77bb8e6 Libre Keycloak theme: registry.gitlab.com/libremfg/frontend/libre-keycloak-theme:v3.0.1 sha256:ed9e6cd57d02c5f7177d328f40fd440021432ed874ffb64c765f1a378fd7c08a Libre Admin UI: registry.gitlab.com/libremfg/frontend/libre-admin-ui:v3.0.1 sha256:f796ac698ebde6bb5f7dd08bb11246e4658abf2b8d61bf3299cd01f3ae48c19e Download v3.0.1-checksums.txt","compatibility#Compatibility":" v3.0.1 compatibility Rhize v3.0.1 has been tested to work with the following third-party applications: Apollo router 1.15.1 Grafana: 9.4.7 Keycloak: 21.1.1 Keycloak Postgres: 15.3.0 Loki: 2.9.3 NATS: 2.10.17 Tempo: 2.3.1 ","upgrade#Upgrade":" Back up first Before upgrading, consider backing up your Rhize services. To review the procedure, read our Back up guides. To upgrade to v3.0.1, first ensure that you have made corrections for the aforementioned breaking changes.\nRhize now limits BPMN users to the libreBpmn audience and bpmn:mutation role. If users do not have this audience and role, they will unable to log in. To fix this:\nLog in to the Keycloak service in your Rhize environment. Select the libre realm. Configure the audience and role for the Users or groups that should be able to run BPMN workflows. For details, read Install Keycloak. Work calendars now query for workCalendarDefinitions, not workCalendars. There is a chance this may break some calendars that already exist. To mitigate this, ensure you set active definitions for the calendars you want to query. Read About calendars and overrides for details about how the service and its relationships work.\nAfter you’ve made the necessary mitigations, follow the upgrade the Upgrade instructions.\nPost upgrade Two schema changes to the state model also might cause breaking changes after upgrade. You can correct these issues by re-uploading the data.\nFor the specific breaks:\nThe StateTransitionInstance entity now has an id and label property. Any such entities that were created without an ID and label are now invalid. To mitigate this:\nQuery for your StateTransitionInstance: query QueryStateTransitionInstance { queryStateTransitionInstance { iid } } Use a mutation to update these to have an ID and label. Any existing StateModelTransitions using a from or to property will break. Change these to a relationship."},"title":"3.0.1"},"/releases/3-0-3/":{"data":{"":"Changelog for version 3.0.3 of the Rhize application.\nRelease date: 13 Nov 2024","changes-by-service#Changes by service":"The following sections document the changes this release brings to each service.\nAdmin Add\nAdd portal menu link Add output container build to $CI_REGISTRY/libremfg/docker/admin-ui:$CI_COMMIT_TAG Change\nChange WorkMaster parameters input to textbox to support multi-line values better Change BPMN Process instances modal default search and width Refactor Material Class Properties table Refactor Material Definition table Refactor Operational Location Class properties table Refactor Operational Location properties table Refactor Operations Event Class table Refactor Operations Event Definition to use infinite scroll Refactor Person Properties table Refactor Personnel Class table Refactor Physical Asset Class table Refactor Physical Asset Classes to use infinite scroll Refactor Physical Asset table Refactor Physical assets to use infinite scroll Refactor Work Masters to use infinite scroll Fix\nFix Person description allowing e character Fix Change material definition Version State prompt text Fix Create or Select a material definition Fix Personnel Class version containers text Fix Personnel Class save as hierarchy scope not saving Fix typos in person page referring to Physical Asset Fix Material Class search Fix Operational Location Class hierarchy scope edit Fix search on large list of Operational Location Class property Fix enable/disable equipment property Fix equipment property edit Fix equipment property edit query Fix Material Class isAssembledFrom relationship edits Fix BPMN view sidebar title Fix equipment nested tree items not showing active version Fix user details and support typo Fix large number of Person failing to display Fix large number of Personnel Classes failing to display Fix large number of Operational Location Classes failing to display Fix Variable page logic to Create and Set as active Fix Operations Definition Segment WorkType selection Fix invalid characters in Operations Definition Segment Component Fix portal hang when not configured Fix Equipment properties text Fix numerical error in input fields on Person version for Name and Details Fix Operational Location change not refreshing screen after edit Fix validation of canCreateNewVersion in Material Class page Fix typos in Operational Location page referring to Operational Location Class Fix audit trail PDF export header text overlap Fix cache issue when enabling and disabling Operations Event Definition Fix previous tag selection script in CI/CD Fix typo in Create Operational Location BPMN engine Fix\nFix linting and container version issues from CI Schema Add\nAdd regex search to Event.messageText Add JobResponseData.valueLong for large values without search Change\nChange baas version dependencies to v3.0.3 from v3.0.0 Fix\nFix pipeline error by pinning version of gqlgen BAAS Fix\nFix large strings breaking badger indexes by constraining string length to 64000 when indexed and not by hash Core Fix\nFix supergraph compose warning on JobState enum descriptions Agent No changes for this release\nAudit Add\nAdd default partition size of 1 month to Postgres container Fix\nFix Postgres initialization of partition tables Keycloak Theme No changes for this release.\nRouter No changes for this release.","checksums#Checksums":"When you install, check the container images against these checksums:\nAdmin: registry.gitlab.com/libremfg/frontend/libre-admin-ui:v3.0.3 sha256:862967a4873ae003a78b9acb336eba68a91eb5fedae8bc55244f770d629b4b35 BPMN Engine: registry.gitlab.com/libremfg/bpmn-engine:v3.0.3 sha256:962d116a79f85012f4c1724aeb28124ec71085a1730a1ce45aa6d7a9c1b4e8e3 BAAS: registry.gitlab.com/libremfg/baas:v3.0.3 sha256:6851576b73b6b4976c77f792f289e492d660913740a2b3073844c0447e4759bc Libre Core: registry.gitlab.com/libremfg/libre-core:v3.0.3 sha256:3fd8f3ea716903bb63d77958dfc1ac6183123b38ee757e62530a26adc0aa6d00 Libre Agent: registry.gitlab.com/libremfg/libre-agent:v3.0.3 sha256:406756b759aed19d5c39a40150ead8a43a50f6e5a66ccc4c6ea64ee2113a67cb Libre Audit: registry.gitlab.com/libremfg/libre-audit:v3.0.3 sha256:a8cac8a63e5bfa62d6ee464014ccab32dfd572db52d45f5b2f94bf9973c008e9 Libre Audit Postgres: registry.gitlab.com/libremfg/libre-audit/postgres:v3.0.3 sha256:6bac6915182bee39f25949bafdc5317e3bbdaf8fcb73ba9c40714c34c9d28a67 Libre Keycloak Theme: registry.gitlab.com/libremfg/frontend/libre-keycloak-theme:v3.0.3 sha256:3278706d0fdb1d0bdeb6485435129240cb05e36c39fb5d5566aa06348090f338 Libre Router Init: registry.gitlab.com/libremfg/libre-router-init:v3.0.3 sha256:aa0a443f0452c29d6720203c81d5f91910a86bcd4a45609b1047ff2b9bd17c95 Download v3.0.3-checksums.txt","compatibility#Compatibility":" v3.0.3 compatibility Rhize v3.0.3 has been tested to work with the following third-party applications: Apollo router 1.15.1 Grafana: 9.4.7 Keycloak: 21.1.1 Keycloak Postgres: 15.3.0 Loki: 2.9.3 NATS: 2.10.17 Tempo: 2.3.1 ","upgrade#Upgrade":"To upgrade to v3.0.3, follow the Upgrade instructions."},"title":"3.0.3"},"/releases/3-0/":{"data":{"":"Rhize version 3.0 is now in general release! 🥳 As a full rewrite of our earlier Libre 2.0 application, this release functionally announces a new product.\nRead more to learn about the features available in the Rhize Manufacturing Data Hub.","features#Features":"Each of the following features supports Rhize’s key design goals:\nProvide a highly reliable means to collect manufacturing data from varied sources Standardize this data in a way that accurately places it within an event in the context of an entire manufacturing operation Offer a programmable engine to write custom logic to process the data and send it between systems Serve as a complete backend application and architecture for MES/MOM frontends Expose this system through an interface that is accessible to the widest number of stakeholders in the manufacturing operation. Knowledge graph and GraphQL Data is stored in the Rhize DB, a graph database with a custom schema that uses the ISA-95 standard as a data model. This database provides several advantages over a relational database or data lake:\nStandardization. The database schema enforces standardization using a data model that adequately represents an entire manufacturing operation. All data stored in Rhize has a common structure, unlike the heterogeneous data of a data lake. Graph Structures. The graph database represents the object model of ISA-95 exactly. Every event in a manufacturing process is connected to all other events. For example, a job response might have associated operations requests, personnel, equipment, and materials that are consumed and produced. All these objects have their associated classes, instances, and contexts (where context could be a time range, operations segment, or zone of information exchange). Intuitive, minimal queries. The Rhize DB is exposed through a GraphQL API, which provides a complete interface through a single endpoint. Users query exactly what they want without needing to handle the relational algebra of SQL or the over-fetching of a REST API. With standardization, graph structure, and complete interfaces, the Rhize DB thus constitutes a knowledge graph that represents the entire state of a manufacturing operation. Users use this knowledge graph to run simulations, discover optimizations, and train machine-learning models for predictive analysis.\nYou can read and write to the knowledge graph through a GraphQL explorer, a BPMN workflow, a custom frontend, or through the Modeling UI. To learn how to use GraphQL for manufacturing analysis, read the Rhize guide to GraphQL.\nModeling UI The Rhize UI is a graphical interface to model and store the objects in your manufacturing process. Without needing programming knowledge, your operators can use the Admin UI to define the items in your role-based equipment hierarchy. For example, you can create and associate equipment with equipment classes, hierarchy scopes, data sources, and segments―all the objects that change infrequently.\nThese models provide the foundational data objects to associate with the dynamic data that you collect, analyze, and handle during ongoing operations. To learn more, read the Model objects guide and its corresponding topic that describes the Master definitions and fields.\nLow-code workflow orchestration While some aspects of manufacturing, such as the plant site, are constant, most manufacturing data is emitted dynamically during production. Rhize’s BPMN editor provides a graphical interface to listen to events, write logic based on event conditions, and process data to send to other devices or store in the knowledge graph. It has gateways to represent conditions, messaging to subscribe and publish to topics, and a JSON interpreter to transform variables and process manufacturing data in batches.\nFor example, you can write a BPMN workflow to do any combination of the following:\nAutomatically write the values associated with a specific job to the database Evaluate incoming data for conditions and write logic to transform this data and send it Subscribe and publish to topics, coordinating communication between systems Listen to events and read and write machine data through OPC-UA To get started, read the Guides to Use BPMN and Write rules to trigger workflows from data source values.\nAgent and message broker Rhize collects data from multiple data sources and protocols. To bridge the Rhize system with your devices, Rhize has an agent to collect data from OPC UA and MQTT servers. The Rhize agent listens for your devices’ messages and publishes them to the message broker.\nBased on NATS, Rhize’s message broker decouples communication through a publish-and-subscribe pattern. Rhize BPMN workflows can subscribe to topics and publish topics back. On your side, your manufacturing devices can publish topics to Rhize and subscribe to topics that are published from some BPMN trigger or to changes in the database. You can also use Rhize workflows to publish and subscribe to topics on an external broker.\nTo learn more, read the guide to Connect a data source and the reference for Agent Configuration.\nAudit trails Manufacturing often happens in strict regulatory environments and in systems where security is critical. The Rhize audit log maintains a record of all changes that happen in the system. To learn more, read the Audit guide.\nSecure, vendor-agnostic infrastructure Rhize is built on open standards and heavily uses open-source cloud infrastructure. As such, Rhize is designed to conform to your operation’s IT and manufacturing processes.\nRhize runs on your IT infrastructure. The data model itself is based on a widely recognized standard, ISA-95. Your teams control its security and access policies, with authentication provided through OpenID Connect.\nHigh availability and reliability Mission-critical systems, such as an MES, must be highly reliable and available. Information flows in manufacturing can also have very high demands for CPU, network throughput, and memory.\nThe design of Rhize accommodates horizontal scaling, where the application runs in clusters of multiple computers. This distributed execution ensures that the system continues functioning during upgrades, periods of high use, and momentary failures in single nodes. Specifically, Rhize uses Kubernetes for container orchestration, a distributed, ACID-compliant database, and a message broker that uses replication to harden against failure.\nThe reliance on open source also means that your operators do not need to learn a specialized skill to deploy Rhize. It uses the same cloud-native applications that power the modern web.\nRead all about how to Deploy Rhize in the Deploy guides.\nA backend for MES applications With the knowledge graph, message broker, and workflow executor, and secure infrastructure, Rhize also provides all the components necessary to write custom applications for Manufacturing Execution Systems. Rather than rely on a vendor to sell an MES system that prescribes exactly what data it can use and how it can use this data, use Rhize to build the custom MES frontend of your choice.\nCombined with rapid prototyping and low-code tools, you can use Rhize to build MES applications quickly and iterate on them as your operators use them in production.","install#Install":"To install, read the Install guide. Or, if upgrading from a release candidate, use the Upgrade guide. If upgrading, be sure to review the changelog to be aware of any possible breaking changes.\nv3.0.0 compatibility Rhize v3.0.0 has been tested to work with the following third-party applications: Apollo router 1.15.1 Grafana: 9.4.7 Keycloak: 21.1.1 Keycloak Postgres: 15.3.0 Loki: 2.3.9 NATS: 2.10.10 Tempo: 2.3.1 When you install, check the container images against these checksums:\nBPMN engine: registry.gitlab.com/libremfg/bpmn-engine:v3.0.0 sha256:92ef7bd7655736fc424b749f757358587b8ae21a994320c0e3dbd596b0d8e1f2 Libre Admin UI: registry.gitlab.com/libremfg/frontend/libre-admin-ui:v3.0.0 sha256:64963e65c1e44abd973617d71d00db7500d684178529bf8335eb4a2974b412e8 Libre Agent: registry.gitlab.com/libremfg/libre-agent:v3.0.0 sha256:037dca7b8cde5f3cd9505de50ba20da2454d292b19860be5de17a03fd4355625 Libre Audit: registry.gitlab.com/libremfg/libre-audit:v3.0.0 sha256:764826140cd2da7764670c22b83914f51459cb2091eaee50a4fb046316e9fcbc Libre BaaS: registry.gitlab.com/libremfg/baas:v3.0.0 sha256:8763118064f0ee0230801114fd8534e76d0db51119dd3077913d3a54276c7621 Libre Core: registry.gitlab.com/libremfg/libre-core:v3.0.0 sha256:cdd97fc7243a06c6eafe65e32a65902084c0fc591fc4bfbf7ceee7012dccde0 Libre Keycloak theme: registry.gitlab.com/libremfg/frontend/libre-keycloak-theme:v3.0.0 sha256:7ed4cabf4ac7c3d65eac50412f8c3619ad9b971ef835797d8275ee65b93e372f Libre router init: registry.gitlab.com/libremfg/libre-router-init:v3.0.0 sha256:c4217c0d5a3f079dd751570f725d381a327c1ab00189bd593ac221a86036c98e Libre Audit Postgres: registry.gitlab.com/libremfg/libre-audit/postgres:v3.0.0 sha256:fb18a4fcb22ad76d4b8e4c404a3a5faf4207835166061d99ffd36801064752ab Download v3.0.0-checksums.txt\nChangelogs The following changelogs document the features, fixes, and refactoring that went into this release.\n3.0.0 3.0.0rc09 3.0.0rc08 3.0.0rc07 3.0.0rc06 3.0.0rc05 ","read-more#Read more":" Get started introduces Rhize’s application and architecture. Manufacturing data hub explains why Rhize chose its design. Use cases explains ways customers use Rhize. ","what-is-rhize#What is Rhize?":"Rhize is the world’s first manufacturing data hub. It collects event data emitted from all levels of a manufacturing process, stores this data in a standardized schema, and exposes access to the event stream so users can orchestrate workflows in real-time.\nCustomers use Rhize to observe and react to past and present states of their manufacturing operation. Its use cases include:\nA single source of truth for manufacturing records. With manufacturing events represented as a knowledge graph, Rhize provides access to any view of the system through a single query to a single endpoint. You can query detailed reports about a specific asset, such as a manufacturing batch, or compare production across broad categories, for example, between equipment items or production lines. A backend for custom MES and MOM applications. As the data hub has a message broker and real-time database, operators can program custom level-three applications for functions such as track and trace, material genealogy, electronic batch record creation, and KPI calculations. An integrator of legacy systems. Rhize accepts data from legacy systems and converts them to a standardized schema. Thus, it serves as a hub to communicate between legacy systems in ways that would otherwise be impossible or very difficult to maintain. "},"title":"Rhize 3.0"},"/releases/changelog/":{"data":{"":" 3.0.0 (general release)Change Log for v3.0.0 of the Rhize application 3.0.0rc09Change Log for v3.0.0rc9 of the Rhize application 3.0.0rc08Release notes for v3.0.0rc08 of the Rhize application 3.0.0rc07Release notes for v3.0.0rc07 of the Rhize application 3.0.0rc06Release notes for v3.0.0rc06 of the Rhize application 3.0.0rc05Release notes for v3.0.0rc05 of the Rhize application "},"title":"Changelog"},"/releases/changelog/3-0-0/":{"data":{"":"Changelog for version 3.0.0 of the Rhize application.\nRelease date: 25th March, 2024","breaking-changes#Breaking changes":"","changes-by-service#Changes by service":"Admin UI Features\nAdd includesPropertiesOf input when creating a new version of OperationsEventClass Add IncludesPropertiesOf option to Equipment Class General tab Add momentjs for timezones as select in Equipment Version timezone field Add BPMN side panel for OPCUA Method Call Add clear option to WorkMaster disable modal Add edits for Static properties Add enable and disable functionality to work calendar definition Add EquipmentClass ISA-95 property type selection Add EquipmentLevel to Equipment Class Add inheritance of Operational Location Class properties into Operational Location Add inherited properties from linked Operations Event Class to Operations Event Class properties page Add manufacturer to Physical Asset Class general tab Add new modal to BPMN Add new Work Calendar Definition select on Equipment Version management Add Operational Location and Spatial Definition to Equipment General tab Add OperationsEventClass to OperationsEventDefinition Add option to disable Operations Definition Add option to disable previous work master version when create a new version Add option to disable Work Calendar Definition Property Add option to disable Work Calendar Entry Add option to enable a disabled Process Segment Add page to add manual Work Calendar Entries Add pagination to Work Calendar Entries Add properties to Work Calendar Definition Add Published date and Hierarchy Scope to WorkMaster Add relationship between Operations Event Definition and Work Master Add scrollbar to Work Calendar Definition Add Spatial Definition to Physical Location \u0026 Physical Asset Add start weekday selection to Work Calendar Definition Add static properties on Equipment Class Add timezone on Work Calendar Add timezone on Work Calendar Entry and Equipment Add user store to UserManager and set automatic silent renew Add Work Calendar Definition manage entries component Add Work Calendar Definition management Add Work Calendar Setup Change\nChange ability to edit approved version of an Operations Event Class Change ability to edit for review version of a Material Class Change data source topic to prevent usage of . in the name Change input validation on Work Calendar Definitions to prevent use of dots . Change input validation on Work Calendars to prevent spaces Change Material Definition Properties table sort order Change the Work Calendar Definition Entry card design to use ne card props Fix\nFix Get Started action trigger Fix ability to create an Operational Location Class Version Fix ability to create duplicate property labels for the same Physical Asset Class Fix ability to edit an Active Process Segment Fix ability to edit an Active Work Calendar Fix ability to edit Operations Definition Segment Parameter Fix ability to edit Person Versions when in Draft or For Review State Fix adding new Material Definition Property Fix adding Personnel Class property by adding safe operator to prevent table from breaking under certain pre-conditions Fix allowing edit of an approved Physical Asset Fix Audit Log date format Fix auto-increment of Person Version, when creating a new one Fix BPMN Viewer Get Instance invalid syntax error Fix changing Process Segment version status from Draft to Active Fix disable of Material Definition Fix disabled nature of Hierarchy Scope when the selected Physical Asset Class version is in Draft or For Review Fix disabled state of buttons when creating new Equipment Fix display of BPMN instances that were created with the active version Fix display of Equipment Property metadata Fix display of inherited properties in Personnel Class Fix display of linked Material Class properties Fix display of more than 1000 Equipment Class in the left sidebar Fix display of more than 1000 Equipment in the left sidebar Fix display of more than 1000 Data Source in the left sidebar Fix display of more than 1000 Hierarchy Scope in the left sidebar Fix display of more than 1000 Material Class in the left sidebar Fix display of more than 1000 Material Definitions in left sidebar Fix display of more than 1000 Operational Definition in the left sidebar Fix display of more than 1000 Operational Location Class in the left sidebar Fix display of more than 1000 Operational Location in the left sidebar Fix display of more than 1000 Person in the left sidebar Fix display of more than 1000 Personnel Class in the left sidebar Fix display of more than 1000 Work Calendar in the left sidebar Fix display of Person Property metadata Fix display of value field in linked properties from Resource Specifications Fix duplicated column name on Operational Location Class Property and Property Type Fix edit of linked property on resource specifications Fix environmental variable styling and navigation Fix Equipment Class version save-as with properties Fix Equipment general tab input boxes to be disabled when ACTIVE or APPROVED Fix Equipment Property error when Work Calendar Definition query failed Fix error message when creating a Work Calendar Definition Entry with a duplicate ID Fix error message when creating a Work Calendar Definition with a duplicate ID Fix heading of Data Source modal when changing to For Review Fix heading of disable modal for Material Class to remove version Fix heading on BPMN Editor modal for marking current BPMN as Active Fix heading on changing Data Source Version from Draft to Active Fix hierarchy scope persisting after creating a new Work Calendar Definition Fix hierarchy scope selection on Physical Asset general tab Fix inconsistent display of inputs when editing draft Data Source Fix instance list repeat queries using start/end times Fix manual page refresh to display added Operations Definition Fix Material Definition inherited properties filter on Draft versions Fix Material Definition property enable/disable Fix missing display of inherited properties in Person from Personnel Class Fix mount localStorage data as moment object to prevent errors and add a mount the table on load the page Fix silenced error message when using createSecret Fix Process Segment Parameter search Fix Operations Segment Parameter search Fix query to support Custom Query without broke the entire response Fix re-enable of disabled Work Calendar Definition Entry Fix references to Physical Asset in Operations Event pages Fix refresh of Equipment when adding using the + symbol Fix refresh when adding a new Operations Definition version Fix required validation for Hierarchy Scope on Equipment Class Version Fix required validation for Hierarchy Scope on Physical Asset Version Fix search for Equipment Class Property by type and UoM Fix See Active toggle functionality on Material Definition Properties Fix sidebar order for Work Calendar Entries Fix Work Calendar Definition enable and disable code review fixes Fix Work Calendar Definition Properties search box placeholder text Fix WorkMaster search Agent No changes since previous release.\nAudit Features\nAdd storage option for postgres Fix\nFix environmental variable conflict BAAS No changes since previous release.\nBPMN Features\nAdd viper configuration for http timeout values Fix\nFix one workflow deprecating when two subscribe to message start Fix execution of nested call activity when depth greater than 2 Change\nChange JSONata-go library to v1.8.4 from v1.6.6 Core Features\nAdd hierarchyScope to CreateEquipmentClassVersionInput Add includesPropertiesOf to CreateEquipmentClassVersionInput Add includesPropertiesOf to CreateOperationsEventClassVersionInput Add inheritedProperties to OperationalLocationClassVersion Add inheritedProperties to OperationsEventClassVersion Add inheritedProperties to PersonnelClassVersion Add samplingRate configuration option to OpenTelemetry Add data source methods when cloning a DataSourceVersion Add hostname to data source payload Add service hostname to trace spans Add specific error message for binding path that evaluates to an empty string Change\nChange core’s GraphQL schema to extend MOMFunctionEnum, OperationsEventLevelEnum and OperationsEventTypeEnum Change domain logger to respect logging.level configuration option Change GraphQL query to include option to retrieve disabled properties and include by default Change schema to use v3.0.0x Change to use a version’s iid when getting operational location class inherited properties Change to use a version’s iid when getting personnel class inherited properties Fix\nFix operationsEventClasses to CreateOperationsEventDefinitionVersionInput Fix rule trigger evaluating on stale data Schema Features\nAdd @id to WorkCalendarDefinitionEntry Add @search to requestState on OperationsRequest Add @search to segmentState on SegmentRequirement Add comments to MaterialLot and MaterialSublot Add comments to SegmentRequirement Add effectiveStart and effectiveEnd to WorkCalendarDefinition Add effectiveStart and effectiveEnd to WorkCalendarDefinitionEntry Add inverse relationships for HiearchyScope Add inverse relationships for OperationalLocation Add relationship between MaterialSublot and MaterialDefinition Add resource actual links to InformationObject Add Work Calendar Information models Add Work Calendar Information models to permissions.json Add fields @cascade directive to subgraph Changes\nChange OperationsRequest relationship to OperationsSchedule to optional Change startRule to required on WorkCalendarDefinitionEntry ","upgrade#Upgrade":"To upgrade to v3.0.0, follow the Upgrade instructions."},"title":"3.0.0 (general release)"},"/releases/changelog/3-0-0rc05/":{"data":{"":"Changelog for version 3.0.0rc05 of the Rhize application.\nRelease date: October 24, 2023","breaking-changes#Breaking changes":" Renamed UI environment variables and changed KEYCLOACK to KEYCLOAK ","changes-by-service#Changes by service":"Core Features\nAdd deleteSyncEquipmentsFromDBtoNATSStatus mutation Add NATS connection name Add fields to InformationObject Fixes\nFix tracing typo Changes\nChange Core to purge keys that are not in the the database Change core to delete property value from KV when the equipment no longer have active version BPMN Features\nAdd DQL Query Service Task Handling Add DQL Mutate Service Task Handling Add verbose log to HandleTaskComplete Add default log level to config.json Add GraphQL resolvers to get by key from known KV Stores Add Errors to GetWorkflowSpecification query Add shutdown handling to drain commandConsumer before shutting down Add a basic Intermediate Timer Event Changes\nChange NATS connection to include a client Name including hostname Change NATS server library to v2.10.2 from v2.9.9 Change NATS client library to v1.30.2 from v1.21.0 Change NATS GET/PUT error messages to be more verbose Change to synchronize WorkflowSpecifications to NATS once, instead of every possible update Change CallActivity to enforce variable context mapping Fixes\nFix linking Workflow Specifications by IID Fix NextVersion datatype in GetWorkflowSpecificationNextVersion query Fix referencing duplicate nodes in LoadBpmnFromXml Fix duplicate WorkflowMessage Error on Import Fix interpretation of escape characters on Linux Remove\nRemove printf statements from GraphQLQueryAdapter Remove license scanning from CI Remove slow execution debugging spans Agent Features\nAdd value to OPC-UA Value span Add Edge-Agent heartbeat details Changes\nChange OPC-UA Value span to log error if status isn’t OK (0x0) Change agent to filter out disabled topics Fixes\nFix OPC-UA Subscription Statistics panic in test suite Admin UI Breaking changes\nRenamed environment variables and change KEYCLOACK to KEYCLOAK Features\nAdd parameter specification in WorkMaster Add Equipment Property Test Add data migration popup auth Add BPMN Node Template for Schema Validation Changes\nChange Libre to Rhize Fixes\nFix no download option for properties on Person Fix no download option for properties on Personnel Class Schema Features\nAdd search to MaterialUse on OperationsDefinition, OperationsSchedule and OperationsPerformance Models Add search to EquipmentUse, Personneluse \u0026 PhsicalAssetUse on OperationsDefinition Add fields to InformationObject Fixes\nAdd missing @id Fix omitting omitempty for non-pointer Boolean types ","upgrade#Upgrade":"To upgrade to `v3.0.0rc05, follow the Upgrade instructions."},"title":"3.0.0rc05"},"/releases/changelog/3-0-0rc06/":{"data":{"":"Changelog for version 3.0.0rc06 of the Rhize application.\nRelease date: October 31, 2023","breaking-changes#Breaking changes":" NATS streams libreBpmn_command and LibreTimerStart must be deleted prior to starting ","changes-by-service#Changes by service":"Core Features\nAdd mutation for dependency check for DataSource, Equipment, EquipmentClass, OperationalLocation, and OperationalLocationClass Changes\nChange to go-module for schema Change struct literal unkeyed fields to keyed BPMN Breaking Changes\nNATS streams libreBpmn_command and LibreTimerStart must be deleted prior to starting Features\nAdd incoming libreBPMN_command data to traces for debugging on trace level logging Add notification to NATS of command progress Add JSONata processing of output-element in multi-instance execution Add JSONata processing of intermediate timer catch duration Changes\nChange DQL mutation node to use application/rdf Change the BPMN process ID to match the trace ID Change to zero-based loop counter for multi-instance execution Change multi-instance nodes error early on sequential parallel execution (not implemented) Remove\nRemove legacy domain code Fixes\nFix multi-instance requiring sequential for parallel execution Fix govulncheck identified issues Agent Changes\nChange to libre-schema go module import Remove\nRemove empty GraphQL API endpoint Admin UI Features\nAdd Physical Asset to Sidebar Add Certificate Authority input option to Rest Service Task Fixes\nFix large memory usage in production Fix Work Master UI Issues Schema Features\nAdd search by hash to material use Add signature to record entries Add requirements for for dependency changes Changes\nChange domain to be a go-module for import Fixes\nFix permissions generation with new gqlgen Remove\nRemove entity interface from generated code ","upgrade#Upgrade":"To upgrade to v3.0.0rc06, follow the Upgrade instructions."},"title":"3.0.0rc06"},"/releases/changelog/3-0-0rc07/":{"data":{"":"Changelog for version 3.0.0rc07 of the Rhize application.\nRelease date: 15th November, 2023","admin-ui#Admin UI":"Features\nAdd ability to create Process Segment Version Add ability to edit linked process segment resource specification property Add ability to link a Hierarchy Scope to a Operational Location Class version Add check for renaming a linked property with an existing property name Add Homepage screen Add optional link from Operational Location Class to a Operational Location Class version Add Physical Asset Properties Add Physical Asset Resource Specifications Change\nChange available BPMN UI palette options to supported objects only Fix\nFix incorrect version indicators in Data Source sidebar Fix Process Segment Version Bug Fix sidebar typo in Work Masters Fix template service task hiding multi-instance properties Remove\nRemove the unused or unsupported BPMN elements from the BPMN UI Agent Features\nAdd hostname as service instance to otel span Fix\nFix invalid errors reported to OTEL BAAS Changes\nChange CDC to use a JetStream from KV Store Fixes\nFix getting user from authorization token for setting _modifiedBy and _createdBy BPMN Features\nAdd a flag to bypass any OIDC requirements so that we can run BPMN without security enabled Add fallback to BAAS when NATS fails in HandleTaskComplete Add input validation on process id to check for dots in the name Add option for custom BPMN complete variable context Add OS hostname to service instance in otel spans Add port for adapter debugger so that adapter runtime configuration and information can be queried Add process ID to log when starting a new instance Add retry backoff to NATS KV Get Add string trim logic to all inputs/outputs on BPMN upload Add test case for High Availability Add token argument to bpmnctl to allow users to pass a token directly Change\nChange BPMN to NAK messages for unknown timers/streams to avoid dropping messages on startup Change CI/CD to use a minimal docker compose docker-compose.ci.yml from app-config-local Change logging message type based on error type when CreateAndRunInstance is called Change NATS client library to v1.31.0 from v1.30.2 Change NATS KV watchers to immediately defer stop to ensure lifecycle handling Change Parallel gateway join to use a GetOnce KV Get Fix\nFix goroutine leak on ack pending Core Features\nAdd dependency check to operations definition \u0026 work master Add docker login for CI/CD Add Equipment KV sync on startup Add OIDC bypass functionality when running in test pipelines Add OperationsEventClass Version Handlers Change\nChange CI/CD to use docker-compose.ci from app-config-local Change to Libre Schema v3.0.0rc7 Change subscriptions and watchers to wait until ready before starting synchronization Change to use libre-schema as a golang module instead of copying Remove\nRemove IntelliJ IDE workspace directory and files ./.idea/* Schema Features\nAdd Comments to OperationsEvent Add example docker-compose.yaml usage Add missing types for _createdBy and _modifiedBy Add Relationships to Class and Definition Versions Add Resource Relationship Network Model Change\nChange dockerfile to use baas v3.0.0rc7 Change library golang.org/x/crypto to v0.15.0 from v0.14.0 Change library golang.org/x/net v0.18.0 from v0.16.0 Change library golang.org/x/sync v0.5.0 from v0.4.0 Change library golang.org/x/tools v0.15.0 from v0.14.0 Fix\nFix missing defaults on _createdBy and _modifiedBy Fix test for Signature relationship to recordEntries ","breaking-changes#Breaking changes":" [SCHEMA] Change types OperationalLocationClass, OperationalLocationClassVersion, OperationalLocationClassProperty, OperationalLocation, OperationalLocationVersion and OperationalLocationProperty to have: isPartOf (0..1), isMadeUpOf (0..*) [BPMN] Change CommandConsumer and Timers to use new JetStream library and durable consumers. This requires you to drop and re-create streams KV_JobResponses, KV_WorkflowSpecifications, libreBpmn_Command and libreTimerStart. ","changes-by-service#Changes by service":"","upgrade#Upgrade":"To upgrade to v3.0.0rc07, follow the Upgrade instructions."},"title":"3.0.0rc07"},"/releases/changelog/3-0-0rc08/":{"data":{"":"Changelog for version 3.0.0rc08 of the Rhize application.\nRelease date: 19th December, 2023","breaking-changes#Breaking changes":" [ADMINUI] Change UI navigation bar design [SCHEMA] Change OperationsEventClass.IncludedIn to reference an OperationsEventClassVersion instead of OperationsEventClass [SCHEMA] Change OperationsEventClass.IncludesPropertiesOf to move to the version instance, OperationsEventClassVersion.IncludesPropertiesOf, instead of header [SCHEMA] Change OperationsEventDefinition.IncludedIn to reference an OperationsEventDefinitionVersion instead of OperationsEventDefinition [SCHEMA] Change OperationsEventDefinition.IncludesPropertiesOf to move to the version instance, OperationsEventDefinitionVersion.IncludesPropertiesOf, instead of header [SCHEMA] Change FromResourceReference and ToResourceReference to combine them into a single resourceReference ","changes-by-service#Changes by service":"Admin UI Features\nAdd Audit Trail View in UI Add BPMN Instance Viewer Add Operations Definition Segment Specifications Add Operations Event Class Page Add Operations Event Definition Page Add option to change Personnel Class version status from DRAFT to ACTIVE Add property metadata to Material Definition Properties page Add the table for parameters and physical assets Change\nChange Audit Log to GraphQL Playground Change Picker implementation Fix\nFix person with two ACTIVE versions\nFix parameter tab\nFix selection of WorkMaster parameter selection\nRemove\nRemove ability to edit active version Data Source general properties Agent Features\nAdd check to avoid continuous resubscription to bad OPCUA topics Add interactive OPC UA server for end-to-end testing Add support for Azure Service Bus Add support for MQTT Change\nChange OPC-UA subscription item reference strategy to use ClientHandles, MonitoredItems, and Node Ids in order Change agent to buffer protocol messages to disk if NATS is offline to avoid message loss Change data source interfaces into smaller pieces for readability and cognitive complexity Change from scratch to alpine base image Change monitored Items with bad status behavior to moved to a new subscription after a configurable timeout to encourage the OPC UA server to start providing value changes again Fix\nFix issue with gopcua client that resulted in OPC UA Session not being recreated after a loss of Secure Channel on reconnect Audit Features\nAdd GraphQL Subgraph to query audit log and query audit log tags Add Influx setup if buckets not available Add InfluxDB as data sink Add configuration option scanning via configuration file, environment, and command line arguments Add restart of consumer on NATS reconnect Add subscription of audit events Add write to data sink BAAS Features\nAdd _modifiedBy user to Audit Event Add check for required OIDC Roles Add warning for missing ScopeMap parameter when using OIDC Bypass Fix\nFix dgraph hanging on shutdown request Remove\nRemove wait groups for Enterprise Dgraph ACL functionality Remove license scanning CI/CD job BPMN Add\nAdd Async Publish Error logging to NATS KVs Add environmental variable expansion to json-schema service task Add graceful shutdown to command consumer port Add log message and time delay to CallActivity watcher Add multi-file JSON schema validation Add profile labels to go-routine launches Change\nChange CallActivity to event driven as opposed to a blocking go-routine to wait for complete of a sync call Change InProgess message to 20s on CommandConsumer from 29s Change libreBpmn.command. strings to use domain constant Change debug level log messages for timer checks and active workflows to trace level Change log level of gateways without inputs to trace level from error Change to git commits to use LN on *.go files Fix\nFix BPMN long save times by only updating the touched Workflow Specification Fix NATS reconnect re-subscribing to startOnNATS Topics Fix docker permissions in end-to-end CI/CD test case Fix memory leak in OIDC context value recursively growing Fix panic on nil workflowspec in HandleTaskComplete Core Features\nAdd hierarchyScope, materialAlternate, spatialDefinition and unitOfMeasure as information objects to GetOperationsEvent operationsEventRecords Add agent MQTT message handling Add binding path test cases Add check for empty migration records Add check for migration dependencies on Operations Event Record Entry Add debug logging to updateOperationsEventRecordEntry Add entity path to migration dependency checks Add initialization for Azure stream Add label to OperationsEventClassProperty, OperationsEventDefinitionProperty, OperationsEventProperty Add operations event definition versioning mutations Add option for Azure data source type Add option to activate newly created version if requested Change\nChange Equipment Class rule triggered event to immediately publish to NATS and then be picked up by Core instead of waiting an triggering to prevent libre-core shutdowns missing the event fire Change Operations Event Record Entry migration to remove existing children before checking for migration dependencies Change IncludesPropertiesOf to be on the version not the header of OperationsEventDefinition \u0026 OperationsEventClass Change async SaveVersionAs because other cases have been tested in sync tests Change consumer creation to delete/add consumer if it fails to update consumer Change database ping to allow no access token provided and context cancelled when pinging database Change default logger to use hostname instead of PID Change logging messages to reflect data source type Change migrations to remove existing children before checking for migration dependencies Remove\nRemove obsolete comments Schema Features\nAdd @id for OperationsEventDefinitionProperty Add JobOrder parent/children relationship Add stateTransitionInstance.previous, .next, and .comments Add automatic scopemap update step Add azure for datasource protocol Add comments to operationsSegment Add event subtype to event Add label to OperationsEventClassProperty Add label to OperationsEventDefinitionProperty Add label to OperationsEventProperty Add permission holder for Audit Add reason and status to operations event Changes\nChange mermaid diagrams to include recent changes ","upgrade#Upgrade":"To upgrade to v3.0.0rc08, follow the Upgrade instructions."},"title":"3.0.0rc08"},"/releases/changelog/3-0-0rc09/":{"data":{"":"Changelog for version 3.0.0rc9 of the Rhize application.\nRelease date: 27th February, 2024","breaking-changes#Breaking changes":"","changes-by-service#Changes by service":"Admin UI No changes since previous release.\nAgent Features\nAdd check to validate OPC UA topics against server metadata Add hostname in message payload Add logic for MQTT connections to resubscribe on disconnect Add option to deconstruct complex JSON payloads to simple types Audit Features\nAdd previous audit value relative to current BAAS Changes\nChange server aborts due to a conflict to report the error and block transaction that was aborted BPMN Features\nAdd a trigger for a NATS message on end of a Work Calendar Entry Add BPMN Run Instance flag to log variables on every task Add check for 0 duration when calculating next entry Add domain entities for tempo/loki queries and generalize them Add go profile guided optimisation Add improved error logging to view instance Add named constants for constant strings Add test case for call activities in high availability Add timeouts to http.Server Add type type assertion checks Add warn when not enough data is returned to process nodes to spans Change\nChange application to refresh token before fetching work calendar Change BPMN Datasource Method Call Method arguments to allow map[string]any, map[int]any, or string Change BPMN engine execution to use JetStream errors for parallel gateway check Change BPMN engine to expand variables and secrets before sanitizing NATS subject Change BPMN Instances to use a unique consumer name Change BPMN to error if Router unavailable on startup Change CSM to include child job to avoid relying on NATS to synchronize Change NATS KV Get timeout to 34 attempts from 7 Change stream expiry to 10 minutes for CommanStreamReplicas Change stringified ints into non-stringy types (e.g. durations as time.Duration) Change tempo/loki facing code into separate driver Change timers to start calculating from the closest year Change to only log task variables on Complete or Error Change VersionState to use GraphQL Enums Change View Instance to pull the latest version when unspecified Change Work Calendar invocations to use natsClient.Publish instead of natsClient.StreamPublishMsg Fix\nFix issue where timers are being called inconsistently Fix test cases for view instance Fix unstable high-availability test Fix variable and secret expansion permissions by including OIDC context Remove\nRemove additional calls to NATS to avoid retrieval issues due to eventual consistency Remove overuse of of arbitrary pointers to strings Remove superfluous marshals/unmarshals of Job Responses during execution Remove superfluous parsing from string -\u003e time.Time only to run time.Format() in UnixToRFC3339 Remove vestigial config argument for CommandConsumer Core No changes since previous release.\nSchema No changes since previous release.","upgrade#Upgrade":"To upgrade to v3.0.0rc09, follow the Upgrade instructions."},"title":"3.0.0rc09"},"/use-cases/":{"data":{"":"Topics about how to use Rhize for end-to-end workflows\nOverview of use casesHandle manufacturing events, access the knowledge graph of the operation, build custom MOM applications. Data collection (eBR example)An example of how Rhize ingests data from various sources to create Electronic Batch Records for pharmaceutical manufacturing Electronic Batch RecordsThe Rhize guide to querying all the information that happened in a manufacturing job. GenealogyThe Rhize guide to modelling and querying the forward and backward genealogy of a material lot "},"title":"Use cases"},"/use-cases/data-collection-ebr/":{"data":{"":" 📝 Looking to implement Rhize for your Pharma operation? Talk to an engineer\nThis document provides examples of how you can use Rhize to automatically ingest data from various sources and store it in a standardized ISA-95 schema. The examples here are to produce an Electronic Batch Record, an important use case in pharmaceutical manufacturing. However, the process described here is very similar to what data collection would like when integrating with any third-party system (such as an MRP, CMMS, and so on), no matter the type of manufacturing process.\nThe procedure has the following steps:\nIdentify the sources of data. Map the fields for these data sources to Rhize’s ISA-95 schema. Write BPMN processes that listen for relevant eBR events and transform the incoming data to the ISA-95 schema. After the batch finishes, query the database with the fields for your eBR. The following sections describe this process in a bit more detail.","next-steps#Next steps":"Fast eBR automation is just one of many use cases of Rhize in the pharmaceutical industry. With the same event data that you automatically ingest and filter in this workflow, you can also:\nProgram reactive logic using BPMN for event orchestration. For example, you might send an alert after detecting a threshold condition. Analyze multiple batch runs for deviations. For example, you can query every instance of a failure mode across all laboratories. Compare batches against some variable. For example, you can compare all runs for two versions of equipment. ","prerequisites#Prerequisites":"This procedure involves multiple data sources and different operations to transform the incoming data and store it in the graph database. Before you start, ensure that you have the following:\nAwareness of the different sources of eBR data If real-time data comes from equipment, Connected data sources Sufficient knowledge of the ISA-95 standard to model the input data as ISA-95 schema In the BPMN process, the ability to filter JSON and call the GraphQL API In larger operations, different teams may help with different parts of the eBR-creation process. For example, your integrators may help appropriately model the data, and the frontend team may render the outputted JSON into a final document.","steps-to-automate-ebr-creation#Steps to automate eBR creation":"The following steps describe the broad procedure to automatically update the database for incoming batch data, then automatically create an eBR after the batch run.\nIdentify the sources of data The first step is to identify the sources of data for your eBR.\nCommon data sources for an eBR include:\nERP documents: high-level operations documents. This might include information about planning and scheduling. MES data: granular process data, tracking objects such as the weight of individual material and the responses for different jobs. LIMS documents: information about the laboratory environment and testing samples Real-time event data: for example, data sent from OPC UA or MQTT servers Exceptions: Errors and alerts raised from the operator screen or automation system Model the fields as ISA-95 schema After you’ve identified the source data, the next step is to map this data into ISA-95 models.\nSome common objects to map include raw and final material, equipment, personnel, operations schedule, segments, job responses, exceptions, and the ERP batch number. Once ingested, all data is linked through common associations in the graph database and is thus accessible through a single query.\nWrite a BPMN workflow to ingest the data in real-time A simplified BPMN workflow. For an example of a real workflow with nodes for each logic step, refer to the next image. With the sources of data and their corresponding models, the next step is to write a BPMN workflow to automatically transform the data and update the database.\nℹ️ You may want to break these steps into multiple parts. Or, for increased modularity, you can call another BPMN workflow with a Call activity. The procedure is as follows:\nCreate a BPMN that is triggered by a relevant eBR event. For example, the workflow might subscribe to a /lims/lab1, or be triggered by a call to the Rhize API. If the data comes from certain equipment, you first need to Connect a data source.\nTransform with JSONata\nRhize has a built-in JSONata interpreter, which can filter and transform JSON. Use a JSONata service task to map the data sources into the corresponding ISA-95 fields that you defined on the previous step.\nUse the output as a variable for the next step.\nPOST data with a graph mutation.\nUse the variable returned by the JSONata step to send a mutation to update the Graph database with the new fields. To learn more, read the Guide to GraphQL with Rhize.\nIn real BPMN workflows, you can dynamically create and assign fields as they enter the system. For example, this workflow creates a new material definition and material-definition version based on whether this object already exists.\nThis step can involve multiple BPMN processes subscribing to different topics. As long as the incoming event data has a common association, for example, through the id of the batch data and associated JobResponse, you can return all eBR fields in one GraphQL query—no recursive SQL joins needed.\nQuery the DB with the eBR fields After the batch finishes, use a GraphQL query to receive all relevant batch data. You only need one precise request to return exactly the data you specify.\nHere is a small, generic snippet of how it looks: Note how the query specifies exactly the fields to return: no further response filtering is required. For an idea of how a more complete query looks, refer to the Electronic Batch Records guide.\nSnippet of a makeEbr query query makeEbr ($filter: JobOrderFilter) { queryJobResponse(filter: $filter) { EXAMPLE_id: id description matActualProduced: materialActual(filter:{materialUse: { eq:Produced }}){ id material: materialDefinitionVersion{ id } quantity quantityUoM { id } } ## More eBR fields } } The only extra step is to use the returned JSON object as the input for however you create your eBR documents."},"title":"Data collection (eBR example)"},"/use-cases/ebr/":{"data":{"":"This document shows you how to use ISA-95 and Rhize to create a generic, reusable model for any use case that involves Electronic batch records. As long as you properly map and ingest the source data, the provided queries here should work for any operation, though you’ll need to tweak them to fit the particular structure of your job response and reporting requirements.\nAn Electronic Batch Record (eBR) is a detailed report about a specific batch. Generating eBRs is an important component of pharmaceutical manufacturing, whose high standards of compliance and complexity of inputs demand a great deal of detail in each report.\nRhize can model events and resources at a high degree of granularity, and its ISA-95 schema creates built-in relationships between these entities. So it makes an ideal backend to generate eBRs with great detail and context. In a single query, you can use Rhize to identify answers to questions such as:\nWhat material, equipment, and personnel were involved in this job? And what was their function in performance? When and where did this job happen? How long did it run for? Why was this work performed? That is, what is the order that initiated the work? What are the results of quality testing for this work? 📝 The focus here is modeling and querying. For a high-level overview of how eBR data may enter the Rhize data hub, read the guide to Data collection. ","background-isa-95-entities-in-an-ebr-query#Background: ISA-95 entities in an eBR query":" 📝 For an introduction to ISA-95 and its terminology, read How to speak ISA-95. The following lists detail the ISA-95 entities that you might need when querying the Rhize database for an eBR. As always, your manufacturing needs and data-collection capabilities determine the exact data that is necessary. It is likely that some of the following fields are irrelevant to your particular use case.\nPerformance information A job response represents a unit of performed work in a manufacturing operation. The job response typically forms the core of an eBR query, as you can query it to obtain duration and all resource actuals involved in the job. A job response may also contain child job responses, as displayed in the following diagram:\nAn example of a job response with child job responses. The parent job has a material actual representing the final produced good. The child job responses also have their own properties that may be important to model. This is just one variation of many. ISA-95 is flexible and the best model always depends on your use case. For an eBR, some important job response properties and associations include the following:\nStart and End times. When work started and how long it lasted. Material Actuals. The quantities of material involved and how they are used: consumed, produced, tested, scrapped, and so on. Material actuals may also have associated lots for unique identification. Test results may be derived from samples of the material actual. Equipment Actuals. The real equipment used in a job, along with associated equipment properties and testing results. Personnel actuals. The people involved in a job or test. Process values. Associated process data and calculations. Comments and Signatures. Additional input from operators. Scheduling information An eBR report might also include information about the work that was demanded. The simplest relationship between performance and demand is the link between a job response and a job order. So your eBR might include information about the order that initiated the response. Through this order, you could also include higher-level scheduling information.\nWhen adding order information, consider whether you need the following properties:\nScheduled start and end times. These might be compared to the real start and end. Material requirements. The material that corresponds to the material actuals in the performance. Requirements may include: Material to be produced, along with their scheduled quantities and units of measure Material to be consumed, along with their scheduled quantities and units of measure Any by-product material and scrap Planned equipment. This can be compared to the real equipment used. Work Directive. The dispatched version of the planned work. The directive may include: Specifications or a BoM (if the requirements are not in the order itself) Any relevant work master configuration (routing, process parameters like temperature, durations, and so on) Quality information Your eBR trace also may record test results. These results provide context about the quality of the work produced in the job response.\nEach resource actual can have a corresponding test result. For example:\nThe material actual and lot may record the sample. The equipment actual may record test locations. Physical asset actuals may record instruments used for the test. Personnel actuals may record who performed the test. ","build-a-reporting-frontend#Build a reporting frontend":"As a final step, you can also transform the JSON payload into a more human-readable presentation. As always, you have a few options. Here are a few, from least to most interactive:\nCreate a PDF report, perhaps using specialized software such as InfoBatch Create a static web report, using basic HTML and CSS Build an interactive report explorer, which may include links to other reports and dynamic visualizations of alerts and performance ","example-query#Example query":"The following snippet is an example of how to pull a full eBR from a single GraphQL query. Each top-level object has an alias, which serves as the key for the object in the JSON payload.\nquery eBR { performance: getJobResponse(id: \"ds1d-119-as\") { # duration, actuals, and so on } planning: getJobOrder(id: \"ds1d-119-jo-119-3\") { # requirements, work directive, and so on } testing: getTestResult(id: \"ds1d-119-tr-3\") { # evaluation properties and tested objects } Full queryExample response: performance eBR Variables\n{ \"getJobResponseId\": \"ds1d-batch-119-jr-fc-make-frosting\", \"getJobOrderId\": \"ds1d-batch-jo-119\", \"getTestResultId\": \"ds1d-batch-tr-119\" } Query\nquery eBR ($getJobResponseId: String $getJobOrderId: String $getTestResultId: String) { performance: getJobResponse(id:$getJobResponseId) { jobResponseId: id startDateTime endDateTime duration jobState workDirective { id } materialActual { id materialUse quantity quantityUoM { id } materialDefinition { id } materialLot { id materialDefinition { id } } materialSubLot { id } properties { id } } equipmentActual { id equipment { id } description children { id } properties { id value valueUnitOfMeasure { id } } } personnelActual { id } } planning: getJobOrder(id: $getJobOrderId) { orderId: id scheduledStartDateTime scheduledEndDateTime materialRequirements { id quantity quantityUoM { id } } equipmentRequirements { id equipment { id } } workMaster{ id parameterSpecifications { id description } } } testing: getTestResult(id: $getTestResultId) { resultsId: id id evaluationDate expiration evaluationCriterionResult equipmentActual { id } materialActual { id materialUse } physicalAsset { id } equipmentActual { id } } } } The performance section of this query may return data that looks something like this. Note that every object does not necessarily have every requested field. In this example, only some material actuals have additional properties.\n{ \"data\": { \"performance\": { \"id\": \"ds1d-batch-119-jr-fc-make-frosting\", \"startDateTime\": \"2024-09-23T23:22:25Z\", \"endDateTime\": \"2024-09-23T23:38:04.783Z\", \"duration\": 939.783, \"materialActual\": [ { \"id\": \"ds1d-batch-fc-cookie-frosting-actual-119\", \"materialUse\": \"Produced\", \"quantity\": 3499.46, \"quantityUoM\": { \"id\": \"g\" }, \"materialLot\": [ { \"id\": \"ds1d-batch-fc-cookie-frosting-lot-119\" } ], \"materialSubLot\": [], \"properties\": [ { \"id\": \"viscosity\", \"value\": \"0.1\", \"valueUnitOfMeasure\": { \"id\": \"mm2/s\" } }, { \"id\": \"temperature\", \"value\": \"22\", \"valueUnitOfMeasure\": { \"id\": \"C\" } } ] }, { \"id\": \"ds1d-batch-fc-butter-actual-119\", \"materialUse\": \"Consumed\", \"quantity\": 1124.05, \"quantityUoM\": { \"id\": \"g\" }, \"materialLot\": [ { \"id\": \"ds1d-batch-fc-butter-lot-119\" } ], \"materialSubLot\": [], \"properties\": [ { \"id\": \"fat-percent\", \"value\": \"15\", \"valueUnitOfMeasure\": null } ] }, { \"id\": \"ds1d-batch-fc-confectioner-sugar-actual-119\", \"materialUse\": \"Consumed\", \"quantity\": 249.08, \"quantityUoM\": { \"id\": \"g\" }, \"materialLot\": [ { \"id\": \"ds1d-batch-fc-confectioner-sugar-lot-119\" } ], \"materialSubLot\": [], \"properties\": [] }, { \"id\": \"ds1d-batch-fc-peanut-butter-actual-119\", \"materialUse\": \"Consumed\", \"quantity\": 2249.63, \"quantityUoM\": { \"id\": \"g\" }, \"materialLot\": [ { \"id\": \"ds1d-batch-fc-peanut-butter-lot-119\" } ], \"materialSubLot\": [], \"properties\": [] } ], \"equipmentActual\": [ { \"id\": \"ds1d-batch-kitchen-mixer-actual-119\", \"description\": null, \"children\": [], \"properties\": [] }, { \"id\": \"ds1d-batch-kitchen-actual-119\", \"description\": null, \"children\": [], \"properties\": [] } ], \"personnelActual\": [ { \"id\": \"ds1d-batch-fc-supervisor-actual-119\" }, { \"id\": \"ds1d-batch-fc-handler-actual-119\" } ] } } } ","next-steps-combine-with-other-use-cases#Next steps: combine with other use cases":"You can reuse and combine the queries here for other use cases that involve tracking and performance analysis. For example, if you want a detailed report for the movement of material, you can combine the queries here with a query for a material lot genealogy. This would provide a detailed report for every job that involved an ancestor or descendent of the queried material.","quick-query#Quick query":"If you just want to build out a GraphQL query for your reporting, use these templates to get started.\nIf you know the IDs for the relevant job response, job order, and test results, you can structure each group as a top-level object. If you want to input only one ID, you can also use nested fields on a response, order, or test specification to pull all necessary information. The Rhize DB stores relationships, so the values are identical—only the structure of the response changes.\nFlatNested query eBR { performance: getJobResponse(id: \"\u003cJOB_ID\u003e\") { # duration, actuals, and so on } planning: getJobOrder(id: \"\u003cORDER_ID\u003e\") { # requirements, work master, and so on } testing: getTestResult(id: \"\u003cTEST_ID\u003e\") { # evaluation properties and tested objects } } query nestedBatchReport { jobResponse: getJobResponse(id: \"ds1d-batch-119-jr-fc-make-frosting\") { id ## more fields about performance materialActual { ## repeat for other resources as needed id quantity testResults { ## test results for material id } ## More material fields } associated_order: jobOrder { id ## more planning fields } } } For more detail, refer to the complete example query."},"title":"Electronic Batch Records"},"/use-cases/genealogy/":{"data":{"":"","background-material-entities-in-rhize#Background: material entities in Rhize":"","examples#Examples":"This document provides a high-level overview of how to use Rhize for material genealogy.\nIn manufacturing, a genealogy is the record of what a material contains or what it is now a part of. As its name implies, genealogy represents the family tree of the material. A material genealogy can help manufacturers in multiple ways:\nPrevent product recalls by isolating deviations early in the material flow Decrease recall time by having a complete record of where material starts and ends Help build reports and analysis of material deviations Create documentation and compliance records for material tracking Rhize provides a standards-based schema to represent material at all levels of granularity. The graph structure of its DB has built-in properties to associate material lots with other information about planned and performed work. This database has a GraphQL API, which can pull full genealogies from terse queries. As such, Rhize makes an ideal backend to use for genealogical use cases.\nData from a Rhize query in an Apache Echart. Read the Build frontend section for details.\nQuick query To get started with genealogy quickly, use these query templates. One template is for the reverse genealogy, and the other is for the forward genealogy. For each, you need to input the Lot ID.\nReverse genealogy queryForward genealogy query query reverseGenealogy{ getMaterialLot(id: \"\u003cLOT_ID\u003e\") { parent_lots: isAssembledFromMaterialLot { id grandparent_lots: isAssembledFromMaterialLot { id great_grandparent_lots: isAssembledFromMaterialLot { id } } } } } query forwardGenealogy{ getMaterialLot(id: \"\u003cLOT_ID\u003e\") { child_lots: isAssembledFromMaterialLot { id grandchildren_lots: isAssembledFromMaterialLot { id great_grandgrandchildren_lots: isAssembledFromMaterialLot { id } } } } } You can also modify the query to include more fields, levels, or get the forward and backward genealogy. For an idea of how a more complete query would look, refer to the Examples section.\nBackground: material entities in Rhize 📝 For a more complete introduction to ISA-95 and its terminology, read How to speak ISA-95. In ISA-95 terminology, the lineage of each material is expressed through the following entities:\nMaterial lots. Unique amounts of identifiable material. For example, a material lot might be a camshaft in an engine or a package of sugar from a supplier. Material Sublots. Uniquely identifiable parts of a material lot. For example, if a box of consumer-packaged goods represents a material lot, the individual serial numbers of the packages within might be the material sublots. Each sublot is unique, but multiple sublots may share properties from their parent material lot (for example, the expiry date). The relationship between lots are expressed through the following properties :\nisAssembledFromMaterial[Sub]lot and isComponentOf[Sub]lot. The material lots or sublots that went into another material. parentMaterialLot and childSubLot. The relationships between a material lot and its sublots. Note that these properties are symmetrical. If lot final-1 has the property {isAssembledFromMaterialLot: \"intermediate-1\"}, then lot intermediate-1 has the property {isComponentOfMaterialLot: \"final-1\" }. The graph structure of the RhizeDB creates these links automatically.\nℹ️ The distinction between sublots and material lots varies with processes. The rest of this document simplifies terminology by using only the word “lots”. Steps to use Rhize for genealogy The following sections describe how to use Rhize to build a genealogy use case. In short:\nIdentify the unique lots in your material flows. Add these lots to your model. Implement how to collect these lots. Query the database. The example uses a simple baking process to demonstrate the material flow.\nA simplified view of how a pallet of packaged goods is assembled from lots and sublots. Identify lots to collect To use Rhize for genealogy, first identify the material lots that you want to identify. How you identify these depends on your processes and material flows. The following guidelines generally are true:\nThe lot must be uniquely identifiable. The level of granularity of the lots is realistic for your current processes. For example, in a small baking operation, lots might come from the following areas:\nThe serial numbers of ingredients from suppliers The batches of baked pastries The wrappers consumed by the packing process The pallets of packed goods (with individual packages being material sublots). ℹ️ For some best practices of how to model, read our blog How much do I need to model? Model these lots into your knowledge graph After you have identified the material lots, model how the data fits with the other components of your manufacturing knowledge graph. At minimum, your material lots must have a material definition with an active version.\nBeyond these requirements, the graph structure of the ISA-95 database provides many ways to create links between lots and other manufacturing entities, including:\nA work request or job response The associated resource actual In aggregations such as part of a material class, or part of the material specifications in a work master. In the aforementioned baking process, the lots may have:\nMaterial classes (raw, intermediate, and final) Associated equipment (such as mixers, ovens, and trays) Associated segments (such as mixing or cooling) Associated measurements and properties Implement how to store your lots in the RhizeDB After you have planned the process and defined your models, next implement how to add material lot IDs to Rhize in the course of your real manufacturing operation.\nYour manufacturing process determines where lot IDs are created. The broad patterns are as follows:\nScheduled. Assign lots at the time of creating the work request or schedule (while the job response might create a material actual that maps to the requested lot ID). Scheduled and event-driven. Generate lot IDs beforehand, and then use a GraphQL call to create records in the Rhize DB after some event. Example events might be a button press or an automated signal that indicates the lot has been physically created. Event-driven. Assign lot IDs at the exact time of work performance. For example, you can write a BPMN workflow to subscribe to a topic that receives information about lots and automatically forwards the IDs to your knowledge graph. In the example baking process, lots may be collected in the following ways:\nScanned from supplier bar code Generated after the quality inspector indicates that a tray is finished Planned in terms of final package number and expiration date Query the data After you start collecting data, you can also start querying it through the materialLot query operations. The following section provides example genealogy queries.\nExamples The following examples show how to query for forward and backward genealogies using the get operation to query material lots.\nℹ️ You could also query for multiple genealogies—either through material lots or through aggregations such as material definitions and specifications— then apply filters. Backward genealogy A backward genealogy examines all material lots that make the assembly of some later material lot.\nIn Rhize, you can query this relationship through the isAssembledFromMaterialLot property, using nesting to indicate the level of material ancestry to return. For example, this returns four levels of backward genealogy for the material lot cookie-box-2f (using a fragment to standardize the properties returned for each lot).\nquery{ getMaterialLot (id:\"cookie-box-2f\") { ...lotFields isAssembledFromMaterialLot { ...lotFields isAssembledFromMaterialLot { ...lotFields isAssembledFromMaterialLot { ...lotFields } } } } } } ## Common fields for all nested material fragment lotFields on MaterialLot{ id quantity quantityUnitOfMeasure{id} materialDefinition{id} } The returned genealogy looks something like the following:\nexample-backward-genealogy.json { \"data\": { \"getMaterialLot\": { \"id\": \"cookie-box-2f\", \"quantity\": 1, \"quantityUnitOfMeasure\": { \"id\": \"cookie box\" }, \"materialDefinition\": { \"id\": \"cookie-box\" }, \"isAssembledFromMaterialLot\": [ { \"id\": \"cookie-unit-dh\", \"quantity\": 1000, \"quantityUnitOfMeasure\": { \"id\": \"cookie unit\" }, \"materialDefinition\": { \"id\": \"cookie-unit\" }, \"isAssembledFromMaterialLot\": [ { \"id\": \"cookie-frosting-9Q\", \"quantity\": 3500, \"quantityUnitOfMeasure\": { \"id\": \"g\" }, \"materialDefinition\": { \"id\": \"cookie-frosting\" }, \"isAssembledFromMaterialLot\": [ { \"id\": \"butter-67\", \"quantity\": 1125, \"quantityUnitOfMeasure\": { \"id\": \"g\" }, \"materialDefinition\": { \"id\": \"butter\" } }, { \"id\": \"confectioner-sugar-yN\", \"quantity\": 250, \"quantityUnitOfMeasure\": { \"id\": \"g\" }, \"materialDefinition\": { \"id\": \"confectioner-sugar\" } }, { \"id\": \"peanut-butter-Cq\", \"quantity\": 2250, \"quantityUnitOfMeasure\": { \"id\": \"g\" }, \"materialDefinition\": { \"id\": \"peanut-butter\" } } ] }, { \"id\": \"cookie-dough-Vr\", \"quantity\": 15000, \"quantityUnitOfMeasure\": { \"id\": \"g\" }, \"materialDefinition\": { \"id\": \"cookie-dough\" }, \"isAssembledFromMaterialLot\": [ { \"id\": \"egg-gY\", \"quantity\": 50, \"quantityUnitOfMeasure\": { \"id\": \"large-egg\" }, \"materialDefinition\": { \"id\": \"egg\" } }, { \"id\": \"flour-kO\", \"quantity\": 7500, \"quantityUnitOfMeasure\": { \"id\": \"g\" }, \"materialDefinition\": { \"id\": \"flour\" } }, { \"id\": \"saZ3\", \"quantity\": 150, \"quantityUnitOfMeasure\": { \"id\": \"g\" }, \"materialDefinition\": { \"id\": \"salt\" } }, { \"id\": \"sugar-32\", \"quantity\": 2500, \"quantityUnitOfMeasure\": { \"id\": \"g\" }, \"materialDefinition\": { \"id\": \"sugar\" } }, { \"id\": \"vanilla-extract-px\", \"quantity\": 10, \"quantityUnitOfMeasure\": { \"id\": \"g\" }, \"materialDefinition\": { \"id\": \"vanilla-extract\" } } ] } ] }, { \"id\": \"cookie-wrapper-NR\", \"quantity\": 150, \"quantityUnitOfMeasure\": { \"id\": \"wrapper\" }, \"materialDefinition\": { \"id\": \"cookie-wrapper\" }, \"isAssembledFromMaterialLot\": [] } ] } } } Forward genealogy A forward genealogy examines the history of how one lot becomes a component of another. For example, if a supplier informs a manufacturer about an issue with a specific raw material, the manufacturer can run a forward genealogy that looks at the downstream material that consumed these bad lots.\nIn Rhize, you can query the forward genealogy through the isComponentOfMaterialLot property, using nesting to indicate the number of levels of forward generations. For example, this query returns the full chain of material that contains (or contains material that contains) the material sublot peanut-butter-Cq:\nquery GetMaterialLot($getMaterialLotId: String) { getMaterialLot(id: \"peanut-butter-Cq\") { id isComponentOfMaterialLot { id isComponentOfMaterialLot { id isComponentOfMaterialLot { id } } } } } This query returns data in the following structure:\n{ \"data\": { \"getMaterialLot\": { \"id\": \"peanut-butter-Cq\", \"isComponentOfMaterialLot\": { \"id\": \"cookie-frosting-9Q\", \"isComponentOfMaterialLot\": { \"id\": \"cookie-unit-dh\", \"isComponentOfMaterialLot\": { \"id\": \"cookie-box-2f\" } } } } } } ","next-steps-display-and-analyze#Next steps: display and analyze":"The preceding steps are all you need to create a data foundation to use Rhize for genealogy. After you’ve started collecting data, you can use the genealogy queries to build frontends and isolate entities for more detailed tracing and performance analysis.\nBuild frontends All data that you store in the Rhize DB is exposed through the GraphQL API. This provides a flexible way to create custom frontends to organize your genealogical analysis in the presentation that makes sense for your use case. For example, you might represent the genealogy in any of the following ways:\nIn a summary report, providing a brief list of the material and all impacted upstream or downstream lots As an interactive list, which you can expand to view a lot’s associated quantities, job order, personnel and so on As the input for a secondary query In a display using some data visualization library For a visual example, the interactive chart in the introduction takes the data from the preceding reverse-genealogy query, transforms it with a JSONata expression, and visualizes the relationship using Apache echarts.\nThe JSONata expression accepts an array of material lots, then recursively renames all IDs and isAssembledFrom properties to name and children, the data structure expected by the visualization. Additional properties remain to provide context in the chart’s tooltips.\n( $makeParent := function($data){ $data.{ \"name\": id, \"value\": quantity \u0026 \" \" \u0026 quantityUnitOfMeasure.id, \"definition\": materialDefinition.id, \"children\": $makeParent(isAssembledFromMaterialLot) } }; $makeParent($.data.getMaterialLot) ) We’ve also embedded Echarts in Grafana workspaces to make interactive dashboards for forward and reverse genealogies:\nAn interactive genealogy dashboard. Filter by material definition, then select specific material lots. Combine with granular tracing and performance analysis While a genealogy is an effective tool on its own, the usefulness of the Rhize data hub compounds with each use case. So genealogy implementations are most effective if you can combine them with the other data stored in your manufacturing knowledge graph.\nFor example, the genealogical record may provide the input for more granular track and trace , in which you use the Lot IDs to determine track material movement across equipment and storage, associated personnel, and so on.\nYou could also combine genealogy with performance analysis, using the genealogical record as the starting point to analyze and predict failures and deviations.","quick-query#Quick query":"","steps-to-use-rhize-for-genealogy#Steps to use Rhize for genealogy":""},"title":"Genealogy"},"/use-cases/overview/":{"data":{"":"Rhize’s flexible, event-centric architecture serves many functions. While Rhize has components that can replace an MES, historian, andon, and real-time monitoring solutions, it can complement them just as well. You can map data from an MES or ERP into the database, creating a coherent data model to unite your operations IT.\nBesides better performance and flexibility, Rhize has far tighter integration of plant and system data. Yet, its data model is generic enough to conform to may use cases, chiefly:\nA manufacturing knowledge graph. Query the entire context and history of the operation. Headless MES or MOM. Use the API to build custom applications for a variety of MES and MOM activities. An event handler. Receive manufacturing message streams and react to them. Of course, each of these use cases has many uses cases. These use cases of Rhize are already implemented in discrete, continuous, and batch manufacturing operations.","calculating-oee#Calculating OEE":"Rhize includes an optional KPI service which can calculate OEE. Using a combintion of Rhize Workflows and Real-time event handling. Data can be transformed and persisted to a time series database in a format that allows the KPI sercvice to calculate key metrics.\nGuide: KPI Service","headless-mes-or-mom#Headless MES or MOM":"Rhize serves as a backend to create custom applications to replace traditional MES or MOM systems. Rather than force its opinion of what an MES interface should look like, Rhize provides the only data model, API, and BPMN engine. Your frontend teams can then use the tools of their choice to make the MES designed for your use case, with all the backend work delegated to Rhize’s normal operation features.\nWith the combination of its event-driven architecture and unified data model, Rhize can:\ncalculate OEE, or far more granular metrics Handle schedules and maintenance orders Track and trace material Execute dynamic workflows. Besides building bespoke frontends, many operators choose to integrate Rhize with low-code systems like Appsmith. For some problems, lowcode models can reduce the time to create applications dramatically, making it easier to create and test prototypes, involve more stakeholders in the application design process, iterate on working models, and generally do useful things more quickly.\nGuides: Model production, Connect process data.","manufacturing-knowledge-graph#Manufacturing knowledge graph":"All data that Rhize collects, whether from sensors or an ERP, is contextual and interconnected. Rather than a relational database, Rhize uses a graph database, where any node can link to any other, and users can query any data combination, without requiring complex joins.\nThe graph database unlocks new possibilities for manufacturing analysis and data science. For example:\nRun queries to find anomalies in an operation—which may trace to a specific site, segment, equipment, material lot, personnel, and so on. Discover places to optimize the system, whether they are bottlenecks to remove or highly productive areas to replicate Train deep-learning models to detect conditions that lead to batch failures. Use the historical record as a model to run simulations of new events Guide: Use the knowledge graph","real-time-event-handling#Real-time event handling":"The fundamental design of Rhize is low-latency and event-driven. Rhize can collect and monitor data from protocols like MQTT and OPC-UA.\nRhize also has components to monitor and react to this data stream, ensuring that you can stop problems early—and program corrective measures to execute automatically. Event orchestration is handled through BPMN, a low-code interface that can listen for events and initiate conditional flows.\nGuide: Handle events"},"title":"Overview of use cases"}}